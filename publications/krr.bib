@article{a696b1346ffd48b98cd45cee53b69329,
title = "Distance-Based Equilibria in Normal-Form Games",
author = "Erman Acar and Reshef Meir",
year = "2020",
month = "1",
day = "20",
language = "English",
journal = "Proceedings of the AAAI Conference on Artificial Intelligence",
issn = "2159-5399",
}

@article{7fcc8392689e4c6f8bc5c8ae55eb89a2,
  title     = "Literally better: Analyzing and improving the quality of literals",
  abstract  = "Quality is a complicated and multifarious topic in contemporary Linked Data research. The aspect of literal quality in particular has not yet been rigorously studied. Nevertheless, analyzing and improving the quality of literals is important since literals form a substantial (one in seven statements) and crucial part of the Semantic Web. Specifically, literals allow infinite value spaces to be expressed and they provide the linguistic entry point to the LOD Cloud. We present a toolchain that builds on the LOD Laundromat data cleaning and republishing infrastructure and that allows us to analyze the quality of literals on a very large scale, using a collection of quality criteria we specify in a systematic way. We illustrate the viability of our approach by lifting out two particular aspects in which the current LOD Cloud can be immediately improved by automated means: value canonization and language tagging. Since not all quality aspects can be addressed algorithmically, we also give an overview of other problems that can be used to guide future endeavors in tooling, training, and best practice formulation.",
  keywords  = "data observatory, Data quality, linked data, quality assessment, quality improvement",
  author    = "Wouter Beek and Filip Ilievski and Jeremy Debattista and Stefan Schlobach and Jan Wielemaker",
  year      = "2018",
  doi       = "10.3233/SW-170288",
  volume    = "9",
  pages     = "131--150",
  journal   = "Semantic Web",
  issn      = "1570-0844",
  publisher = "IOS Press",
  number    = "1",
}


@article{d589859477984c67919e8ae0d5586e4c,
  title     = "The Knowledge Graph as the Default Data Model for Machine Learning",
  abstract  = "In modern machine learning, raw data is the pre-ferred input for our models. Where a decade ago data sci-entists were still engineering features, manually picking out the details they thought salient, they now prefer the data as raw as possible. As long as we can assume that all relevant and irrelevant information is present in the input data, we can design deep models that build up intermediate represen-tations to sift out relevant features. In some areas, however, we struggle to find this raw form of data. One such area involves heterogeneous knowledge: entities, their attributes and internal relations. The Semantic Web community has in-vested decades of work on just this problem: how to repre-sent knowledge, in various domains, in as raw and as usable a form as possible, satisfying many use cases. This work has led to the Linked Open Data Cloud, a vast and distributed knowledge graph. If we can develop methods that operate on this raw form of data–the knowledge graph–we can dispense with a great deal of ad-hoc feature engineering and train deep models end-to-end in many more domains. In this position paper, we describe current research in this area and discuss some of the promises and challenges of this approach.",
  keywords  = "End-to-End Learning, Knowledge Graphs, Machine Learning, Position paper, Semantic Web",
  author    = "Xander Wilcke and Peter Bloem and {De Boer}, Victor",
  year      = "2017",
  month     = "10",
  doi       = "10.3233/DS-170007",
  pages     = "1--19",
  journal   = "Data Science",
  issn      = "2451-8484",
  publisher = "IOS Press",
}


@article{8809d20f6f33464690787510fcb71cc7,
  title    = "An integrated approach for linked data browsing",
  abstract = "The Netherlands' Cadastre, Land Registry and Mapping Agency – in short Kadaster – collects and registers administrative and spatial data on property and the rights involved. Currently, the Kadaster is publishing its geo-spatial data assets as Linked Open Data. The Kadaster manages hundreds of datasets that describe hundreds of millions of geospatial objects, including all Dutch buildings, roads, and forests. The Kadaster exposes this large collection of data to thousands of daily users that operate from within different contexts and that need to be supported in different use cases. Therefore, Kadaster must offer diverse, yet complementary, approaches for browsing and exploring the data it publishes. Specifically, it supports the following paradigms for browsing and exploring its data assets: hierarchical browsing, graph navigation, faceted browsing, and tabular browsing. These paradigms are useful for different tasks, cover different use cases, and are implemented by reusing and/or developing Open Source libraries and applications.",
  keywords = "Data browsing, Faceted browsing, Geo-spatial data, Graph navigation, Linked open data",
  author   = "W. Beek and E. Folmer",
  note     = "FOSS4G-Europe 2017 – Academic Track, 18–22 July 2017, Marne La Vallée, France",
  year     = "2017",
  month    = "7",
  doi      = "10.5194/isprs-archives-XLII-4-W2-35-2017",
  volume   = "42",
  pages    = "35--38",
  journal  = "International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",
  issn     = "1682-1750",
  number   = "4W2",
}


@article{621c186fc6a745579df4014420e89239,
  title    = "Geoyasgui: The geosparql query editor and result set visualizer",
  abstract = "The Netherlands' Cadastre, Land Registry and Mapping Agency - in short Kadaster - collects and registers administrative and spatial data on property and the rights involved. This includes for ships, aircraft and telecommunications networks. Doing so, Kadaster protects legal certainty. The Kadaster publishes many large authoritative datasets including several key registers of the Dutch Government (Topography, Addresses and Buildings). Furthermore Kadaster is also developing and maintaining the PDOK shared service, in which about 100 spatial datasets are being published in several formats, including an incredible amount of detailed geospatial objects. Geospatial objects include all plots of land, all buildings, all roads and all lampposts. These objects are spatially and/or conceptually related, but are maintained by different data curators. As a result these datasets are syntactically and architecturally disjoint, and using them together currently requires non-trivial human labor. In response to this, Kadaster is currently publishing its geo-spatial data assets as Linked Open Data. The standardized query language for Linked Open Geodata is GeoSPARQL. Unfortunately, current tooling does not support writing and evaluating GeoSPARQL queries. This paper presents GeoYASGUI, a GeoSPARQL editor and result-set viewer with IDE capabilities. GeoYASGUI is not a new software product, but an integration of and a collection of updates to existing Open Source libraries. With GeoYASGUI it becomes possible to query the rich Open Data assets of the Kadaster.",
  keywords = "GeoSPARQL, IDE, Linked open data, Open government, Semantic web",
  author   = "W. Beek and E. Folmer and L. Rietveld and J. Walker",
  year     = "2017",
  month    = "7",
  doi      = "10.5194/isprs-archives-XLII-4-W2-39-2017",
  volume   = "42",
  pages    = "39--42",
  journal  = "International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",
  issn     = "1682-1750",
  number   = "4W2",
}


@inbook{c2d7a26c23234fb98ddb139ed67389f2,
  title     = "Querying Software Architecture Knowledge as Linked Open Data",
  abstract  = "It is difficult for software professionals to find all the architectural knowledge they need from architecture documentation, and this results in wasted time and mistakes in projects.This is the case even when architecture documentation is indexed by an ontology and stored in a semantic wiki.We present a prototype tool called AK-Finder which queries architectural knowledge stored in a semantic wiki as Linked Open Data using a SPARQL endpoint.Our tool retrieves knowledge from the semantic wiki and answers questions for architectural review, design, and development activities.The tool exemplifies how systems used in the software project lifecycle can integrate and improve access to architectural knowledge in a semantic wiki.",
  keywords  = "Ontology-based software architecture documentation, Architectural knowledge retrieval, Semantic wiki, Ontology, SPARQL, Query, Endpoint, Linked Open Data",
  author    = "{de Graaf}, K.A. and Anthony Tang and Peng Liang and Ali Khalili",
  year      = "2017",
  month     = "4",
  booktitle = "International Conference on Software Architecture (ICSA)",
  publisher = "IEEE",
}


@article{c84bd6e3e40c418e85cea116fd2aacbf,
  title     = "Analyzing interactions on combining multiple clinical guidelines",
  abstract  = "Accounting for patients with multiple health conditions is a complex task that requires analysing potential interactions among recommendations meant to address each condition. Although some approaches have been proposed to address this issue, important features still require more investigation, such as (re)usability and scalability. To this end, this paper presents an approach that relies on reusable rules for detecting interactions among recommendations coming from various guidelines. It extends a previously proposed knowledge representation model (TMR) to enhance the detection of interactions and it provides a systematic analysis of relevant interactions in the context of multimorbidity. The approach is evaluated in a case study on rehabilitation of breast cancer patients, developed in collaboration with experts. The results are considered promising to support the experts in this task.",
  keywords  = "Clinical knowledge representation, Combining clinical guidelines, Comorbidity, Interactions among guidelines, Multimorbidity",
  author    = "Veruska Zamborlini and {Da Silveira}, Marcos and Cedric Pruski and {ten Teije}, Annette and Edwin Geleijn and {van der Leeden}, Marike and Martijn Stuiver and {van Harmelen}, Frank",
  year      = "2017",
  month     = "3",
  doi       = "10.1016/j.artmed.2017.03.012",
  journal   = "Artificial Intelligence in Medicine",
  issn      = "0933-3657",
  publisher = "Elsevier",
}


@article{f79133cc037d4f42a6bd4bd84d446488,
  title     = "Meta-data for a lot of LOD",
  abstract  = "This paper introduces the LOD Laundromat meta-dataset, a continuously updated RDF meta-dataset that describes the documents crawled, cleaned and (re)published by the LOD Laundromat. This meta-dataset of over 110 million triples contains structural information for more than 650,000 documents (and growing). Dataset meta-data is often not provided alongside published data, it is incomplete or it is incomparable given the way they were generated. The LOD Laundromat meta-dataset provides a wide range of structural dataset properties, such as the number of triples in LOD Laundromat documents, the average degree in documents, and the distinct number of Blank Nodes, Literals and IRIs. This makes it a particularly useful dataset for data comparison and analytics, as well as for the global study of the Web of Data. This paper presents the dataset, its requirements, and its impact.",
  author    = "Laurens Rietveld and Wouter Beek and Rinke Hoekstra and Stefan Schlobach",
  year      = "2017",
  month     = "1",
  doi       = "10.3233/SW-170256",
  volume    = "8",
  pages     = "1067--1080",
  journal   = "Semantic Web",
  issn      = "1570-0844",
  publisher = "IOS Press",
  number    = "6",
}


@misc{163c47c85bee47e38861a09ed5a1253c,
  title  = "A linked open data based system for flexible delineation of geographic areas",
  author = "A. Khalili and {van den Besselaar}, P.A.A. and {de Graaf}, K.A.",
  year   = "2017",
}


@inbook{4c7885c87ca2424b84f30e0ce5fbf6b0,
  title     = "An empirical study on how the distribution of ontologies affects reasoning on the web",
  abstract  = "The Web of Data is an inherently distributed environment where ontologies are located in (physically) remote locations and are subject to constant changes. Reasoning is affected by these changes, but the extent and significance of this dependency is not well-studied yet. To address this problem, this paper presents an empirical study on how the distribution of ontological data on the Web affects the outcome of reasoning. We study (1) to what degree datasets depend on external ontologies and (2) to what extent the inclusion of additional ontological information via IRI de-referencing and the owl:imports directive to the input datasets leads to new derivations. We based our study on many RDF datasets and on a large collection of RDFa, and JSON-LD data embedded into HTML pages. We used both Jena and Pellet in order to evaluate the results under different semantics. Our results indicate that remote ontologies are often crucial to obtain non-trivial derivations. Unfortunately, in many cases IRIs were broken and the owl:imports is rarely used. Furthermore, in some cases the inclusion of remote knowledge either did not yield any additional derivation or led to errors. Despite these cases, in general, we found that inclusion of additional ontologies via IRIs de-referencing and owl:imports directive is very effective for producing new derivations. This indicates that the two W3C standards for fetching remote ontologies have found their way into practice.",
  keywords  = "JSON-LD, OWL, RDF, RDFa, Reasoning, Web of data",
  author    = "Bazoobandi, {Hamid R.} and Jacopo Urbani and {van Harmelen}, Frank and Henri Bal",
  year      = "2017",
  doi       = "10.1007/978-3-319-68288-4_5",
  isbn      = "9783319682877",
  volume    = "10587 LNCS",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "69--86",
  booktitle = "The Semantic Web – ISWC 2017 - 16th International Semantic Web Conference, Proceedings",
}


@book{fcb11c81af894034a33d5a6d705166b0,
  title     = "ARIADNE: Final Report on Data Mining",
  abstract  = "Recent years have witnessed a growing interest from archaeological communities in Linked Data. ARIADNE, the AdvancedResearch Infrastructure for Archaeological Data set Networking in Europe, facilitates a central web portal that providesaccess to archaeological data from various sources. Parts of these data have been being published as Linked Data, andare currently available in the Linked Open Data cloud. With it, the nature of these data has shifted from unstructuredto structured. This presents new opportunities for data mining. While general-purpose software exists, recent studieshave revealed the importance of two domain-specific requirements: 1) produce interpretable results, and 2) allow trustin the underlying model. In this work, we investigate to what extend interpretable data mining can contribute to theunderstanding of linked archaeological data. A case study washeld, which involved the mining of semantic association rules over data sets of increasing levels of knowledgegranularity, followed by the qualitative evaluation of these rules by domain experts. Experiments have shown that theapproach yielded mostly plausible patterns, some of which were seen as highly relevant.",
  author    = "W.X. Wilcke and {de Boer}, V. and {van Harmelen}, F.A.H. and {de Kleijn}, M.T.M. and M. Wansleeben and Harry Dimitropoulos and Holly Wright",
  year      = "2017",
  series    = "ARIADNE",
  publisher = "Ariadne",
  number    = "D16.3",
}


@article{ba103be04d354166b43301342eb3215b,
  title     = "CEDAR: The Dutch Historical Censuses as Linked Open Data",
  abstract  = "Here, we describe the CEDAR dataset, a five-star Linked Open Data representation of the Dutch historical censuses. These were conducted in the Netherlands once every 10 years from 1795 to 1971. We produce a linked dataset from a digitized sample of 2,288 tables. It contains more than 6.8 million statistical observations about the demography, labour and housing of Dutch society in the 18th, 19th and 20th centuries. The dataset is modeled using the RDF Data Cube, Open Annotation, and PROV vocabularies. These are used to represent the multidimensionality of the data, to express rules of data harmonization, and to keep track of the provenance of all data points and their transformations, respectively. We link observations within the dataset to well known standard classification systems in social history, such as the Historical International Standard Classification of Occupations (HISCO) and the Amsterdamse Code (AC). The three contributions of the dataset are (1) an easier access to integrated census data for historical researchers; (2) richer connections to related Linked Data resources; and (3) novel concept schemes of historical relevance, like classifications of historical religions and historical house types.",
  keywords  = "census data, Linked Open Data, RDF Data Cube, Social history",
  author    = "Albert Meroño-Peñuela and Ashkan Ashkpour and Christophe Guéret and Stefan Schlobach",
  year      = "2017",
  doi       = "10.3233/SW-160233",
  volume    = "8",
  pages     = "297--310",
  journal   = "Semantic Web",
  issn      = "1570-0844",
  publisher = "IOS Press",
  number    = "2",
}


@inbook{db25399349794a8f86dce8ed0ec8ac69,
  title     = "Constructing disease-centric knowledge graphs: A case study for depression (short version)",
  abstract  = "In this paper we show how we used multiple large knowledge sources to construct a much smaller knowledge graph that is focussed on single disease (in our case major depression disorder). Such a disease-centric knowledge-graph makes it more convenient for doctors (in our case psychiatric doctors) to explore the relationship among various knowledge resources and to answer realistic clinical queries.",
  author    = "Zhisheng Huang and Jie Yang and {van Harmelen}, Frank and Qing Hu",
  year      = "2017",
  doi       = "10.1007/978-3-319-59758-4_5",
  isbn      = "9783319597577",
  volume    = "10259 LNAI",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "48--52",
  booktitle = "Artificial Intelligence in Medicine - 16th Conference on Artificial Intelligence in Medicine, AIME 2017, Proceedings",
}


@inbook{0a3a2ec2685448339a2ca93455e0e36f,
  title     = "Constructing Knowledge Graphs of Depression",
  abstract  = "Knowledge Graphs have been shown to be useful tools for integrating multiple medical knowledge sources, and to support such tasks as medical decision making, literature retrieval, determining healthcare quality indicators, co-morbodity analysis and many others. A large number of medical knowledge sources have by now been converted to knowledge graphs, covering everything from drugs to trials and from vocabularies to gene-disease associations. Such knowledge graphs have typically been generic, covering very large areas of medicine. (e.g. all of internal medicine, or arbitrary drugs, arbitrary trials, etc.). This has had the effect that such knowledge graphs become prohibitively large, hampering both efficiency for machines and usability for people. In this paper we show how we use multiple large knowledge sources to construct a much smaller knowledge graph that is focussed on single disease (in our case major depression disorder). Such a disease-centric knowledge-graph makes it more convenient for doctors (in our case psychiatric doctors) to explore the relationship among various knowledge resources and to answer realistic clinical queries (This paper is an extended version of [1].).",
  author    = "Zhisheng Huang and Jie Yang and {van Harmelen}, Frank and Qing Hu",
  year      = "2017",
  doi       = "10.1007/978-3-319-69182-4_16",
  isbn      = "9783319691817",
  volume    = "10594 LNCS",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "149--161",
  booktitle = "Health Information Science - 6th International Conference, HIS 2017, Proceedings",
}


@inbook{f0a401b2137b45e5b2d32d0149c3160a,
  title     = "Detecting New Evidences for Evidence-Based Medical Guidelines with Journal Filtering",
  abstract  = "Evidence-based medical guidelines are systematically developed recommendations with the aim to assist practitioner and patients decisions regarding appropriate health care for specific clinical circumstances, and are based on evidence described in medical research papers. Evidence-based medical guidelines should be regularly updated, such that they can serve medical practice using based on the latest medical research evidence. A usual approach to detecting new evidences is to use a set of terms which appear in a guideline conclusion or recommendation and create queries over a bio-medical search engine such as PubMed with a ranking over a selected subset of terms to search for relevant new research papers. However, the sizes of the found relevant papers are usually very large (i.e. over a few hundreds, even thousands), which results in a low precision of the search. This makes it for medical professionals quite difficult to find which papers are really interesting and useful for updating the guideline. We propose a filtering step to decrease the number of papers. More exactly we are interested in the question if we can reduce the number of papers with no or a slightly lower recall. A plausible approach is to introduce journal filtering, such that evidence appear in those top journals are preferred. In this paper, we extend our approach of detecting new papers for updating evidence-based medical guideline with a journal filtering step. We report our experiments and show that (1) the method with journal filtering can indeed gain a large reduction of the number of papers (69.73%) with a slightly lower recall (14.29%); (2) we show that the journal filtering method keeps relatively more high level evidence papers (category A) and removes all the low level evidence papers (category D).",
  author    = "Qing Hu and Zisheng Huang and {ten Teije}, Annette and {van Harmelen}, Frank",
  year      = "2017",
  doi       = "10.1007/978-3-319-55014-5_8",
  isbn      = "9783319550138",
  volume    = "10096 LNAI",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "120--132",
  editor    = "David Riaño and Richard Lenz and Manfred Reichert",
  booktitle = "Knowledge Representation for Health Care: HEC 2016 International Joint Workshop, KR4HC/ProHealth 2016, Munich, Germany, September 2, 2016, Revised Selected Papers",
}


@misc{1db5e48c46a24fdfbe5f7395eeeaccec,
  title  = "Enriching Scientometrics with Linked Data: the SMS platform",
  author = "O.A.K. Idrissou and A. Khalili and {van den Besselaar}, P.A.A. and {de Graaf}, K.A. and A. Loizou and {van Harmelen}, Frank",
  year   = "2017",
}


@inbook{e64aef34c4ba4b16841be18a17276f21,
  title     = "Evaluating research portfolios, a method and a case",
  author    = "{van den Besselaar}, P.A.A. and A. Khalili and Ulf Sandstrom",
  year      = "2017",
  booktitle = "Proceedings STI 2017 conference",
}


@misc{a6ba9f04e8e24b2c955199260697e8d0,
  title  = "Evaluating research portfolio's through ontology based text annotation",
  author = "{van den Besselaar}, P.A.A. and A. Khalili and Ulf Sandstrom",
  year   = "2017",
}


@inbook{cb46e483292d4cd99b533f825c47870e,
  title     = "Formalization and Computation of Diabetes Quality Indicators with Patient Data from a Chinese Hospital",
  abstract  = "Clinical quality indicators are tools to measure the quality of healthcare and can be classified into structure-related, process-related and outcome-related indicators. The objective of this study is to investigate whether Electronic Medical Record (EMR) data from a Chinese diabetes specialty hospital can be used for the automated computation of a set of 38 diabetes quality indicators, especially process-related indicators. The clinical quality indicator formalization (CLIF) method and tool and SNOMED CT were adopted to formalize diabetes indicators into executable queries. The formalized indicators were run on the patient data to test the feasibility of their automated computation. In this study, we successfully formalized and computed 32 of 38 quality indicators based on the EMR data. The results indicate that the data from our Chinese EMR can be used for the formalization and computation of most diabetes indicators, but that it can be improved to support the computation of more indicators.",
  keywords  = "Clinical quality, Diabetes mellitus, Electronic medical record, Formalization, Quality indicators, Secondary use of patient data",
  author    = "Haitong Liu and {ten Teije}, Annette and Kathrin Dentler and Jingdong Ma and Shijing Zhang",
  year      = "2017",
  doi       = "10.1007/978-3-319-55014-5_2",
  isbn      = "9783319550138",
  volume    = "10096 LNAI",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "23--35",
  editor    = "David Riaño and Richard Lenz and Manfred Reichert",
  booktitle = "Knowledge Representation for Health Care: HEC 2016 International Joint Workshop, KR4HC/ProHealth 2016, Munich, Germany, September 2, 2016, Revised Selected Papers",
}


@inbook{ea956dc374914693bcdda59a8557174d,
  title     = "Fostering Serendipitous Knowledge Discovery using an Adaptive Multigraph-based Faceted Browser",
  abstract  = "Serendipity, the art of making an unsought finding plays also an important role in the emerging field of data science, allowing the discovery of interesting and valuable facts not initially sought for. Previous research has extracted many serendipity-fostering patterns applicable to digital data-driven systems. Linked Open Data (LOD) on the Web which is powered by the Follow-Your-Nose effect, provides already a rich source for serendipity. The serendipity most often takes place when browsing data. Therefore, flexible and intuitive browsing user interfaces which support serendipity triggers such as enigmas, anomalies and novelties, can increase the likelihood of serendipity on LOD. In this work, we propose a set of serendipity-fostering design features supported by an adaptive multigraph-based faceted browsing interface to catalyze serendipity on Semantic Web and LOD environments.",
  author    = "Ali Khalili and {van Andel}, Pek and {van den Besselaar}, Peter and {de Graaf}, {Klaas Andries}",
  year      = "2017",
  doi       = "10.1145/3148011.3148037",
  isbn      = "978-1-4503-5553-7/17/12",
  booktitle = "The Ninth International Conference on Knowledge Capture: K-CAP 2017",
  publisher = "ACM",
}


@misc{68a3538d56a949669e47b156b3b5b212,
  title  = "From university rankings to portfolio analysis and evaluation",
  author = "{van den Besselaar}, P.A.A. and A. Khalili and Ulf Sandstrom",
  year   = "2017",
}


@inbook{68afba2d8f25485fa3ade85bf8a66553,
  title     = "Generalizing the Detection of Clinical Guideline Interactions Enhanced with LOD",
  author    = "{Carretta Zamborlini}, Veruska and Rinke Hoekstra and {Da Silveira}, Marcos and Cedric Pruski and {ten Teije}, Annette and {van Harmelen}, Frank",
  year      = "2017",
  doi       = "10.1007/978-3-319-54717-6_20",
  isbn      = "978-3-319-54717-6",
  pages     = "360--386",
  editor    = "Ana Fred and Hugo Gamboa",
  booktitle = "Biomedical Engineering Systems and Technologies: 9th International Joint Conference, BIOSTEC 2016, Rome, Italy, February 21--23, 2016, Revised Selected Papers",
  publisher = "Springer International Publishing Switzerland",
}


@misc{a48b60672af940c4a911566a973f08aa,
  title  = "Improving social research using heterogeneous data: the SMS platform",
  author = "{van den Besselaar}, P.A.A. and A. Khalili and {van Harmelen}, Frank and {de Graaf}, K.A.",
  year   = "2017",
}


@inbook{336aadbd02e346dcadb4630b4fe216fa,
  title     = "Is my:sameAs the same as your:sameAs?",
  abstract  = "Linking between entities in different datasets is a crucial element of the Semantic Web architecture, since those links allow us to integrate datasets without having to agree on a uniform vocabulary. However, it is widely acknowledged that the owl:sameAs construct is too blunt a tool for this purpose. It entails full equality between two resources independent of context. But whether or not two resources should be considered equal depends not only on their intrinsic properties, but also on the purpose or task for which the resources are used. We present a system for constructing contextspecific equality links. In a first step, our system generates a set of probable links between two given datasets. These potential links are decorated with rich metadata describing how, why, when and by whom they were generated. In a second step, a user then selects the links which are suited for the current task and context, constructing a context-specific “Lenticular Lens”. Such lenses can be combined using operators such as union, intersection, difference and composition. We illustrate and validate our approach with",
  author    = "Idrissou, {Al Koudous} and Rinke Hoekstra and {van Harmelen}, Frank and Ali Khalili and {van den Besselaar}, Peter",
  year      = "2017",
  booktitle = "The ninth international conference on knowledge capture: k-cap 2017",
}


@inbook{5d2c3965da59435382ceb720c06021ad,
  title     = "Knowledge-Driven Paper Retrieval to Support Updating of Clinical Guidelines",
  author    = "{Carretta Zamborlini}, Veruska and Qing Hu and Z. Huang and {Da Silveira}, Marcos and Cedric Pruski and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H.",
  year      = "2017",
  doi       = "10.1007/978-3-319-55014-5_5",
  isbn      = "978-3-319-55014-5",
  pages     = "71--89",
  editor    = "David Riaño and Richard Lenz and Manfred Reichert",
  booktitle = "Knowledge Representation for Health Care: HEC 2016 International Joint Workshop, KR4HC/ProHealth 2016, Munich, Germany, September 2, 2016, Revised Selected Papers",
  publisher = "Springer International Publishing Switzerland",
}


@misc{5f262cbfd4654e7da5153fc5a03cdc6a,
  title    = "Knowledge Representation for Clinical Guidelines: with applications to Multimorbidity Analysis and Literature Search",
  keywords = "Knowledge Representation, Semantic Web, Clinical Guidelines, Multimorbidity, Comorbidity, Guideline Update",
  author   = "{Carretta Zamborlini}, V.",
  year     = "2017",
  school   = "Vrije Universiteit Amsterdam",
}


@inbook{d84639693b2349e3b8afcf9dacfd35d0,
  title     = "Linked Data Reactor: Towards Data-aware User Interfaces",
  author    = "Ali Khalili and {de Graaf}, {Klaas Andries}",
  year      = "2017",
  booktitle = "SEMANTiCS 2017",
}


@inbook{488e681637ff47f4bf544f43c457ec82,
  title     = "LOD-a-lot: A queryable dump of the LOD cloud",
  abstract  = "LOD-a-lot democratizes access to the Linked Open Data (LOD) Cloud by serving more than 28 billion unique triples from 650, K datasets over a single self-indexed file. This corpus can be queried online with a sustainable Linked Data Fragments interface, or downloaded and consumed locally: LOD-a-lot is easy to deploy and demands affordable resources (524, GB of disk space and 15.7, GB of RAM), enabling Web-scale repeatable experimentation and research even by standard laptops.",
  author    = "Fernández, {Javier D.} and Wouter Beek and Martínez-Prieto, {Miguel A.} and Mario Arias",
  year      = "2017",
  doi       = "10.1007/978-3-319-68204-4_7",
  isbn      = "9783319682037",
  volume    = "10588 LNCS",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "75--83",
  booktitle = "The Semantic Web – ISWC 2017 - 16th International Semantic Web Conference, Proceedings",
}


@inbook{97e7492b3f604d8597c2ea8af941a33e,
  title     = "LOD lab: Scalable linked data processing",
  abstract  = "With tens if not hundreds of billions of logical statements, the Linked Open Data (LOD) is one of the biggest knowledge bases ever built. As such it is a gigantic source of information for applications in various domains, but also given its size an ideal test-bed for knowledge representation and reasoning, heterogeneous nature, and complexity. However, making use of this unique resource has proven next to impossible in the past due to a number of problems, including data collection, quality, accessibility, scalability, availability and findability. The LOD Laundromat and LOD Lab are recent infrastructures that addresses these problems in a systematic way, by automatically crawling, cleaning, indexing, analysing and republishing data in a unified way. Given a family of simple tools, LOD Lab allows researchers to query, access, analyse and manipulate hundreds of thousands of data documents seamlessly, e.g. facilitating experiments (e.g. for reasoning) over hundreds of thousands of (possibly integrated) datasets based on content and meta-data. This chapter provides the theoretical basis and practical skills required for making ideal use of this large scale experimental platform. First we study the problems that make it so hard to work with Semantic Web data in its current form. We’ll also propose generic solutions and introduce the tools the reader needs to get started with their own experiments on the LOD Cloud.",
  author    = "Wouter Beek and Laurens Rietveld and F. Ilievski and Stefan Schlobach",
  year      = "2017",
  doi       = "10.1007/978-3-319-49493-7_4",
  isbn      = "9783319494920",
  volume    = "9885 LNCS",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "124--155",
  booktitle = "Reasoning Web: Logical Foundation of Knowledge Graph Construction and Query Answering - 12th International Summer School 2016, Tutorial Lectures",
}


@misc{c38c16616d624e5799d49a5241a73f30,
  title    = "Logics for causal inference under uncertainty",
  keywords = "causality, causal discovery, probabilistic logics, structure learning, probabilistic soft logic",
  author   = "S. Magliacane",
  year     = "2017",
  school   = "Vrije Universiteit Amsterdam",
}


@book{1539ee1ae92840a1beb2609e405321d0,
  title     = "Opening the SMS platform to users: Deliverable D7.2 - RISIS project",
  abstract  = "In this deliverable we describe the SMS (Semantically Mapping Science) data integration platform (http://sms.risis.eu), the technical core within the RISIS data infrastructure for Science, Technology and Innovation Studies (STI). The aim of the platform is to produce richer data to be used in social research – through the integration of heterogeneous datasets, ranging from tabular statistical data to unstructured data found on the Web. We outline the platform’s architecture and functions. There arealso some example use cases mentioned to show how the platform enables data integration in practice.",
  author    = "{van den Besselaar}, P.A.A. and A. Khalili and {de Graaf}, K.A. and O.A.K. Idrissou and {van Harmelen}, Frank",
  year      = "2017",
  publisher = "RISIS",
}


@book{beaec0ed935045dba04a24fd27207fae,
  title     = "Report on the Finalization of Geocoding and Geo-clustering Activities and Preliminary Analysis of European Innovation Dynamics: RISIS deliverable D9.3",
  author    = "Eran Leck and Daphne Getz and A. Khalili and {van den Besselaar}, P.A.A. and et. al.",
  year      = "2017",
  publisher = "RISIS",
}


@inbook{1ea4f94b25b648a69d0416b2d937605c,
  title     = "Semantically Mapping Science (SMS) Platform",
  author    = "Ali Khalili and {van den Besselaar}, Peter and {Al Koudous}, Idrissou and {de Graaf}, {Klaas Andries} and {van Harmelen}, Frank",
  year      = "2017",
  pages     = "1--6",
  booktitle = "SemSci 2017: Enabling Open Semantic Science",
  publisher = "CEUR Workshop Proceedings",
}


@misc{61aa29f69da34a6f832f7f20ba0ac5ea,
  title  = "SMS: a platform for linking and enriching data for science and innovation studies: An example",
  author = "{van den Besselaar}, P.A.A. and A. Khalili and O.A.K. Idrissou and {de Graaf}, K.A. and {van Harmelen}, Frank",
  year   = "2017",
}


@article{60914ca9ba2543df8bf408a89914faa8,
  title     = "Understanding knowledge networks",
  abstract  = "The emergence of Linked Open Data (LOD) enables data on the Web to have a well defined structure and thereby to represent and in? terlink information from different sources and application areas. This web of data is a complex socially created network, where concepts and rela? tions are connected in intricate ways, collectively forming a network of knowledge. These data are published in a decentralized fashion and they stem from different sources, have different types of relationships, and use different terminologies, ontologies and meta models. While this so-called LOD cloud has become a very valuable resource, we know only very little about the general structural properties of the contained data, which im? pedes our ability to use and organize this resource in an efficient and accu? rate manner. The objective of this paper is to provide a basic understand? ing of LOD from the point of view of network structures. We analyze LOD networks with respect to fundamental network properties such as degree distribution and clustering. Using these metrics, we compare our results to non-LOD networks, such as email, Web, and protein networks, that have been reported in the literature. Our results show that the LOD cloud ex? hibits a broad variety of different network structures, consistent with the diversity found in other types of networks.",
  keywords  = "Knowledge network, Linked data, Network structure",
  author    = "Krishna Mangaladevi and Wouter Beek and Tobias Kuhn",
  year      = "2017",
  volume    = "1946",
  pages     = "38--49",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@inbook{8c1aa17784b541469b5e2520b80a7746,
  title     = "Visualizing Linked Data as Habitable Cities",
  author    = "{de Graaf}, {Klaas Andries} and Ali Khalili",
  year      = "2017",
  booktitle = "3rd Workshop on Visualization and Interaction for Ontologies and Linked Data (VOILA)",
}


@inbook{18f7f3aa55944236966cf716a46f80d6,
  title     = "WYSIWYQ -- What You See Is What You Query",
  author    = "Ali Khalili and Albert Meroño-Peñuela",
  year      = "2017",
  booktitle = "Visualization and Interaction for Ontologies and Linked Data (VOILA) 2017",
}


@article{16800c26e53243ca86fbe34eaf7de0c8,
  title   = "基于知识图谱的精细化工辅助研发平台",
  author  = "Zhisheng Huang and Jinguang Gu and Bin Peng",
  year    = "2017",
  volume  = "3",
  pages   = "43--55",
  journal = "Journal of Technology Intelligence Engineering",
  issn    = "2095-915X",
  number  = "1",
}


@article{82074b235a8a424bb7645c2c2887f457,
  title     = "The YASGUI family of SPARQL clients",
  abstract  = "The size and complexity of the Semantic Web and its technology stack makes it difficult to query. Access to Linked Data could be greatly facilitated if it were supported by a tool with a strong focus on usability. In this paper we present the YASGUI family of SPARQL clients, a continuation of the YASGUI tool introduced more than two years ago. The YASGUI family of SPARQL clients enables publishers to improve ease of access for their SPARQL endpoints, and gives consumers of Linked Data a robust, feature-rich and user friendly SPARQL editor. We show that the YASGUI family had significant impact on the landscape of Linked Data management: YASGUI components are integrated in state-of-the-art triple-stores and Linked Data applications, and used as front-end by a large number of Linked Data publishers. Additionally, we show that the YASGUI web service - which provides access to any SPARQL endpoint - has a large and growing user base amongst Linked Data consumers.",
  keywords  = "data publishing, Linked Data, query formulation, SPARQL",
  author    = "Laurens Rietveld and Rinke Hoekstra",
  year      = "2016",
  month     = "12",
  doi       = "10.3233/SW-150197",
  volume    = "8",
  pages     = "373--383",
  journal   = "Semantic Web",
  issn      = "1570-0844",
  publisher = "IOS Press",
  number    = "3",
}


@inbook{6d083df218bb468baf4d48b4dc55be0b,
  title     = "A Monitoring System for the Safety of Building Structure Based on W2T Methodology",
  abstract  = "With the development of the Internet of things, monitoring systems for the safety of building structure (SBS) provide people with the important data about the main supporting points in the buildings. More and more data give the engineers an overload work problem, which can be solved by a systematic method making these monitoring systems more reliable, efficient and intelligent. Under the framework of the Wisdom Web of Things (W2T), we design a monitoring system for the SBS, by using the semantic technology. This system establishes a data cycle among the physical world (buildings), the social world (humans) and the cyber world (computers), and provides various services in the monitoring process to alleviate the engineers’ workload. In this system, the sensors which are connected via cable or wireless way, are used to monitor the different parameters of building structure. The semantic data can be obtained and represented by RDF to describe the meanings of sensor data, and can provide the application background for users. LarKC, a platform for scalable semantic data processing, is used for semantic querying about the data. Based on this uniform representation of data and semantic processing, intelligent services can be provided by the effective data analysis. This provides the possibility to integrate all of the monitoring systems for the safety of building structure in urban computing.",
  author    = "Haiyuan Wang and Zhisheng Huang and Ning Zhong and Jiajin Huang and Yuzhong Han and Feng Zhang",
  year      = "2016",
  month     = "11",
  doi       = "10.1007/978-3-319-44198-6",
  isbn      = "978-3-319-44196-2",
  pages     = "323--335",
  editor    = "Ning Zhong and Jianhua Ma and Jiming Liu and Runhe Huang and Xiaohui Tao",
  booktitle = "Wisdom Web of Things",
  publisher = "Springer",
}


@inbook{66b040c64f254e379e952c8ef535213a,
  title     = "Suitable Route Recommendation Inspired by Cognition",
  abstract  = "With the increasing popularity of mobile phones, large amounts of real and reliable mobile phone data are being generated every day. These mobile phone data represent the practical travel routes of users and imply the intelligence of them in selecting a suitable route. Usually, an experienced user knows which route is congested in a specified period of time but unblocked in another period of time. Moreover, a route used frequently and recently by a user is usually the suitable one to satisfy the user’s needs. ACT-R (Adaptive Control of Thought-Rational) is a computational cognitive architecture, which provides a good framework to understand the principles and mechanisms of information organization, retrieval and selection in human memory. In this chapter, we employ ACT-R to model the process of selecting a suitable route of users. We propose a cognition-inspired route recommendation method to mine the intelligence of users in selecting a suitable route, evaluate the suitability of the routes, and recommend an ordered list of routes for subscribers. Experiments show that it is effective and feasible to recommend the suitable routes inspired by cognition.",
  author    = "Hui Wang and Jiajin Huang and Erzhong Zhou and Zhisheng Huang and Ning Zhong",
  year      = "2016",
  month     = "11",
  doi       = "10.1007/978-3-319-44198-6_13",
  isbn      = "978-3-319-44196-2",
  pages     = "303--322",
  editor    = "Ning Zhong and Jianhua Ma and Jiming Liu and Runhe Huang and Xiaohui Tao",
  booktitle = "Wisdom Web of Things",
  publisher = "Springer",
}


@inbook{cdce099b6d134e23a733033930db4362,
  title     = "WaaS—Wisdom as a Service",
  abstract  = "An emerging hyper-world encompasses all human activities in a social-cyber-physical space. Its power derives from the Wisdom Web of Things (W2T) cycle, namely, “from things to data, information, knowledge, wisdom, services, humans, and then back to things.” The W2T cycle leads to a harmonious symbiosis among humans, computers and things, which can be constructed by large-scale converging of intelligent information technology applications with an open and interoperable architecture. The recent advances in cloud computing, the Internet/Web of Things, big data and other research fields have provided just such an open system architecture with resource sharing/services. The next step is therefore to develop an open and interoperable content architecture with intelligence sharing/services for the organization and transformation in the “data, information, knowledge and wisdom (DIKW)” hierarchy. This chapter introduces Wisdom as a Service (WaaS) as a content architecture based on the “paying only for what you use” IT business trend. The WaaS infrastructure, WaaS economics, and the main challenges in WaaS research and applications are discussed. A case study is described to demonstrate the usefulness and significance of WaaS. Relying on the clouds (cloud computing), things (Internet of Things) and big data, WaaS provides a practical approach to realize the W2T cycle in the hyper-world for the coming age of ubiquitous intelligent IT applications.",
  author    = "Jianhui Chen and Jianhua Ma and Ning Zhong and Yiyu Yao and Jiming Liu and Runhe Huang and Wenbin Li and Zhisheng Huang and Yang Gao",
  year      = "2016",
  month     = "11",
  doi       = "10.1007/978-3-319-44198-6_2",
  isbn      = "978-3-319-44196-2",
  pages     = "27--46",
  editor    = "Ning Zhong and Jianhua Ma and Jiming Liu and Runhe Huang and Xiaohui Tao",
  booktitle = "Wisdom Web of Things",
}


@article{22ff35d5e5d7447b852af14f2bf7aba5,
  title   = "Linked Data for Digital History: Lessons Learned from Three Case Studies",
  author  = "{de Boer}, V. and {Merono Penuela}, A. and C.J. Ockeloen",
  year    = "2016",
  month   = "10",
  pages   = "139--162",
  journal = "Anejos de la Revista de Historiografía",
  number  = "4",
}


@article{a4774a07abff41b98843780ec580fafc,
  title     = "Special Issue: Modern Hardware",
  author    = "Peter Boncz and Wolfgang Lehner and Thomas Neumann",
  year      = "2016",
  month     = "10",
  doi       = "10.1007/s00778-016-0440-7",
  volume    = "25",
  pages     = "623--624",
  journal   = "VLDB Journal",
  issn      = "1066-8888",
  publisher = "Springer Verlag",
  number    = "5",
}


@article{6e2aeddc45c14074974ab9577f40e126,
  title     = "Interview with Frank van Harmelen on {"}Linked Data and Business Information Systems{"}",
  author    = "Soeren Auer and {van Harmelen}, Frank",
  year      = "2016",
  month     = "10",
  doi       = "10.1007/s12599-016-0448-y",
  volume    = "58",
  pages     = "371--373",
  journal   = "Business & Information Systems Engineering",
  issn      = "2363-7005",
  publisher = "Springer Gabler",
  number    = "5",
}


@article{8fcbb631b38f4a52999735c46e47434a,
  title     = "Predictive modeling of colorectal cancer using a dedicated pre-processing pipeline on routine electronic medical records",
  abstract  = "Over the past years, research utilizing routine care data extracted from Electronic Medical Records (EMRs) has increased tremendously. Yet there are no straightforward, standardized strategies for pre-processing these data. We propose a dedicated medical pre-processing pipeline aimed at taking on many problems and opportunities contained within EMR data, such as their temporal, inaccurate and incomplete nature. The pipeline is demonstrated on a dataset of routinely recorded data in general practice EMRs of over 260,000 patients, in which the occurrence of colorectal cancer (CRC) is predicted using various machine learning techniques (i.e., CART, LR, RF) and subsets of the data. CRC is a common type of cancer, of which early detection has proven to be important yet challenging. The results are threefold. First, the predictive models generated using our pipeline reconfirmed known predictors and identified new, medically plausible, predictors derived from the cardiovascular and metabolic disease domain, validating the pipeline's effectiveness. Second, the difference between the best model generated by the data-driven subset (AUC 0.891) and the best model generated by the current state of the art hypothesis-driven subset (AUC 0.864) is statistically significant at the 95% confidence interval level. Third, the pipeline itself is highly generic and independent of the specific disease targeted and the EMR used. In conclusion, the application of established machine learning techniques in combination with the proposed pipeline on EMRs has great potential to enhance disease prediction, and hence early detection and intervention in medical practice.",
  keywords  = "Colorectal cancer, Data mining, Data processing, Electronic medical records, Machine learning",
  author    = "Reinier Kop and Mark Hoogendoorn and {ten Teije}, Annette and Büchner, {Frederike L.} and Pauline Slottje and Moons, {Leon M G} and Numans, {Mattijs E.}",
  year      = "2016",
  month     = "9",
  doi       = "10.1016/j.compbiomed.2016.06.019",
  volume    = "76",
  pages     = "30--38",
  journal   = "Computers in Biology and Medicine",
  issn      = "0010-4825",
  publisher = "Elsevier Limited",
}


@inbook{820af4d900b54d74bd8f9f6287d4cd14,
  title     = "Data Blocks: Hybrid OLTP and OLAP on compressed storage using both vectorization and compilation",
  abstract  = "This work aims at reducing the main-memory footprint in high performance hybrid OLTP&OLAP databases, while retaining high query performance and transactional throughput. For this purpose, an innovative compressed columnar storage format for cold data, called Data Blocks is introduced. Data Blocks further incorporate a new light-weight index structure called Positional SMA that narrows scan ranges within Data Blocks even if the entire block cannot be ruled out. To achieve highest OLTP performance, the compression schemes of Data Blocks are very light-weight, such that OLTP transactions can still quickly access individual tuples. This sets our storage scheme apart from those used in specialized analytical databases where data must usually be bit-unpacked. Up to now, high-performance analytical systems use either vectorized query execution or {"}just-in-time{"} (JIT) query compilation. The fine-grained adaptivity of Data Blocks necessitates the integration of the best features of each approach by an interpreted vectorized scan subsystem feeding into JIT-compiled query pipelines. Experimental evaluation of HyPer, our full-edged hybrid OLTP&OLAP database system, shows that Data Blocks accelerate performance on a variety of query workloads while retaining high transaction throughput.",
  author    = "Harald Lang and Tobias Mühlbauer and Florian Funke and Peter Boncz and Thomas Neumann and Alfons Kemper",
  year      = "2016",
  month     = "6",
  doi       = "10.1145/2882903.2882925",
  volume    = "26-June-2016",
  pages     = "311--326",
  booktitle = "SIGMOD 2016 - Proceedings of the 2016 International Conference on Management of Data",
  publisher = "Association for Computing Machinery (ACM)",
}


@inbook{81f07f72c43d466a8c80eb41d581cc13,
  title     = "VectorH: Taking SQL-on-Hadoop to the next level",
  abstract  = "Actian Vector in Hadoop (VectorH for short) is a new SQL-on-Hadoop system built on top of the fast Vectorwise analytical database system. VectorH achieves fault tolerance and storage scalability by relying on HDFS, and extends the state-of-the-art in SQL-on-Hadoop systems by instrumenting the HDFS replication policy to optimize read locality. VectorH integrates with YARN for workload management, achieving a high degree of elasticity. Even though HDFS is an append-only file-system, and VectorH supports (update-averse) ordered tables, trickle updates are possible thanks to Positional Delta Trees (PDTs), a diffferential update structure that can be queried efficiently. We describe the changes made to single-server Vectorwise to turn it into a Hadoop-based MPP system, encompassing workload management, parallel query optimization and execution, HDFS storage, transaction processing and Spark integration. We evaluate VectorH against HAWQ, Impala, SparkSQL and Hive, showing orders of magnitude better performance.",
  author    = "Andrei Costea and Adrian Ionescu and Bogdan Raducanu and Michał Świtakowski and Cristian Bârca and Juliusz Sompolski and Alicja Łuszczak and Michał Szafrański and {De Nijs}, Giel and Peter Boncz",
  year      = "2016",
  month     = "6",
  doi       = "10.1145/2882903.2903742",
  volume    = "26-June-2016",
  pages     = "1105--1117",
  booktitle = "SIGMOD 2016 - Proceedings of the 2016 International Conference on Management of Data",
  publisher = "Association for Computing Machinery (ACM)",
}


@inbook{4c8c9c9bb95a4376a43d9eaa67f7db00,
  title     = "Powerful and efficient bulk shortest-path queries: Cypher language extension & Giraph implementation",
  author    = "Peter Rutgers and Claudio Martella and Spyros Voulgaris and Peter Boncz",
  year      = "2016",
  month     = "6",
  doi       = "10.1145/2960414.2960420",
  volume    = "24-June-2016",
  booktitle = "ACM International Conference Proceeding Series",
}


@article{d35101d170504aa89903f06633f2236d,
  title     = "ClioPatria: A SWI-prolog infrastructure for the semantic web",
  abstract  = "ClioPatria is a comprehensive semantic web development framework based on SWI-Prolog. SWI-Prolog provides an efficient C-based main-memory RDF store that is designed to cooperate naturally and efficiently with Prolog, realizing a flexible RDF-based environment for rule based programming. ClioPatria extends this core with a SPARQL and LOD server, an extensible web frontend to manage the server, browse the data, query the data using SPARQL and Prolog and a Git-based plugin manager. The ability to query RDF using Prolog provides query composition and smooth integration with application logic. ClioPatria is primarily positioned as a prototyping platform for exploring novel ways of reasoning with RDF data. It has been used in several research projects in order to perform tasks such as data integration and enrichment and semantic search.",
  keywords  = "Logic programming, Semantic Web framework, Triple store",
  author    = "Jan Wielemaker and Wouter Beek and Michiel Hildebrand and {Van Ossenbruggen}, Jacco",
  year      = "2016",
  month     = "6",
  doi       = "10.3233/SW-150191",
  volume    = "7",
  pages     = "529--541",
  journal   = "Semantic Web",
  issn      = "1570-0844",
  publisher = "IOS Press",
  number    = "5",
}


@article{e1ddb0419e3f41a181b986d27ca0553e,
  title     = "Bitwise dimensional co-clustering for analytical workloads",
  abstract  = "Analytical workloads in data warehouses often include heavy joins where queries involve multiple fact tables in addition to the typical star-patterns, dimensional grouping and selections. In this paper we propose a new processing and storage framework called bitwise dimensional co-clustering (BDCC) that avoids replication and thus keeps updates fast, yet is able to accelerate all these foreign key joins, efficiently support grouping and pushes down most dimensional selections. The core idea of BDCC is to cluster each table on a mix of dimensions, each possibly derived from attributes imported over an incoming foreign key and this way creating foreign key connected tables with partially shared clusterings. These are later used to accelerate any join between two tables that have some dimension in common and additionally permit to push down and propagate selections (reduce I/O) and accelerate aggregation and ordering operations. Besides the general framework, we describe an algorithm to derive such a physical co-clustering database automatically and describe query processing and query optimization techniques that can easily be fitted into existing relational engines. We present an experimental evaluation on the TPC-H benchmark in the Vectorwise system, showing that co-clustering can significantly enhance its already high performance and at the same time significantly reduce the memory consumption of the system.",
  keywords  = "Clustering, Data warehouse, Database design, Indexing, OLAP, Query processing, Sandwich operators, Storage",
  author    = "Stephan Baumann and Peter Boncz and Sattler, {Kai Uwe}",
  year      = "2016",
  month     = "6",
  doi       = "10.1007/s00778-015-0417-y",
  volume    = "25",
  pages     = "291--316",
  journal   = "VLDB Journal",
  issn      = "1066-8888",
  publisher = "Springer Verlag",
  number    = "3",
}


@article{283d4b8ef23a46bb9ecc246cfc723196,
  title     = "How organisation of architecture documentation affects architectural knowledge retrieval",
  abstract  = "A common approach to software architecture documentation in industry projects is the use of file-based documents. This approach offers a single-dimensional arrangement of the architectural knowledge. Knowledge retrieval from file-based architecture documentation is efficient if the organisation of knowledge supports the needs of the readers; otherwise it can be difficult. In this paper, we compare the organisation and retrieval of architectural knowledge in a file-based documentation approach and an ontology-based documentation approach. The ontology-based approach offers a multi-dimensional organisation of architectural knowledge by means of a software ontology and semantic wiki, whereas file-based documentation typically uses hierarchical organisation by directory structure and table of content. We conducted case studies in two companies to study the efficiency and effectiveness of retrieving architectural knowledge from the different organisations of knowledge. We found that the use of better knowledge organisation correlates with the efficiency and effectiveness of AK retrieval. Professionals who used the knowledge organisation found this beneficial.",
  keywords  = "Software architecture documentation, software architectural knowledge, architectural knowledge retrieval, software ontologies, semantic wiki, ontology-based documentation",
  author    = "{de Graaf}, K.A. and P. Liang and A. Tang and J.C. Vliet",
  year      = "2016",
  month     = "6",
  doi       = "10.1016/j.scico.2015.10.014",
  volume    = "121",
  pages     = "75--99",
  journal   = "Science of Computer Programming",
  issn      = "0167-6423",
  publisher = "Elsevier",
}


@article{9254330a91e749cdb89b65db174225b4,
  title     = "LOD Laundromat: Why the Semantic Web needs centralization (even if we don't like it)",
  abstract  = "LOD Laundromat poses a centralized solution for today's Semantic Web problems. This approach adheres more closely to the original vision of a Web of Data, providing uniform access to a large and ever-increasing subcollection of the LOD Cloud.",
  keywords  = "Internet/Web technologies, Linked Data, Linked Open Data, LOD, Semantic Web",
  author    = "Wouter Beek and Laurens Rietveld and Stefan Schlobach and {van Harmelen}, Frank",
  year      = "2016",
  month     = "3",
  doi       = "10.1109/MIC.2016.43",
  volume    = "20",
  pages     = "78--81",
  journal   = "IEEE Internet Computing",
  issn      = "1089-7801",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  number    = "2",
}


@inbook{e7e1e9893fe14b078edbf1779f4b1c68,
  title     = "A Contextualised Semantics for owl: sameAs",
  abstract  = "Identity relations are at the foundation of the Semantic Web and the Linked Data Cloud. In many instances the classical interpretation of identity is too strong for practical purposes. This is particularly the case when two entities are considered the same in some but not all contexts. Unfortunately, modeling the specific contexts in which an identity relation holds is cumbersome and, due to arbitrary reuse and the Open World Assumption, it is impossible to anticipate all contexts in which an entity will be used. We propose an alternative semantics for owl:sameAs that partitions the original relation into a hierarchy of subrelations. The subrelation to which an identity statement belongs depends on the dataset in which the statement occurs. Adding future assertions may change the subrelation to which an identity statement belongs, resulting in a context-dependent and non-monotonic semantics. We show that this more fine-grained semantics is better able to characterize the actual use of owl:sameAs as observed in Linked Open Datasets.",
  author    = "W.G.J. Beek and K.S. Schlobach and {van Harmelen}, F.A.H.",
  year      = "2016",
  doi       = "10.1007/978-3-319-34129-3_25",
  isbn      = "9783319341286",
  volume    = "9678",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "405--419",
  booktitle = "The Semantic Web. Latest Advances and New Domains - 13th International Conference, ESWC 2016, Heraklion, Crete, Greece, May 29 - June 2, 2016, Proceedings",
}


@inbook{3545ab263cc3471aace89f0a628dd12d,
  title     = "Adaptive delineation of functional urban areas using Linked Open Data.",
  author    = "A. Khalili and {van den Besselaar}, P.A.A.",
  year      = "2016",
  pages     = "72",
  booktitle = "RISIS Deliverable 9.2",
  publisher = "SNI",
}


@inbook{cec94fa27b524331a0c5f29e4d4c28c9,
  title     = "Adaptive Linked Data-Driven Web Components: Building Flexible and Reusable Semantic Web Interfaces - Building Flexible and Reusable Semantic Web Interfaces",
  abstract  = "Due to the increasing amount of Linked Data openly published on the Web, user-facing Linked Data Applications (LDAs) are gaining momentum. One of the major entrance barriers for Web developers to contribute to this wave of LDAs is the required knowledge of Semantic Web (SW) technologies such as the RDF data model and SPARQL query language. This paper presents an adaptive component-based approach together with its open source implementation for creating flexible and reusable SW interfaces driven by Linked Data. Linked Data-driven (LD-R) Web components abstract the complexity of the underlying SW technologies in order to allow reuse of existing Web components in LDAs, enabling Web developers who are not experts in SW to develop interfaces that view, edit and browse Linked Data. In addition to the modularity provided by the LD-R components, the proposed RDF-based configuration method allows application assemblers to reshape their user interface for different use cases, by either reusing existing shared configurations or by creating their proprietary configurations.",
  author    = "Ali Khalili and A. Loizou and {van Harmelen}, F.A.H.",
  year      = "2016",
  doi       = "10.1007/978-3-319-34129-3_41",
  isbn      = "978-3-319-34128-6",
  volume    = "9678",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer",
  pages     = "677--692",
  booktitle = "The Semantic Web. Latest Advances and New Domains - 13th International Conference, ESWC 2016, Heraklion, Crete, Greece, May 29 - June 2, 2016, Proceedings",
}


@misc{ca63a27c612148d2bf29a6b1283c8e58,
  title  = "A Deep Neural Network for Link Prediction on Knowledge Graphs",
  author = "W.X. Wilcke and {de Boer}, V. and {van Harmelen}, F.A.H. and {de Kleijn}, M.T.M.",
  year   = "2016",
}


@article{e3f25d50b5a44b00a2906e5dd944c2a9,
  title     = "An ecosystem for Linked Humanities Data",
  abstract  = "The main promise of the digital humanities is the ability to perform scholar studies at a much broader scale, and in a much more reusable fashion. The key enabler for such studies is the availability of sufficiently well described data. For the field of socio-economic history, data usually comes in a tabular form. Existing efforts to curate and publish datasets take a top-down approach and are focused on large collections. This paper presents QBer and the underlying structured datahub, which address the long tail of research data by catering for the needs of individual scholars. QBer allows researchers to publish their (small) datasets, link them to existing vocabularies and other datasets, and thereby contribute to a growing collection of interlinked datasets. We present QBer, and evaluate our first results by showing how our system facilitates two use cases in socio-economic history.",
  keywords  = "digital humanities, structured data, linked data, QBer",
  author    = "Rinke Hoekstra and Albert Meroño-Peñuela and Kathrin Dentler and Auke Rijpma and Richard Zijdeman and Ivo Zandhuis",
  year      = "2016",
  doi       = "10.1007/978-3-319-47602-5_54",
  volume    = "1608",
  pages     = "85--96",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@inbook{bd128ebb57df4bfdb31cc455c81cdbc6,
  title     = "Are names meaningful? Quantifying social meaning on the semantic web",
  abstract  = "According to its model-theoretic semantics, Semantic Web IRIs are individual constants or predicate letters whose names are chosen arbitrarily and carry no formal meaning. At the same time it is a well-known aspect of Semantic Web pragmatics that IRIs are often constructed mnemonically, in order to be meaningful to a human interpreter. The latter has traditionally been termed ‘social meaning’, a concept that has been discussed but not yet quantitatively studied by the Semantic Web community. In this paper we use measures of mutual information content and methods from statistical model learning to quantify the meaning that is (at least) encoded in Semantic Web names. We implement the approach and evaluate it over hundreds of thousands of datasets in order to illustrate its efficacy. Our experiments confirm that many Semantic Web names are indeed meaningful and, more interestingly, we provide a quantitative lower bound on how much meaning is encoded in names on a per-dataset basis. To our knowledge, this is the first paper about the interaction between social and formal meaning, as well as the first paper that uses statistical model learning as a method to quantify meaning in the Semantic Web context. These insights are useful for the design of a new generation of Semantic Web tools that take such social meaning into account.",
  author    = "{de Rooij}, Steven and Wouter Beek and Peter Bloem and {van Harmelen}, Frank and Stefan Schlobach",
  year      = "2016",
  doi       = "10.1007/978-3-319-46523-4_12",
  isbn      = "978-3-319-46522-7",
  volume    = "9981 LNCS",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "184--199",
  booktitle = "The Semantic Web - 15th International Semantic Web Conference, ISWC 2016, Proceedings",
}


@article{39ec163cbc9a41078f10c194ebcd2f3e,
  title     = "A Task-based Comparison of Linguistic and Semantic Document Retrieval Methods in the Medical Domain",
  abstract  = "Text-based and semantics-based methods are both studied intensively as methods for document retrieval. In order to gain insight in the respective merits of these two approaches, we have performed a controlled experiment where we executed a real-life task using both textbased and semantics-based techniques. To maximise the lessons that we could draw about the two approaches, we have performed an experiment where we used the same task (searching papers from the scientific literature needed for updating a medical guideline), the same test-case (updating the 2004 Dutch national breast-cancer guideline), the same gold standard (the updated 2012 Dutch national breast-cancer guideline) and the same corpus (PubMed). We then performed this task using two different methods: retrieving papers based on keywords (text-based approach) and retrieving papers based on semantic annotations (semantics-based approach). Based on this experiment, we discuss the insights that we gained from this dual set of experiments.",
  keywords  = "Concept-based search, Document retrieval, Keyword search, Relation-based search, Semantic annotation",
  author    = "Mohammad Shafahi and Qing Hu and Hamideh Afsarmanesh and Z. Huang and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H.",
  year      = "2016",
  volume    = "1613",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@inbook{91383e82af8f4f5c82634e58bfdbf443,
  title     = "A topic-centric approach to detecting new evidences for evidence-based medical guidelines",
  abstract  = "Evidence-based Medical guidelines are developed based on the best available evidence in biomedical science and clinical practice. Such evidence-based medical guidelines should be regularly updated, so that they can optimally serve medical practice by using the latest evidence from medical research. The usual approach to detect such new evidence is to use a set of terms from a guideline recommendation and to create queries for a biomedical search engine such as PubMed, with a ranking over a selected subset of terms to search for relevant new evidence. However, the terms that appear in a guideline recommendation do not always cover all of the information we need for the search, because the contextual information (e.g. time and location, user profile, topics) is usually missing in a guideline recommendation. Enhancing the search terms with contextual information would improve the quality of the search results. In this paper, we propose a topic-centric approach to detect new evidence for updating evidence-based medical guidelines as a context-aware method to improve the search. Our experiments show that this topic centric approach can find the goal evidence for 12 guideline statements out of 16 in our test set, compared with only 5 guideline statements that were found by using a non-topic centric approach.",
  keywords  = "Context-awareness, Evidence-based medical guidelines, Medical guideline update, Semantic distance, Topic-centric approach",
  author    = "Qing Hu and Zisheng Huang and {ten Teije}, Annette and {van Harmelen}, Frank and Marshall, {M. Scott} and Andre Dekker",
  year      = "2016",
  pages     = "282--289",
  booktitle = "HEALTHINF 2016 - 9th International Conference on Health Informatics, Proceedings; Part of 9th International Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2016",
  publisher = "SciTePress",
}


@inbook{489b9422588546ec9f2a8c9c806559fd,
  title     = "Exploiting emergent schemas to make RDF systems more efficient",
  abstract  = "We build on our earlier finding that more than 95% of the triples in actual RDF triple graphs have a remarkably tabular structure, whose schema does not necessarily follow from explicit metadata such as ontologies, but for which an RDF store can automatically derive by looking at the data using so-called “emergent schema” detection techniques. In this paper we investigate how computers and in particular RDF stores can take advantage from this emergent schema to more compactly store RDF data and more efficiently optimize and execute SPARQL queries. To this end, we contribute techniques for efficient emergent schema aware RDF storage and new query operator algorithms for emergent schema aware scans and joins. In all, these techniques allow RDF schema processors fully catch up with relational database techniques in terms of rich physical database design options and efficiency, without requiring a rigid upfront schema structure definition.",
  author    = "Pham, {Minh Duc} and Peter Boncz",
  year      = "2016",
  doi       = "10.1007/978-3-319-46523-4_28",
  isbn      = "9783319465227",
  volume    = "9981 LNCS",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "463--479",
  booktitle = "The Semantic Web - 15th International Semantic Web Conference, ISWC 2016, Proceedings",
}


@inbook{82b812259aed4390bb18653971bd6591,
  title     = "Generalizing the Detection of Internal and External Interactions in Clinical Guidelines",
  abstract  = "This paper presents a method for formally representing Computer-Interpretable Guidelines to deal with multimorbidity. Although some approaches for merging guidelines exist, improvements are still required for combining several sources of information and coping with possibly conflicting pieces of evidence coming from clinical studies. Our main contribution is twofold: (i) we provide general models and rules for representing guidelines that expresses evidence as causation beliefs; (ii) we introduce a mechanism to exploit external medical knowledge acquired from Linked Open Data (Drugbank, Sider, DIKB) to detect potential interactions between recommendations. We apply this framework to merge three guidelines (Osteoarthritis, Diabetes, and Hypertension) in order to illustrate the capability of this approach for detecting potential conflicts between guidelines and eventually propose alternatives.",
  keywords  = "Clinical guidelines, Knowledge representation, Ontologies, Semantic web",
  author    = "{Carretta Zamborlini}, Veruska and Rinke Hoekstra and {Da Silveira}, Marcos and Cedric Pruski and {ten Teije}, Annette and {van Harmelen}, Frank",
  year      = "2016",
  doi       = "10.5220/0005704101050116",
  pages     = "105--116",
  booktitle = "HEALTHINF 2016 - 9th International Conference on Health Informatics, Proceedings; Part of 9th International Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2016",
  publisher = "SciTePress",
}


@article{6dc9278699694bd4b43db86691dfa64c,
  title     = "grlc Makes GitHub Taste Like Linked Data APIs",
  abstract  = "Building Web APIs on top of SPARQL endpoints is becoming a common practice to enable universal access to the integration favorable dataspace of Linked Data. However, the Linked Data community cannot expect users to learn SPARQL to query this dataspace, and Web APIs are the most common way of enabling programmatic access to data on the Web. However, the implementation of Web APIs around Linked Data is often a tedious and repetitive process. Recent work speeds up thisLinked Data API construction by wrapping it around SPARQL queries, which carry out the API functionality under the hood. Inspired by this, in this paper we present grlc, a lightweight server that translates SPARQL queries curated in GitHub repositories to Linked Data APIs on the fly.",
  keywords  = "Git, GitHub, Linked data APIs, SPARQL",
  author    = "Albert Meroño-Peñuela and Rinke Hoekstra",
  year      = "2016",
  doi       = "10.1007/978-3-319-47602-5_48",
  volume    = "1629",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@inbook{44bee764c491494189957818fb36f123,
  title     = "grlc Makes GitHub Taste Like Linked Data APIs",
  abstract  = "Building Web APIs on top of SPARQL endpoints is becoming common practice. It enables universal access to the integration favorable data space of Linked Data. In the majority of use cases, users cannot be expected to learn SPARQL to query this data space. Web APIs are the most common way to enable programmatic access to data on the Web. However, the implementation of Web APIs around Linked Data is often a tedious and repetitive process. Recent work speeds up this Linked Data API construction by wrapping it around SPARQL queries, which carry out the API functionality under the hood. Inspired by this development, in this paper we present grlc, a lightweight server that takes SPARQL queries curated in GitHub repositories, and translates them to Linked Data APIs on the fly.",
  keywords  = "Git, GitHub, Linked data APIs, RESTFul, SPARQL",
  author    = "{Merono Penuela}, A. and R.J. Hoekstra",
  year      = "2016",
  doi       = "10.1007/978-3-319-47602-5_48",
  isbn      = "9783319476018",
  volume    = "9989 LNCS",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "342--353",
  booktitle = "The Semantic Web - ESWC 2016 Satellite Events, Revised Selected Papers",
}


@inbook{84bd5d6049614c52b3d233eeff1f1c76,
  title     = "How Good Are Query Optimizers, Really?",
  abstract  = "Finding a good join order is crucial for query performance. In this paper, we introduce the Join Order Benchmark (JOB) and experimentally revisit the main components in the classic query optimizer architecture using a complex, real-world data set and realistic multi-join queries. We investigate the quality of industrial-strength cardinality estimators and find that all estimators routinely produce large errors. We further show that while estimates are essential for finding a good join order, query performance is unsatisfactory if the query engine relies too heavily on these estimates. Using another set of experiments that measure the impact of the cost model, we find that it has much less influence on query performance than the cardinality estimates. Finally, we investigate plan enumeration techniques comparing exhaustive dynamic programming with heuristic algorithms and find that exhaustive enumeration improves performance despite the sub-optimal cardinality estimates.",
  author    = "Viktor Leis and Andrey Gubichev and Atanas Mirchev and Peter Boncz and Alfons Kemper and Thomas Neumann",
  year      = "2016",
  volume    = "9",
  pages     = "204--215",
  booktitle = "Proceedings of the VLDB Endowment",
  publisher = "Association for Computing Machinery (ACM)",
  edition   = "3",
}


@article{4d178cf0055f4dd1acf70f17b8d01a25,
  title     = "Inferring recommendation interactions in clinical guidelines",
  abstract  = "The formal representation of clinical knowledge is still an open research topic. Classical representation languages for clinical guidelines are used to produce diagnostic and treatment plans. However, they have important limitations, e.g. when looking for ways to re-use, combine, and reason over existing clinical knowledge. These limitations are especially problematic in the context of multimorbidity; patients that suffer from multiple diseases. To overcome these limitations, this paper proposes a model for clinical guidelines (TMR4I) that allows the re-use and combination of knowledge from multiple guidelines. Semantic Web technology is applied to implement the model, allowing us to automatically infer interactions between recommendations, such as recommending the same drug more than once. It relies on an existing Linked Data set, DrugBank, for identifying drug-drug interactions. We evaluate the model by applying it to two realistic case studies on multimorbidity that combine guidelines for two (Duodenal Ulcer and Transient Ischemic Attack) and three diseases (Osteoarthritis, Hypertension and Diabetes) and compare the results with existing methods.",
  keywords  = "Clinical knowledge representation, OWL, SPARQL, SWRL, combining medical guidelines, multimorbidity, reasoning, rules",
  author    = "{Carretta Zamborlini}, Veruska and Rinke Hoekstra and {Da Silveira}, Marcos and Cedric Pruski and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H.",
  year      = "2016",
  doi       = "10.3233/SW-150212",
  volume    = "7",
  pages     = "421--446",
  journal   = "Semantic Web",
  issn      = "1570-0844",
  publisher = "IOS Press",
  number    = "4",
}


@misc{60d8c039cc9e446685f5a19939709225,
  title  = "Integrating Diachronous Conceptual Lexicons through Linked Open Data",
  author = "E. Maks and {van Erp}, M.G.J. and P.T.J.M. Vossen and R.J. Hoekstra and {van der Sijs}, N.",
  note   = "Proceedings title: DHBenelux Place of publication: Luxembourg",
  year   = "2016",
}


@book{8575d1a8fc274801afefe62bcdff596a,
  title     = "Interim report on the disambiguation results: RISIS Deliverable D25.1",
  author    = "Stefan Schlobach and O.A.K. Idrissou and R.J. Hoekstra and A. Khalili and {van Harmelen}, Frank and {van den Besselaar}, P.A.A.",
  year      = "2016",
  publisher = "Vrije Universiteit",
}


@misc{a0ef482c38564118959ba0dac4312ad3,
  title  = "Kasadaka: a rapid prototyping platform for the rural poor",
  author = "Lo, {A. Gossa} and K.S. Schlobach and {de Boer}, V.",
  year   = "2016",
}


@inbook{210cc82a653b4c559a54a863de63cad2,
  title     = "Knowledge Services Using Rule-Based Formalization for Eligibility Criteria of Clinical Trials",
  abstract  = "Rule-based formalization of eligibility criteria in clinical trials have distinguished features such as declaration, easy maintenance, reusability, and expressiveness. In this paper, we present several knowledge services which can be provided by the rule-based formalization of eligibility criteria. The rule-based formalization can be generated automatically by using the logic programming Prolog with the support of NLP tools for the semantic annotation and relation extraction with medical ontologies/terminologies such as UMLS and SNOMED CT. We show how those automatically generated rule-based formalization for eligibility criteria can be used for the patient recruitment service in SemanticCT, a semantically-enabled system for clinical trials.",
  author    = "Z. Huang and Q. Hu and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H. and S. Ait-Mokhtar",
  year      = "2016",
  doi       = "10.1007/978-3-319-48335-1_6",
  isbn      = "9783319483344",
  volume    = "10038 LNCS",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "49--61",
  booktitle = "Health Information Science - 5th International Conference, HIS 2016, Proceedings",
}


@article{f4c2294d2bb6422bb192e86078f68a6d,
  title     = "LDBC Graphalytics: A Benchmark for Large-Scale Graph Analysis on Parallel and Distributed Platforms",
  abstract  = "In this paper we introduce LDBC Graphalytics, a new industrial-grade benchmark for graph analysis platforms. It consists of six deterministic algorithms, standard datasets, synthetic dataset generators, and reference output, that enable the objective comparison of graph analysis platforms. Its test harness produces deep metrics that quantify multiple kinds of system scalability, such as horizontal/vertical and weak/strong, and of robustness, such as failures and performance variability. The benchmark comes with open-source software for generating data and monitoring performance. We describe and analyze six implementations of the benchmark (three from the community, three from the industry), providing insights into the strengths and weaknesses of the platforms. Key to our contribution, vendors perform the tuning and benchmarking of their platforms.",
  author    = "Alexandru Iosup and Tim Hegeman and Ngai, {Wing Lung} and Stijn Heldens and Arnau Prat-Pérez and Thomas Manhardt and Hassan Chafi and Mihai Capota and Narayanan Sundaram and Anderson, {Michael J.} and Tanase, {Ilie Gabriel} and Yinglong Xia and Lifeng Nai and Boncz, {Peter A.}",
  year      = "2016",
  doi       = "10.14778/3007263.3007270",
  volume    = "9",
  pages     = "1317--1328",
  journal   = "PVLDB",
  issn      = "2150-8097",
  publisher = "Very Large Data Base Endowment Inc.",
  number    = "13",
}


@article{2405656e886645b996d943e2e03a1801,
  title     = "Linked Data Reactor: a Framework for Building Reactive Linked Data Applications",
  abstract  = "This paper presents Linked Data Reactor (LD-Reactor or LD-R) as a framework for developing exible and reusable User Interface components for Linked Data applications. LD-Reactor utilizes Facebook's ReactJS components, Flux architecture and Yahoo's Fluxible framework for isomorphic Web applications. It also exploits Semantic-UI framework for exible UI themes. LD-R aims to apply the idea of component-based application development into RDF data model hence enhancing current user interfaces to view, browse and edit Linked Data.",
  author    = "Ali Khalili",
  year      = "2016",
  volume    = "1615",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@misc{9d1847e6571e4a07a23d8b2857aaed65,
  title  = "LOTUS: Adaptive Search for Big Linked Data",
  author = "F. Ilievski and W.G.J. Beek and {van Erp}, M.G.J. and L.J. Rietveld and K.S. Schlobach",
  year   = "2016",
}


@inbook{f61941f98374409c990fb5f25443b77d,
  title     = "LOTUS: Adaptive text search for big linked data",
  abstract  = "Finding relevant resources on the Semantic Web today is a dirty job: no centralized query service exists and the support for natural language access is limited. We present LOTUS: Linked Open Text Un- leaShed, a text-based entry point to a massive subset of today’s Linked Open Data Cloud. Recognizing the use case dependency of resource re- trieval, LOTUS provides an adaptive framework in which a set of match- ing and ranking algorithms are made available. Researchers and develop- ers are able to tune their own LOTUS index by choosing and combining the matching and ranking algorithms that suit their use case best. In this paper, we explain the LOTUS approach, its implementation and the functionality it provides. We demonstrate the ease with which LOTUS enables text-based resource retrieval at an unprecedented scale in con- crete and domain-specific scenarios. Finally, we provide evidence for the scalability of LOTUS with respect to the LOD Laundromat, the largest collection of easily accessible Linked Open Data currently available.",
  keywords  = "Findability, Scalable data management, Semantic search, Text indexing",
  author    = "F. Ilievski and Wouter Beek and {van Erp}, Marieke and Laurens Rietveld and Stefan Schlobach",
  year      = "2016",
  doi       = "10.1007/978-3-319-34129-3_29",
  isbn      = "9783319341286",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "470--485",
  booktitle = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
}


@article{fafe4ddf3ff14b408a8d400a4331a3f0,
  title     = "Managing metadata for science, technology and innovation studies: The RISIS case",
  abstract  = "Here, we describe the RISIS-SMS metadata system, developed to support the use of heterogeneous datasets in the field of Science, Technology and Innovation Studies (STIS). These data are partly within the RISIS infrastructure, but often elsewhere. The system has three aims: (i) to help researchers to search for and understand data that will help to answer specific research questions, without having to access or download the data. As datasets often have restricted access, browsing metadata is a key feature of the system: researchers need help identifying the relevant data from different sources for their research, and for which data it is worthwhile asking for access; (ii) to support the enrichment of dataBy linking the metadata system to the Linked Open Data environment (LOD); (iii) to facilitate application-driven data integration.",
  keywords  = "metadata, linked data, research infrastructures, digital humanities, science & technologies studies",
  author    = "Idrissou, {Al Koudous} and Ali Khalili and Rinke Hoekstra and Besselaar, {Peter Van den}",
  year      = "2016",
  volume    = "1608",
  pages     = "15--20",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@article{9a629914bb37409db65a1698e87e8040,
  title     = "Phil@Scale: Computational methods within philosophy",
  abstract  = "In this paper we report the results of Phil@Scale, a project directed at the development of computational methods for (the history of) philosophy.1 In this project, philosophers and computer scientists together created SalVe, a tool that helps philosophers answering text-based questions. SalVe has been tested successfully on the Wissenschaftslehre (1837), an extensive work by the Bohemian polymath Bernard Bolzano (1781-1848). Bolzano was a philosopher, mathematician and theologian whose work has been of fundamental importance for the development of Western logic and the foundation of sciences such as mathematics and computer science. The testing of SalVe on the Wissenschaftslehre reveals that with respect to certain questions within philosophy valuable contributions are obtained by applying even rather simple, well-known computational techniques. We conclude that there is definitely a future for computational methods within text-based philosophical research. We explain how SalVe can be used within philosophical research that relies on textual sources. We will start out with an explanation of our aims in developing SalVe and give a short description of SalVe's functionalities, followed by a technical description of the tool. Then we will give a concrete example of how SalVe aids philosophical research. We conclude the paper with an evaluation of the potential of Digital Humanities tools for philosophy, and the challenges that face us if we wish to continue this development further.",
  author    = "{Van Wierst}, Pauline and Sanne Vrijenhoek and Stefan Schlobach and Arianna Betti",
  year      = "2016",
  volume    = "1681",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@misc{0bb6ee06d05a48a2b0000ac820655834,
  title    = "Publishing and Consuming Linked Data",
  abstract = "13897",
  author   = "L.J. Rietveld",
  note     = "Exacte Wetenschappen Naam instelling promotie: Vrije Universiteit Amsterdam Naam instelling onderzoek: Vrije Universiteit Amsterdam",
  year     = "2016",
  school   = "Vrije Universiteit Amsterdam",
}


@misc{c61af5870a364375bc61f835f38c24de,
  title    = "Refining Statistical Data on the Web",
  abstract = "13901",
  author   = "{Merono Penuela}, Albert",
  note     = "Exacte Wetenschappen Naam instelling promotie: Vrije Universiteit Amsterdam Naam instelling onderzoek: Vrije Universiteit Amsterdam",
  year     = "2016",
  school   = "Vrije Universiteit Amsterdam",
}


@book{bfd5feefda6646ad8bb867d8da1b4691,
  title     = "RISIS Integrated platform, Architecture and roadmap.: RISIS Deliverable 25.2",
  author    = "Guillaume Orsal and Philippe Bruecker and Marc Barbier and {van den Besselaar}, P.A.A. and A. Khalili",
  year      = "2016",
  publisher = "Universite Paris Est - IFRIS",
}


@article{62ad2a6cd97a4be38502d5f84e54ec29,
  title     = "SCRY: Extending SPARQL with custom data processing methods for the life sciences",
  keywords  = "customization, data processing, rdf generation, SPARQL, extension",
  author    = "Bas Stringer and Albert Meroño-peñuela and Sanne Abeln and {van Harmelen}, Frank and Jaap Heringa",
  year      = "2016",
  volume    = "1795",
  pages     = "1--10",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@article{48ce34ae6f064ad08f20a48bdfaaea18,
  title     = "Selected papers from the combined EKAW 2014 and Semantic Web journal track",
  author    = "Stefan Schlobach and Krzysztof Janowicz",
  year      = "2016",
  doi       = "10.3233/SW-160229",
  volume    = "7",
  pages     = "333--334",
  journal   = "Semantic Web",
  issn      = "1570-0844",
  publisher = "IOS Press",
  number    = "4",
}


@inbook{6b8910eb91d746ed84e869023557eb2a,
  title     = "SMS: a linked open data infrastructure for science and innovation studies.",
  author    = "{van den Besselaar}, Peter and Ali Khalili and Oladele Idrissou and K.S. Schlobach and {van Harmelen}, F.A.H.",
  year      = "2016",
  pages     = "106--114",
  editor    = "Ismael Rafols",
  booktitle = "Peripheries, Frontiers and Beyond; proceedings of the 21st STI Conference",
  publisher = "University Valencia",
}


@article{29b76753dd464f068489de2701391d51,
  title     = "SWISH: An integrated semantic web notebook",
  author    = "Wouter Beek and Jan Wielemaker",
  year      = "2016",
  volume    = "1690",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@inbook{888950bfc54943609de37cd94c2c9063,
  title     = "SWISH: An Integrated Semantic Web Notebook",
  author    = "Wouter Beek and Jan Wielemaker",
  year      = "2016",
  booktitle = "Proceedings of the ISWC 2016 Posters Demonstrations Track co-located with 15th International Semantic Web Conference (ISWC 2016), Kobe, Japan, October 19, 2016.",
}


@article{aeeb6a77c22d4ecebcef47cedec27955,
  title     = "SWISH for prototyping clinical guideline interactions theory",
  abstract  = "SWISH provides a general purpose collaborative infrastructure for applying Prolog reasoning over an RDF dataset together with features that facilitates prototyping Semantic Web applications. In this paper we report on the use of SWISH for efficiently developing a prototype for detection of clinical guideline interactions. These guidelines are a set of medical recommendations meant for supporting doctors on tackling a single disease. However, often guidelines need to be combined for treating patients that suffer from multiple diseases, and then a number of interactions can occur. The generic interaction rules are implemented in SWI-Prolog and the guideline RDF-data is enriched with clinical Linked Open Data (LOD) (e.g. Drugbank, Sider). We show the implementation of the proposed theory about interaction detection in a case-study on combining three guidelines. The experiment is interactively described using a SWISH notebook and the results are graphical visualised empowered by graphviz.",
  keywords  = "Clinical guideline interactions, Multimorbidity, Prolog, RDF, SWISH",
  author    = "{Carretta Zamborlini}, Veruska and Jan Wielemaker and {Da Silveira}, Marcos and Cedric Pruski and {ten Teije}, Annette and {van Harmelen}, F.A.H.",
  year      = "2016",
  volume    = "1795",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@inbook{4f2fea23a5e849ff801af292650d9fdf,
  title     = "The semantic web in an SMS",
  abstract  = "Many ICT applications and services, including those from the Semantic Web, rely on the Web for the exchange of data. This includes expensive server and network infrastructures. Most rural areas of developing countries are not reached by the Web and its possibilities, while at the same time the ability to share knowledge has been identified as a key enabler for development. To make widespread knowledge sharing possible in these rural areas, the notion of the Web has to be downscaled based on the specific low-resource infrastructure in place. In this paper, we introduce SPARQL over SMS, a solution for Web-like exchange of RDF data over cellular networks in which HTTP is substituted by SMS. We motivate and validate this through two use cases in West Africa. We present the design and implementation of the solution, along with a data compression method that combines generic compression strategies and strategies that use Semantic Web specific features to reduce the size of RDF before it is transferred over the low-bandwidth cellular network.",
  author    = "Onno Valkering and {de Boer}, Victor and Gossa Lô and Romy Blankendaal and Stefan Schlobach",
  year      = "2016",
  doi       = "10.1007/978-3-319-49004-5_45",
  isbn      = "9783319490038",
  volume    = "10024 LNAI",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "697--712",
  booktitle = "Knowledge Engineering and Knowledge Management - 20th International Conference, EKAW 2016, Proceedings",
}


@inbook{fa13d88f82b7462283a1cc9b8b232f44,
  title     = "The song remains the same: Lossless conversion and streaming of MIDI to RDF and back",
  abstract  = "In this demo, we explore the potential of RDF as a representation format for digital music. Digital music is broadly used today in many professional music production environments. For decades, MIDI (Musical Instrument Digital Interface) has been the standard for digital music exchange between musicians and devices, albeit not in a Web friendly way. We show the potential of expressing digital music as Linked Data, using our midi2rdf suite of tools to convert and stream digital music in MIDI format to RDF. The conversion allows for lossless round tripping: we can reconstruct a MIDI file identical to the original using its RDF representation. The streaming uses an existing, novel generative audio matching algorithm that we use to broadcast, with very low latency, RDF triples of MIDI events coming from arbitrary analog instruments.",
  keywords  = "Linked data, MIDI, Music streams, RDF",
  author    = "Albert Meroño-Peñuela and Rinke Hoekstra",
  year      = "2016",
  doi       = "10.1007/978-3-319-47602-5_38",
  isbn      = "9783319476018",
  volume    = "9989 LNCS",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "194--199",
  booktitle = "The Semantic Web - ESWC 2016 Satellite Events, Revised Selected Papers",
}


@misc{7559caac92f74c2381a30ea519bbe559,
  title  = "Towards an open data infrastructure for STI data. OECD Blue Sky Conference, Gent, September 2016",
  author = "{van den Besselaar}, Peter and Ali Khalili and {de Graaf}, {Klaas Andries} and Oladele Idrissou and A. Loizou and K.S. Schlobach and {van Harmelen}, F.A.H.",
  year   = "2016",
}


@article{fd1c0df2db4246e19aabe172262616ca,
  title   = "大数据时代的语义技术",
  author  = "Zhisheng Huang",
  year    = "2016",
  volume  = "2016",
  journal = "Journal of Digital Library Forum",
  issn    = "1673-2286",
  number  = "10",
}


@article{e8f8022d7e21492db221d89474cd72a4,
  title     = "The Aggregate Dutch Historical Censuses",
  abstract  = "Historical censuses have an enormous potential for research. In order to fully use this potential, harmonization of these censuses is essential. During the last decades, enormous efforts have been undertaken in digitizing the published aggregated outcomes of the Dutch historical censuses (1795-1971). Although the accessibility has been improved enormously, researchers must cope with hundreds of heterogeneous and disconnected Excel tables. As a result, the census is still for the most part an untapped source of information. The authors describe the main harmonization challenges of the census and how they work toward one harmonized dataset. They propose a specific approach and model in creating an interlinked census dataset in the Semantic Web using the Resource Description Framework technology.",
  keywords  = "harmonization, historical censuses, historical demography, RDF, Semantic Web, social and economic history",
  author    = "Ashkan Ashkpour and Albert Meroño-Peñuela and Kees Mandemakers",
  year      = "2015",
  month     = "10",
  doi       = "10.1080/01615440.2015.1026009",
  volume    = "48",
  pages     = "230--245",
  journal   = "Historical Methods",
  issn      = "0161-5440",
  publisher = "Routledge",
  number    = "4",
}


@inbook{c469e91b8f534261bde83b6a2a70a732,
  title     = "Graphalytics: A big data benchmark for graph-processing platforms",
  abstract  = "Graphs are increasingly used in industry, governance, and science. This has stimulated the appearance of many and diverse graph-processing platforms. Although platform di- versity is beneficial, it also makes it very challenging to select the best platform for an application domain or one of its im- portant applications, and to design new and tune existing platforms. Continuing a long tradition of using benchmark- ing to address such challenges, in this work we present our vision for Graphalytics, a big data benchmark for graph- processing platforms. We have already benchmarked with Graphalytics a variety of popular platforms, such as Giraph, GraphX, and Neo4j.",
  author    = "Mihai Capota and Tim Hegeman and Alexandru Iosup and Arnau Prat-Pérez and Orri Erling and Peter Boncz",
  year      = "2015",
  month     = "5",
  doi       = "10.1145/2764947.2764954",
  booktitle = "3rd International Workshop on Graph Data Management Experiences and Systems, GRADES 2015 - co-located with SIGMOD/PODS 2015",
  publisher = "Association for Computing Machinery, Inc",
}


@inbook{3d0fe75c8f744dfa901d715dbb8ad283,
  title     = "The LDBC social network benchmark: Interactive workload",
  abstract  = "The Linked Data Benchmark Council (LDBC) is now two years underway and has gathered strong industrial participation for its mission to establish benchmarks, and benchmarking practices for evaluating graph data management systems. The LDBC introduced a new choke-point driven methodology for developing benchmark workloads, which combines user input with input from expert systems architects, which we outline. This paper describes the LDBC Social Network Benchmark (SNB), and presents database benchmarking innovation in terms of graph query functionality tested, correlated graph generation techniques, as well as a scalable benchmark driver on a workload with complex graph dependencies. SNB has three query workloads under development: Interactive, Business Intelligence, and Graph Algorithms. We describe the SNB Interactive Workload in detail and illustrate the workload with some early results, as well as the goals for the two other workloads.",
  author    = "Orri Erling and Alex Averbuch and Josep-Lluis Larriba-Pey and Hassan Chafi and Andrey Gubichev and Arnau Prat and Pham, {Minh Duc} and Peter Boncz",
  year      = "2015",
  month     = "5",
  doi       = "10.1145/2723372.2742786",
  volume    = "2015-May",
  pages     = "619--630",
  booktitle = "SIGMOD 2015 - Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data",
  publisher = "Association for Computing Machinery (ACM)",
}


@inbook{d48a78a529e14a778429727d45d18381,
  title     = "Deriving an emergent relational schema from RDF data",
  abstract  = "We motivate and describe techniques that allow to detect an {"}emergent{"} relational schema from RDF data. We show that on a wide variety of datasets, the found structure explains well over 90% of the RDF triples. Further, we also describe technical solutions to the semantic challenge to give short names that humans find logical to these emergent tables, columns and relationships between tables. Our techniques can be exploited in many ways, e.g., to improve the efficiency of SPARQL systems, or to use existing SQL-based applications on top of any RDF dataset using a RDBMS.",
  keywords  = "RDF, Relational schema, Structure recognition",
  author    = "Pham, {Minh Duc} and Linnea Passing and Orri Erling and Peter Boncz",
  year      = "2015",
  month     = "5",
  doi       = "10.1145/2736277.2741121",
  pages     = "864--874",
  booktitle = "WWW 2015 - Proceedings of the 24th International Conference on World Wide Web",
  publisher = "Association for Computing Machinery, Inc",
}


@article{3711c0eae7cb45b9bec21e26f594fb4b,
  title     = "Why the Data Train Needs Semantic Rails",
  abstract  = "While catchphrases such as big data, smart data, data-intensive science, or smart dust highlight different aspects, they share a common theme - namely, a shift toward a data-centered perspective in which the synthesis and analysis of data at an ever-increasing spatial, temporal, and thematic resolution promise new insights, while, at the same time, reduce the need for strong domain theories as starting points. In terms of the envisioned methodologies, those catchphrases tend to emphasize the role of predictive analytics, that is, statistical techniques including data mining and machine learning, as well as supercomputing. Interestingly, however, while this perspective takes the availability of data as a given, it does not answer the question how one would discover the required data in today's chaotic information universe, how one would understand which data sets can be meaningfully integrated, and how to communicate the results to humans and machines alike. The semantic web addresses these questions. In the following, we argue why the data train needs semantic rails. We point out that making sense of data and gaining new insights work best if inductive and deductive techniques go hand-in-hand instead of competing over the prerogative of interpretation.",
  author    = "Krzysztof Janowicz and Pascal Hitzler and Hendler, {James A.} and {van Harmelen}, Frank",
  year      = "2015",
  month     = "3",
  volume    = "36",
  pages     = "5--14",
  journal   = "The AI Magazine",
  issn      = "0738-4602",
  publisher = "AI Access Foundation",
  number    = "1",
}


@article{f968edbb167c4315a9c9922f6017f3b6,
  title   = "Tracking down the habitat of folk songs",
  author  = "B. Janssen and A. Merono and A. Ashkpour and C.D.M. Gueret",
  year    = "2015",
  month   = "1",
  journal = "eHumanities Magazine 4",
}


@article{7ccc5e56b3e5482ba6566d77dd0cf715,
  title   = "We've Always Been Digital",
  author  = "{Merono Penuela}, Albert",
  year    = "2015",
  month   = "1",
  journal = "eHumanities Magazine 4",
}


@inbook{85421a7bb476400da0040eb31b196374,
  title     = "A Compact In-Memory Dictionary for RDF data",
  abstract  = "While almost all dictionary compression techniques focus on static RDF data, we present a compact in-memory RDF dictionary for dynamic and streaming data. To do so, we analysed the structure of terms in real-world datasets and observed a high degree of common prefixes. We studied the applicability of Trie data structures on RDF data to reduce the memory occupied by common prefixes and discovered that all existing Trie implementations lead to either poor performance, or an excessive memory wastage. In our approach, we address the existing limitations of Tries for RDF data, and propose a new variant of Trie which contains some optimizations explicitly designed to improve the performance on RDF data. Furthermore, we show how we use this Trie as an in-memory dictionary by using as numerical ID a memory address instead of an integer counter. This design removes the need for an additional decoding data structure, and further reduces the occupied memory. An empirical analysis on realworld datasets shows that with a reasonable overhead our technique uses 50–59% less memory than a conventional uncompressed dictionary.",
  author    = "Bazoubandi, {Hamid R.} and {de Rooij}, Steven and Jacopo Urbani and {ten Teije}, Annette and {van Harmelen}, Frank and Henri Bal",
  note      = "Proceedings title: Proceedings of the twelfth European Semantic Web Conference Publisher: Springer Place of publication: Berlin",
  year      = "2015",
  doi       = "10.1007/978-3-319-18818-8_13",
  volume    = "9088",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "205--220",
  booktitle = "The Semantic Web: Latest Advances and New Domains - 12th European Semantic Web Conference, ESWC 2015, Proceedings",
}


@inbook{efec5bf19fa345ff8264ce823017de8c,
  title     = "Analyzing Recommendations Interactions in Clinical Guidelines: Impact of action type hierarchies and causation beliefs",
  abstract  = "Accounting for patients with multiple health conditions is a complex task that requires analysing potential interactions among recommendations meant to address each condition. Although some approaches have been proposed to address this issue, important features still require more investigation, such as (re)usability and scalability. To this end, this paper presents an approach that relies on reusable rules for detecting interactions among recommendations coming from various guidelines. It extends previously proposed models by introducing the notions of action type hierarchy and causation beliefs, and provides a systematic analysis of relevant interactions in the context of multimorbidity. Finally, the approach is assessed based on a case-study taken from the literature to highlight the added value of the approach.",
  keywords  = "Clinical knowledge representation, Combining medical guidelines, Multimorbidity",
  author    = "{Carretta Zamborlini}, Veruska and {Da Silveira}, Marcos and Cedric Pruski and {ten Teije}, Annette and {van Harmelen}, Frank",
  year      = "2015",
  doi       = "10.1007/978-3-319-19551-3_40",
  isbn      = "9783319195506",
  volume    = "9105",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "317--326",
  booktitle = "Artificial Intelligence in Medicine - 15th Conference on Artificial Intelligence in Medicine, AIME 2015, Proceedings",
}


@inbook{f9ec22a41c9d463d8226fb6a652fa451,
  title     = "An Intelligent Monitoring System for the Safety of Building Structure under the W2T Framework",
  abstract  = "Monitoring systems for the safety of building structure (SBS) can provide people with important data related to main supporting points in a building and then help people to make a reasonable maintenance schedule. However, more and more data bring a challenge for data management and data mining. In order to meet this challenge, under the framework of Wisdom Web of Things (W2T), we design a monitoring system for the SBS by using the semantic and the multisource data fusion technologies. This system establishes a dynamical data cycle among the physical world (buildings), the social world (humans), and the cyber world (computers) and provides various services in the monitoring process to alleviate engineers' workload. Furthermore, all data in the cyber world are organized as the raw data, the semantic information, and the multisource knowledge. Based on this organization, we can concentrate on the data fusion from the viewpoints of time, space, and multisensor. At last, a prototype system powered by the semantic platform LarKC is tested from the aspects of sample performance and time consumption. In particular, noisy data (i.e., inconsistent, abnormal, or error data) are detected through the fusion of multisource knowledge, and some rule-based reasoning is conducted to provide personalized service.",
  author    = "H. Wang and Z. Huang and N. Zhong and Y. Han and F. Zhang",
  year      = "2015",
  doi       = "10.1155/2015/378694",
  booktitle = "Journal of Distributed Sensor Networks",
}


@book{dfa31ef6a2fd4867a6b27654aabff10c,
  title     = "ARIADNE: First Report on Data Mining",
  abstract  = "Recent years have witnessed a growing interest from archaeological communities in Linked Data. ARIADNE, the AdvancedResearch Infrastructure for Archaeological Data set Networking in Europe, facilitates a central web portal that providesaccess to archaeological data from various sources. Parts of these data have been being published as Linked Data, andare currently available in the Linked Open Data cloud. With it, the nature of these data has shifted from unstructuredto structured. This presents new opportunities for data mining. In this work, we investigate to what extend data mining can contribute to the understanding of linked archaeological data, and which form would best meet the communities' needs.",
  author    = "W.X. Wilcke and {de Boer}, Viktor and {van Harmelen}, F.A.H. and {de Kleijn}, Mauritius and M. Wansleeben",
  year      = "2015",
  series    = "Ariadne",
  publisher = "Ariadne",
  number    = "D16.1",
}


@inbook{c13a481d2b0045689c082bf384bd5015,
  title     = "A Semantic Smart Hospital Information System for Mental Disorders",
  author    = "Youjun Li and Z. Wan and J. Huang and J. Chen and Z. Huang and N. Zhong",
  year      = "2015",
  booktitle = "Proceedings of the 2015 IEEE/WIC/ACM International Conference on Web Intelligence (WI2015)",
}


@inbook{6c103170c3d44d4cb733f26940c0b629,
  title     = "Automatic Verification of Road Signs based on Ontology",
  author    = "D. Wang and Z. Huang and N. Zhong and D. Xu and X. Zhang and Zhi Wang",
  year      = "2015",
  series    = "4",
  pages     = "384--392",
  booktitle = "Journal of Wuhan University",
}


@inbook{a23086b321be43a5b2468b356ba2b7c5,
  title     = "Cognition-inspired route evaluation using mobile phone data",
  abstract  = "With the increasing popularity of mobile phones, large amounts of real and reliable mobile phone data are being generated every day. These mobile phone data represent the practical travel routes of users and imply the intelligence of them in selecting a suitable route. Usually, an experienced user knows which route is congested in a specified period of time but unblocked in another period of time. Moreover, a route used frequently and recently by a user is usually the suitable one to satisfy the user’s needs. Adaptive control of thought-rational (ACT-R) is a computational cognitive architecture, which provides a good framework to understand the principles and mechanisms of information organization, retrieval and selection in human memory. In this paper, we employ ACT-R to model the process of selecting a suitable route of users. We propose a cognition-inspired route evaluation method to mine the intelligence of users in selecting a suitable route, evaluate the suitability of the routes, and then recommend an ordered list of routes for subscribers. Experiments show that it is effective and feasible to evaluate the suitability of the routes inspired by cognition.",
  author    = "H. Wang and J. Huang and E. Zhou and Z. Huang and N. Zhong",
  year      = "2015",
  doi       = "10.1007/s11047-014-9479-9",
  pages     = "637--648",
  booktitle = "Journal of Natural Computing",
}


@article{7b4c0b8e14584116bb59fef9cce62f7f,
  title     = "CrowdLearn: Crowd-sourcing the Creation of Highly-structured e-Learning Content",
  author    = "D. Tarasowa and A. Khalili and S. Auer",
  note      = "PT: J; NR: 18; TC: 0; J9: INT J ENG PEDAGOG; PG: 8; GA: CZ3HA; UT: WOS:000366993800006",
  year      = "2015",
  doi       = "10.3991/ijep.v5i4.4951",
  volume    = "5",
  pages     = "47--54",
  journal   = "International Journal of Engineering Pedagogy",
  issn      = "2192-4880",
  publisher = "International Association of Online Engineering",
  number    = "4",
}


@book{50a1d221618d4a5f9f76280af1efa706,
  title     = "Depicting and selecting preferable approaches to knowledge clustering - RISIS deliverable",
  author    = "E Israel and L Villard and M Revollo and D Getz and V Segal and P Larédo and A. Khalili and A. Loizou and {van den Besselaar}, P.A.A. and V Veglio and B Lepori and E. Reale",
  year      = "2015",
  publisher = "SNI",
}


@inbook{1da843e38a53485383b9e72917ebd0b6,
  title     = "Detecting New Evidence for Evidence-based Guidelines Using a Semantic Distance Method",
  author    = "Q. Hu and Z. Huang and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H.",
  year      = "2015",
  pages     = "307--316",
  booktitle = "Proceedings 15th Conference on Artificial Intelligence in Medicine",
  publisher = "Springer",
}


@article{aaf8c24aa8184513aff121db59682115,
  title     = "Enhancing reuse of structured eligibility criteria and supporting their relaxation.",
  abstract  = "Patient recruitment is one of the most important barriers to successful completion of clinical trials and thus to obtaining evidence about new methods for prevention, diagnostics and treatment. The reason is that recruitment is effort consuming. It requires the identification of candidate patients for the trial (the population under study), and verifying for each patient whether the eligibility criteria are met. The work we describe in this paper aims to support the comparison of population under study in different trials, and the design of eligibility criteria for new trials. We do this by introducing structured eligibility criteria, that enhance reuse of criteria across trials. We developed a method that allows for automated structuring of criteria from text. Additionally, structured eligibility criteria allow us to propose suggestions for relaxation of criteria to remove potentially unnecessarily restrictive conditions. We thereby increase the recruitment potential and generalizability of a trial.Our method for automated structuring of criteria enables us to identify related conditions and to compare their restrictiveness. The comparison is based on the general meaning of criteria, comprised of commonly occurring contextual patterns, medical concepts and constraining values. These are automatically identified using our pattern detection algorithm, state of the art ontology annotators and semantic taggers. The comparison uses predefined relations between the patterns, concept equivalences defined in medical ontologies, and threshold values. The result is a library of structured eligibility criteria which can be browsed using fine grained queries. Furthermore, we developed visualizations for the library that enable intuitive navigation of relations between trials, criteria and concepts. These visualizations expose interesting co-occurrences and correlations, potentially enhancing meta-research.The method for criteria structuring processes only certain types of criteria, which results in low recall of the method (18%) but a high precision for the relations we identify between the criteria (94%). Analysis of the approach from the medical perspective revealed that the approach can be beneficial for supporting trial design, though more research is needed.",
  author    = "K. Milian and Rinke Hoekstra and A. Bucur and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H. and J. Paulissen",
  year      = "2015",
  doi       = "10.1016/j.jbi.2015.05.005",
  volume    = "56",
  pages     = "205--219",
  journal   = "Journal of Biomedical Informatics",
  issn      = "1532-0464",
  publisher = "Academic Press Inc.",
}


@inbook{6a9d7f0947f9432c9fa05c3a7d1cae74,
  title     = "Error analysis of Word Sense Disambiguation",
  author    = "R. Izquirdo and M.C. Postma and P.T.J.M. Vossen",
  year      = "2015",
  booktitle = "25th Meeting of Computational Linguistics in the Netherlands",
}


@inbook{dc50695aa6fb421080b44cf8bf27b079,
  title     = "Finding Evidence for Updates in Medical Guidelines",
  abstract  = "Medical guidelines are documents that describe optimal treatment for patients by medical practitioners based on current medical research (evidence), in the form of step-by-step recommendations. Because the field of medical research is very large and always evolving, keeping these guidelines up-to-date with the current state of the art is a difficult task. In this paper, we propose a method for finding relevant evidence for supporting the medical guideline updating process. Our method that takes from the evidence-based medical guideline the recommendations and their corresponding evidence as its input, and that queries PubMed, the world's largest search engine for medical citations, for potential new or improved evidence. We built a prototype and performed a feasibility study on a set of old recommendations, and compared the output to evidence for the newer version. The system succeeded in finding goal articles for 11 out of 16 recommendations, but in total, only 20 out of 71 articles were retrieved. Our ranking method for most relevant articles worked well for small result sets, but for large result sets it failed to rank the goal articles in the top 25 results.",
  keywords  = "Evidence-based medicine, Medical guideline updates, Medical guidelines",
  author    = "Roelof Reinders and {ten Teije}, Annette and Zisheng Huang",
  year      = "2015",
  pages     = "91--102",
  booktitle = "HEALTHINF 2015 - 8th International Conference on Health Informatics, Proceedings; Part of 8th International Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2015",
  publisher = "SciTePress",
}


@article{4739c874bc7b4b8b80b515650dace968,
  title     = "foxPSL: A Fast, Optimized and eXtended PSL implementation",
  abstract  = "In this paper, we describe foxPSL, a fast, optimized and extended implementation of Probabilistic Soft Logic (PSL) based on the distributed graph processing framework Signal/Collect. PSL is one of the leading formalisms of statistical relational learning, a recently developed field of machine learning that aims at representing both uncertainty and rich relational structures, usually by combining logical representations with probabilistic graphical models. PSL can be seen as both a probabilistic logic and a template language for hinge-loss Markov Random Fields, a type of continuous Markov Random fields (MRF) in which Maximum a Posteriori inference is very efficient, since it can be formulated as a constrained convex minimization problem, as opposed to a discrete optimization problem for standard MRFs. From the logical perspective, a key feature of PSL is the capability to represent soft truth values, allowing the expression of complex domain knowledge, like degrees of truth, in parallel with uncertainty. foxPSL supports the full PSL pipeline from problem definition to a distributed solver that implements the Alternating Direction Method of Multipliers (ADMM) consensus optimization. It provides a Domain Specific Language that extends standard PSL with a class system and existential quantifiers, allowing for efficient grounding. Moreover, it implements a series of configurable optimizations, like optimized grounding of constraints and lazy inference, that improve grounding and inference time. We perform an extensive evaluation, comparing the performance of foxPSL to a state-of-the-art implementation of ADMM consensus optimization in GraphLab, and show an improvement in both inference time and solution quality. Moreover, we evaluate the impact of the optimizations on the execution time and discuss the trade-offs related to each optimization.",
  author    = "S. Magliacane and P. Stutz and P. Groth and A. Bernstein",
  year      = "2015",
  doi       = "10.1016/j.ijar.2015.05.012",
  journal   = "International Journal of Approximate Reasoning",
  issn      = "0888-613X",
  publisher = "Elsevier Inc.",
}


@inbook{ae668f92c8e24c3da114a3bae1694df5,
  title     = "FoxPSL: An Extended and Scalable PSL Implementation",
  author    = "S. Magliacane and P. Stutz and P. Groth and A. Bernstein",
  year      = "2015",
  booktitle = "AAAI Spring Symposium 2015 on Knowledge Representation and Reasoning",
}


@article{603d52e2383f4ea7a256bf976ffb4c31,
  title     = "Frank: The LOD cloud at your fingertips?",
  abstract  = "Large-scale, algorithmic access to LOD Cloud data has been hampered by the absence of queryable endpoints for many datasets, a plethora of serialization formats, and an abundance of idiosyncrasies such as syntax errors. As of late, very large-scale - hundreds of thousands of document, tens of billions of triples - access to RDF data has become possible thanks to the LOD Laundromat Web Service. In this paper we showcase Frank, a command-line interface to a very large collection of standards-compliant, real-world RDF data that can be used to run Semantic Web experiments and stress-test Linked Data applications.",
  author    = "Wouter Beek and Laurens Rietveld",
  year      = "2015",
  volume    = "1361",
  pages     = "41--46",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@inbook{a7ac3d57a0f6415187e64c0db66479c6,
  title     = "Frank: Algorithmic Access to the LOD Cloud",
  author    = "W.G.J. Beek and L.J. Rietveld",
  year      = "2015",
  pages     = "41--46",
  booktitle = "Proceedings of the Developer's Workshop ESWC 2015",
}


@misc{c0d3f8b79d574d49a286377515c14b7b,
  title    = "Frank: The LOD cloud at your fingertips?",
  abstract = "Large-scale, algorithmic access to LOD Cloud data has been hampered by the absence of queryable endpoints for many datasets, a plethora of serialization formats, and an abundance of idiosyncrasies such as syntax errors. As of late, very large-scale — hundreds of thousands of document, tens of billions of triples — access to RDF data has become possible thanks to the LOD Laundromat Web Service. In this paper we showcase Frank, a command-line interface to a very large collection of standards-compliant, real-world RDF data that can be used to run Semantic Web experiments and stress-test Linked Data applications.",
  author   = "Wouter Beek and Laurens Rietveld",
  year     = "2015",
}


@inbook{44d79201f5274af0aa9b72d3ca9edf98,
  title     = "Hubble: Linked data Hub for clinical decision support",
  abstract  = "The AERS datasets is one of the few remaining, large publicly available medical data sets that until now have not been published as Linked Data. It is uniquely positioned amidst othermedical datasets. This paper describes the Hubble prototype system for clinical decision support that demonstrates the speed, ease and flexibility of producing and using a Linked Data version of the AERS dataset for clinical practice and research.",
  keywords  = "Adverse event, Clinical decision support, Health care, Linked data",
  author    = "Rinke Hoekstra and Sara Magliacane and Laurens Rietveld and {De Vries}, Gerben and Adianto Wibisono and Stefan Schlobach",
  year      = "2015",
  doi       = "10.1007/978-3-662-46641-4_45",
  isbn      = "9783662466407",
  volume    = "7540",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "458--462",
  booktitle = "The Semantic Web: ESWC 2012 Satellite Events - Revised Selected Papers",
}


@article{fddfc1d0460c4d7a99bb5e2cbb32156d,
  title     = "Identification of patients at risk for colorectal cancer in primary care: an explorative study with routine healthcare data",
  abstract  = "Background Early diagnosis of colorectal cancer (CRC) is likely to reduce burden of disease and improve treatment success. Estimation of the individual patient risk for CRC diagnostic determinants in a primary care setting has not been very successful as yet. The aim of our study is to improve prediction of CRC in patients selected for colonoscopy in the primary healthcare setting using readily available routine healthcare data. Patients and methods A cross-sectional study was carried out in the Julius General Practitioners' Network database. Patients referred for colonoscopy by their general practitioner (GP) between 2007 and 2012 were selected. We evaluated the association between long-term registered patient characteristics, symptoms and conditions, and colonoscopy test results with multivariable logistic regression. Results Two per cent (2787/140 000) of the patients between 30 and 85 years were found to be newly referred for colonoscopy by their GP, of whom 57 (2%) were diagnosed with CRC. Age 50 years or over, hypertension and the absence of preceding consultations for abdominal pain were independent predictors for CRC and/or high-risk adenomas, with an area under the curve of 0.65. Conclusion Three factors in routine care data combined might prove valuable in future strategies to improve the prediction of CRC risk in primary care. Improvement in quality and availability of routine care data for research and risk stratification is needed to optimize its usability for prediction purposes in daily practice. Impact Only referring patients at the highest risk for colonoscopy by the GP could decrease superfluous colonoscopies.",
  author    = "N.R Koning and L.N.G Moons and F.L. Büchner and C.W. Helsper and {ten Teije}, A.C.M. and M.E. Numans",
  year      = "2015",
  doi       = "10.1097/MEG.0000000000000472",
  volume    = "27",
  pages     = "1443--1448",
  journal   = "European Journal of Gastroenterology and Hepatology",
  issn      = "0954-691X",
  publisher = "Lippincott Williams and Wilkins",
  number    = "12",
}


@inbook{86f39ce706f04073a8fc2aec8926fed6,
  title     = "Identifying Evidence Quality for Updating Evidence-based Medical Guideline",
  author    = "Z. Huang and Q. Hu and {ten Teije}, A. and {van Harmelen}, F.",
  year      = "2015",
  booktitle = "Proceedings of International Joint WorkshopKR4HC 2015 - ProHealth 2015",
}


@inbook{cd7cd90ab9284d68ae98b8f54f65ca35,
  title     = "Identifying Evidence Quality for Updating Evidence-based Medical Guidelines",
  author    = "Z. Huang and Q. Hu and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H.",
  year      = "2015",
  isbn      = "9783319265841",
  pages     = "51--64",
  editor    = "D Riano and R. Lenz and S. Miksch and M. Peleg and M. Reichert and {ten Teije}, A.C.M.",
  booktitle = "Knowledge Representation for Health Care, AIME 2015 International Joint Workshop, KR4HC/ProHealth 2015",
  publisher = "Springer",
}


@article{0cba7e1e14c04d4c8cf48f43f0aec6a2,
  title     = "Inferring Recommendation Interactions in Clinical Guidelines: Case-studies on Multimorbidity",
  author    = "V. Zamborlini and Rinke Hoekstra and {da Silveira}, M. and C. Pruski and {ten Teije}, A.C.M.",
  year      = "2015",
  journal   = "Semantic Web",
  issn      = "1570-0844",
  publisher = "IOS Press",
}


@book{a82caf00e1a143759c7f5543b6c84ea5,
  title     = "{Knowledge Representation for Health Care (AIME 2015 International Joint Workshop, KR4HC/ProHealth 2015)",
  author    = "D Riano and R. Lenz and S Miksch and M Peleg and M. Reichert and {ten Teije}, A.C.M.",
  year      = "2015",
  doi       = "10.1007/978-3-319-26585-8",
  isbn      = "9783319265841",
  series    = "LNAI",
  publisher = "Springer",
  number    = "9485",
}


@inbook{ef39b81fadc345f788ca499a78ccbb4c,
  title     = "Linked data-as-a-service: The semantic web redeployed",
  abstract  = "Ad-hoc querying is crucial to access information from Linked Data, yet publishing queryable RDF datasets on the Web is not a trivial exercise. The most compelling argument to support this claim is that the Web contains hundreds of thousands of data documents, while only 260 queryable SPARQL endpoints are provided. Even worse, the SPARQL endpoints we do have are often unstable, may not comply with the standards, and may differ in supported features. In other words, hosting data online is easy, but publishing Linked Data via a queryable API such as SPARQL appears to be too difficult. As a consequence, in practice, there is no single uniform way to query the LOD Cloud today. In this paper, we therefore combine a large-scale Linked Data publication project (LOD Laundromat) with a low-cost server-side interface (Triple Pattern Fragments), in order to bridge the gap between the Web of downloadable data documents and the Web of live queryable data. The result is a repeatable, low-cost, open-source data publication process. To demonstrate its applicability, we made over 650,000",
  keywords  = "API, Data publishing, Linked Data, Web Services",
  author    = "Laurens Rietveld and Ruben Verborgh and Wouter Beek and {Vander Sande}, Miel and Stefan Schlobach",
  year      = "2015",
  doi       = "10.1007/978-3-319-18818-8_29",
  isbn      = "9783319188171",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "471--487",
  booktitle = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
}


@inbook{5662be17f4b148779076a81e8a5e7e40,
  title     = "Linked Data-as-a-Service: The Semantic Web Redeployed",
  author    = "L.J. Rietveld and R. Verborgh and W.G.J. Beek and {Vander Sande}, M. and S. Schlobach",
  year      = "2015",
  pages     = "471--487",
  booktitle = "The Semantic Web. Latest Advances and New Domains",
  publisher = "Springer International Publishing",
}


@misc{1ecf8244410e4eb0bd41746e1c8cf4ff,
  title  = "Linked Data Journalism: Linking Data Sources to Journalistic Articles",
  author = "W.G.J. Beek",
  note   = "Proceedings title: The Future Of Journalism 2015: Risks, Threats and Opportunities",
  year   = "2015",
  pages  = "7--7",
}


@inbook{2cf3a78e7c914c72ae78bbb27c786359,
  title     = "LOD lab: Experiments at LOD scale",
  abstract  = "Contemporary Semantic Web research is in the business of optimizing algorithms for only a handful of datasets such as DBpedia, BSBM, DBLP and only a few more. This means that current practice does not generally take the true variety of Linked Data into account. With hundreds of thousands of datasets out in the world today the re- sults of SemanticWeb evaluations are less generalizable than they should and—this paper argues—can be. This paper describes LOD Lab: a fun- damentally different evaluation paradigm that makes algorithmic evalu- ation against hundreds of thousands of datasets the new norm. LOD Lab is implemented in terms of the existing LOD Laundromat architecture combined with the new open-source programming interface Frank that supportsWeb-scale evaluations to be run from the command-line.We il- lustrate the viability of the LOD Lab approach by rerunning experiments from three recent SemanticWeb research publications and expect it will contribute to improving the quality and reproducibility of experimental work in the Semantic Web community. We show that simply rerunning existing experiments within this new evaluation paradigm brings up in- teresting research questions as to how algorithmic performance relates to (structural) properties of the data. 1",
  author    = "Laurens Rietveld and Wouter Beek and Stefan Schlobach",
  year      = "2015",
  doi       = "10.1007/978-3-319-25010-6_23",
  isbn      = "9783319250090",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "339--355",
  booktitle = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
}


@inbook{003059bab43a4cfea14b6f9c81792be7,
  title     = "LOD Lab: Experiments at LOD Scale",
  author    = "L.J. Rietveld and W.G.J. Beek and S. Schlobach",
  year      = "2015",
  pages     = "339--355",
  booktitle = "The Semantic Web-ISWC 2015",
  publisher = "Springer International Publishing",
}


@misc{10773ee446144f8889d2a541e44b4469,
  title  = "LOD Lab: Experiments at LOD Scale (Compressed Contribution)",
  author = "L.J. Rietveld and W.G.J. Beek and S. Schlobach",
  note   = "Proceedings title: Proceedings of BNAIC 2015",
  year   = "2015",
}


@misc{bf34a1ca29a24eefbed37092286f292a,
  title    = "LOTUS: Linked open text unleashed",
  abstract = "It is dificult to find resources on the Semantic Web today, in particular if one wants to search for resources based on natural language keywords and across multiple datasets. In this paper, we present LOTUS: Linked Open Text UnleaShed, a full-text lookup index over a huge Linked Open Data collection. We detail LOTUS' approach, its implementation, its coverage, and demonstrate the ease with which it allows the LOD cloud to be queried in different domain-specific scenarios.",
  keywords = "Big Data, Findability, Semantic Search, Text Indexing",
  author   = "F. Ilievski and Wouter Beek and {Van Erp}, Marieke and Laurens Rietveld and Stefan Schlobach",
  year     = "2015",
}


@inbook{0bddd0fa7d2745f4b659b060e1597345,
  title     = "LOTUS: Linked Open Text UnleaShed",
  author    = "F. Ilievski and W.G.J. Beek and {Van Erp}, M. and L.J. Rietveld and K.S. Schlobach",
  year      = "2015",
  booktitle = "COLD workshop at ESWC",
}


@misc{d58f9739dfb04c9585aadb79a5a0d22c,
  title  = "Named Entity Disambiguation with two-stage coherence optimization",
  author = "F. Ilievski and {Van Erp}, M. and P.T.J.M. Vossen and K.S. Schlobach and W.G.J. Beek",
  note   = "Proceedings title: 25th Meeting of Computational Linguistics in the Netherlands",
  year   = "2015",
}


@article{2bbb756cc8d549ae9cf1e78dc13ed97a,
  title     = "On the Advantage of Using Dedicated Data Mining Techniques to Predict Colorectal Cancer",
  author    = "R. Kop and M. Hoogendoorn and L.N.G Moons and M.E. Numans and {ten Teije}, A.C.M.",
  note      = "Proceedings title: Proceedings of the 15th Conference on Artificial Intelligence in Medicine (AIME 2015) Publisher: Springer Place of publication: Berlin Editors: J.H. Holmes, R. Bellazzi, L. Sacchi, N. Peek",
  year      = "2015",
  volume    = "9105",
  pages     = "133--142",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@misc{f2949c1c28a4488bb10bcdf21d5fed4a,
  title   = "Ontology-based Software Architecture Documentation",
  author  = "{de Graaf}, K.A.",
  note    = "Exacte Wetenschappen Naam instelling promotie: VU University Naam instelling onderzoek: VU University",
  year    = "2015",
  school  = "Vrije Universiteit Amsterdam",
}


@inbook{3436a96fc8344c3994c9bc2da24e972e,
  title     = "Open Source Dutch WordNet",
  author    = "M.C. Postma and P.T.J.M. Vossen",
  year      = "2015",
  booktitle = "25th Meeting of Computational Linguistics in the Netherlands",
}


@article{ebcc6df4bd9e4a40bdf32aecd291a172,
  title     = "Patents as instruments for exploring innovation dynamics: geographic and technological perspectives on “photovoltaic cells”",
  abstract  = "The recently developed Cooperative Patent Classifications of the U.S. Patent and Trade Office (USPTO) and the European Patent Office (EPO) provide new options for an informed delineation of samples in both USPTO data and the Worldwide Patent Statistical Database (PatStat) of EPO. Among the ‘‘technologies for the mitigation of climate change’’ (class Y02), we zoom in on nine material technologies for photovoltaic cells; and focus on one of them (CuInSe2) as a lead case. Two recently developed techniques for making patent maps with interactive overlays—geographical ones using Google Maps and maps based on citation relations among International Patent Classifications (IPC)—are elaborated into dynamic versions that allow for online animations and comparisons by using split screens. Various forms of animation are discussed. The longitudinal development of Rao-Stirling diversity in the IPC-based maps provided us with a heuristics for studying technological diversity in terms of generations of the technology. The longitudinal patterns are clear in USPTO data more than in PatStat data because PatStat aggregates patent information from countries in different stages of technological development, whereas one can expect USPTO patents to be competitive at the technological edge.",
  author    = "L. Leydesdorff and F. Alkemade and G. Heimeriks and R.J. Hoekstra",
  year      = "2015",
  doi       = "10.1007/s11192-014-1447-8",
  volume    = "102",
  pages     = "629--651",
  journal   = "Scientometrics",
  issn      = "0138-9130",
  publisher = "Springer Netherlands",
  number    = "1",
}


@inbook{7e35bc89b7d441a499e18607b9905801,
  title     = "Processing of Events in Chinese Medical Guidelines and Automatic Generation of the Semantic Data",
  author    = "Y. Fan and J. Gu and Z. Huang",
  year      = "2015",
  booktitle = "Journal of Chinese Digital Medicine",
}


@inbook{d5855680969b492dbf48f27fc5899a81,
  title     = "Prov-O-Viz - Understanding the Role of Activities in Provenance",
  abstract  = "This paper presents PROV-O-Viz, aWeb-based visualizationtool for PROV-based provenance traces coming from various sources, that leverages Sankey Diagrams to reflect the flow of information through activities.We briefly discuss the advantages of this approach compared to other provenance visualization tools. PROV-O-Viz has already been used to visualize provenance traces generated by very different applications.",
  keywords  = "Information flow, Linked data, Provenance, Reusability, Sankey, Visualization",
  author    = "Rinke Hoekstra and Paul Groth",
  year      = "2015",
  doi       = "10.1007/978-3-319-16462-5_18",
  volume    = "8628",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "215--220",
  booktitle = "Provenance and Annotation of Data and Processes - 5th International Provenance and Annotation Workshop, IPAW 2014, Revised Selected Papers",
}


@inbook{5a276f356ca0483e9b2a28c47ab7259d,
  title     = "Semantic representation of evidence-based medical guidelines and its use cases",
  author    = "Q. Hu and Z. Huang and Jinguang Gu",
  year      = "2015",
  pages     = "397--404",
  booktitle = "Journal of Wuhan University (Natural Science Edition)",
}


@article{4db74fcd21e94f12a3087524690c0849,
  title     = "Semantics for Big Data",
  author    = "{van Harmelen}, F.A.H. and J.A. Hendler and P. Hitzler and K. Janowicz",
  note      = "PT: J; NR: 3; TC: 0; J9: AI MAG; PG: 2; GA: CG2BK; UT: WOS:000353079700001",
  year      = "2015",
  doi       = "10.1609/aimag.v36i1.2559",
  volume    = "36",
  pages     = "3--4",
  journal   = "The AI Magazine",
  issn      = "0738-4602",
  publisher = "AI Access Foundation",
  number    = "1",
}


@article{ce395ed7fb1e4849b0b97819b268f04f,
  title     = "Semantic Technologies for Historical Research: A Survey",
  abstract  = "During the nineties of the last century, historians and computer scientists created together a research agenda around the life cycle of historical information. It comprised the tasks of creation, design, enrichment, editing, retrieval, analysis and presentation of historical information with help of information technology. They also identified a number of problems and challenges in this field, some of them closely related to semantics and meaning. In this survey paper we study the joint work of historians and computer scientists in the use of Semantic Web methods and technologies in historical research. We analyse to what extent these contributions help in solving the open problems in the agenda of historians, and we describe open challenges and possible lines of research pushing further a still young, but promising, historical Semantic Web.",
  author    = "A. Merono and A. Ashkpour and {van Erp}, M.G.J. and K. Mandemakers and L. Breure and A. Scharnhorst and K.S. Schlobach and {van Harmelen}, F.A.H.",
  year      = "2015",
  doi       = "10.3233/SW-140158",
  volume    = "6",
  pages     = "539--564",
  journal   = "Semantic Web",
  issn      = "1570-0844",
  publisher = "IOS Press",
  number    = "6",
}


@book{57a9dfbd0c9d463393881dd4ab7a6853,
  title     = "Techniques of Knowledge Management for the Semantic Web",
  author    = "G Qi and Z. Huang and J. Du",
  year      = "2015",
  isbn      = "9787040437003",
  publisher = "Higher Education Press",
}


@book{874814a36ae64150a962b38011a681f1,
  title     = "The SMS platform - RISIS deliverable",
  author    = "{van den Besselaar}, P.A.A. and A. Khalili and A Idrissou and A. Loizou and {van Harmelen}, F.A.H.",
  year      = "2015",
  publisher = "Vrije Universiteit",
}


@article{5c4ec912a8724128a621d0bda7072d62,
  title     = "To SCRY Linked Data: Extending SPARQL the Easy Way",
  abstract  = "Scientific communities are increasingly publishing datasets on the Web following the Linked Data principles, storing RDF graphs in triplestores and making them available for querying through SPARQL. However, solving domain-specific problems often relies on information that cannot be included in such triplestores. For example, it is virtually impossible to foresee, and precompute, all statistical tests users will want to run on these datasets, especially if data from external triplestores is involved. A straightforward solution is to query the triplestore with SPARQL and compute the required information post-hoc. However, post-hoc scripting is laborious and typically not reusable, and the computed information is not accessible within the original query. Other solutions allow this computation to happen at query time, as with SPARQL Extensible Value Testing (EVT) and Linked Data APIs. However, such approaches can be difficult to apply, due to limited interoperability and poor extensibility. In this paper we present SCRY, the SPARQL compatible service layer, which is a lightweight SPARQL endpoint that interprets parts of basic graph patterns as calls to user defined services. SCRY allows users to incorporate algorithms of arbitrary complexity within standards-compliant SPARQL queries, and to use the generated outputs directly within these same queries. Unlike traditional SPARQL endpoints, the RDF graph against which SCRY resolves its queries is generated at query time, by executing services encoded in the basic graph patterns. SCRY's federation-oriented design allows for easy integration with existing SPARQL endpoints, effectively extending their functionality in a decoupled, tool independent way and allowing the power of SemanticWeb technology to be more easily applied to domain-specific problems.",
  author    = "Bas Stringer and Albert Meroño-Peñuela and Anthonis Loizou and Sanne Abeln and Jaap Heringa",
  year      = "2015",
  volume    = "1501",
  pages     = "8--14",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@misc{4bcb545190dd4f6fa9ebc3994f6ab0f7,
  title    = "Updating Compressed Column-Stores",
  abstract = "12638",
  author   = "S.A.B.C. Heman",
  note     = "Exacte Wetenschappen Naam instelling promotie: VU University Naam instelling onderzoek: VU University",
  year     = "2015",
  school   = "Vrije Universiteit Amsterdam",
}


@inbook{39cec9e5b32a474a9a5c5f053031bfd3,
  title     = "USING ONTOLOGY TECHNOLOGY IN BIM SYSTEM",
  author    = "H. Wang and Z. Huang and N. Zhong",
  year      = "2015",
  booktitle = "Journal of Industrial Construction",
}


@inbook{903d15404652483fb5177c158462b43e,
  title     = "What happened to …?” Entity-based Timeline Extraction",
  author    = "T. Caselli and A.S. Fokkens-Zwirello and R. Morante and P.T.J.M. Vossen",
  year      = "2015",
  booktitle = "25th Meeting of Computational Linguistics in the Netherlands",
}


@article{5f009551bb6544cb85bc3b285bbea25d,
  title     = "WYSIWYM - Integrated visualization, exploration and authoring of semantically enriched un-structured content",
  abstract  = "The Semantic Web and Linked Data gained traction in the last years. However, the majority of information still is contained in unstructured documents. This can also not be expected to change, since text, images and videos are the natural way how humans interact with information. Semantic structuring on the other hand enables the (semi-)automatic integration, repurposing, rearrangement of information. NLP technologies and formalisms for the integrated representation of unstructured and semantic content (such as RDFa and Microdata) aim at bridging this semantic gap. However, in order for humans to truly benefit from this integration, we need ways to author, visualize and explore unstructured and semantically enriched content in an integrated manner. In this paper, we present the WYSIWYM (What You See is What You Mean) concept, which addresses this issue and formalizes the binding between semantic representation models and UI elements for authoring, visualizing and exploration. With RDFaCE, Pharmer and conTEXT we present and evaluate three complementary showcases implementing the WYSIWYM concept for different application domains.",
  keywords  = "authoring, exploration, Semantic Web, visual mapping, Visualization, WYSIWYG, WYSIWYM",
  author    = "Ali Khalili and Soren Auer",
  year      = "2015",
  doi       = "10.3233/SW-140157",
  volume    = "6",
  pages     = "259--275",
  journal   = "Semantic Web",
  issn      = "1570-0844",
  publisher = "IOS Press",
  number    = "3",
}


@article{5526532ed2bf45d0a35184eb812c11c1,
  title     = "WaaS: Wisdom as a service",
  abstract  = "An emerging hyper-world encompasses all human activities in a social-cyber-physical space. Its power derives from the Wisdom Web of Things (W2T) cycle, namely, 'from things to data, information, knowledge, wisdom, services, humans, and then back to things.'' The W2T cycle leads to a harmonious symbiosis among humans, computers, and things, which can be constructed by large-scale converging of intelligent information technology applications with an open and interoperable architecture. The recent advances in cloud computing, the Internet of Things, Web of Things, Big Data, and other research fields have provided just such an open system architecture with resource sharing and services. The next step is to develop an open and interoperable content architecture with intelligent sharing and services for the organization and transformation in the data, information, knowledge, and wisdom (DIKW) hierarchy. This article introduces wisdom as a service (WaaS), a content architecture based on the pay-as-you-go IT trend. The WaaS infrastructure and the main challenges in WaaS research and applications are discussed. A case study is also described. Relying on cloud computing and big data, WaaS provides a practical approach to realize the W2T cycle in the hyper-world for the coming age of ubiquitous intelligent IT applications.",
  keywords  = "Big Data, cloud computing, hyper-world, intelligent systems, IoT, W2T cycle, WaaS, WoT",
  author    = "Jianhui Chen and Jianhua Ma and Ning Zhong and Yiyu Yao and Jiming Liu and Runhe Huang and Wenbin Li and Zhisheng Huang and Yang Gao and Jianping Cao",
  year      = "2014",
  month     = "11",
  doi       = "10.1109/MIS.2014.19",
  volume    = "29",
  pages     = "40--47",
  journal   = "IEEE Intelligent Systems",
  issn      = "1541-1672",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  number    = "6",
}


@inbook{d2310ad12c5c4c5db6aa7eca3c9ab94c,
  title     = "A Conceptual Model for Detecting Interactions among Medical Recommendations in Clinical Guidelines",
  abstract  = "Representation of clinical knowledge is still an open research topic. In particular, classical languages designed for representing clinical guidelines, which were meant for producing diagnostic and treatment plans, present limitations such as for re-using, combining, and reasoning over existing knowledge. In this paper, we address such limitations by proposing an extension of the TMR conceptual model to represent clinical guidelines that allows re-using and combining knowledge from several guidelines to be applied to patients with multimorbidities. We provide means to (semi)automatically detect interactions among recommendations that require some attention from experts, such as recommending more than once the same drug. We evaluate the model by applying it to a realistic case study involving 3 diseases (Osteoarthritis, Hypertension and Diabetes) and compare the results with two other existing methods.",
  keywords  = "Clinical knowledge representation, Combining medical guidelines, Multimorbidity, Reasoning",
  author    = "{Carretta Zamborlini}, Veruska and {Da Silveira}, Marcos and Cedric Pruski and Rinke Hoekstra and {ten Teije}, Annette and {van Harmelen}, Frank",
  note      = "Proceedings title: Proceedings of the 19th International Conference on Knowledge Engineering and Knowledge Management (EKAW 2014) Publisher: Springer Editors: K Janowicz, S Schlobach, S Lambrix, E Hyvonen",
  year      = "2014",
  volume    = "8876",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  pages     = "591--606",
  booktitle = "Knowledge Engineering and Knowledge Management - 19th International Conference, EKAW 2014, Proceedings",
}


@inbook{e5d1e3e7c14c4001b3c0a19e8e714fe7,
  title     = "Advances in Large-Scale RDF Data Management",
  author    = "P.A. Boncz and O. Erling and M.D. Pham",
  year      = "2014",
  series    = "LNCS 8661",
  publisher = "Springer",
  pages     = "21--44",
  booktitle = "Linked Open Data",
}


@inbook{38890af079984fd4ae0fcb9ceed8dfe1,
  title     = "A Network Analysis of Dutch Regulations",
  author    = "R.J. Hoekstra",
  year      = "2014",
  isbn      = "9788849527698",
  editor    = "Radboud Winkels and Nicola Lettieri and Sebastiano Faro",
  booktitle = "Network Analysis in Law",
}


@article{2e5460ceafef430e8b4f948f915bf90c,
  title     = "API-centric Linked Data Integration: The Open PHACTS Discovery Platform Case Study",
  abstract  = "Data integration is a key challenge faced in pharmacology where there are numerous heterogeneous databases spanning multiple domains (e.g. chemistry and biology). To address this challenge, the Open PHACTS consortium has developed the Open PHACTS Discovery Platform that leverages Linked Data to provide integrated access to pharmacology databases. Between its launch in April 2013 and March 2014, the platform has been accessed over 13.5 million times and has multiple applications that integrate with it. In this work, we discuss how Application Programming Interfaces can extend the classical Linked Data Application Architecture to facilitate data integration. Additionally, we show how the Open PHACTS Discovery Platform implements this extended architecture.",
  author    = "P.T. Groth and A. Loizou and A.J.G Gray and C.A. Goble and L Harland and S Pettifier",
  year      = "2014",
  doi       = "10.1016/j.websem.2014.03.003",
  journal   = "Journal of Web Semantics",
  issn      = "1570-8268",
  publisher = "Elsevier",
}


@article{da523417e0894dfb85435688a3609874,
  title     = "Applying linked data approaches to pharmacology: Architectural decisions and implementation",
  abstract  = "The discovery of new medicines requires pharmacologists to interact with a number of information sources ranging from tabular data to scientific papers, and other specialized formats. In this application report, we describe a linked data platform for integrating multiple pharmacology datasets that form the basis for several drug discovery applications. The functionality offered by the platform has been drawn from a collection of prioritised drug discovery business questions created as part of the Open PHACTS project, a collaboration of research institutions and major pharmaceutical companies. We describe the architecture of the platform focusing on seven design decisions that drove its development with the aim of informing others developing similar software in this or other domains. The utility of the platform is demonstrated by the variety of drug discovery applications being built to access the integrated data. An alpha version of the OPS platform is currently available to the Open PHACTS consortium and a first public release will be made in late 2012, see http://www.openphacts.org/ for details. © 2014 - IOS Press and the authors.",
  author    = "A.J.G Gray and P.T. Groth and A. Loizou and S Askjaer and C Brenninkmeijer and K. Burger and C. Chichester and C.T. Evelo and C.A. Goble and L Harland and S Pettifier and M. Thompson and A Waagmeester and A.J William",
  year      = "2014",
  doi       = "10.3233/SW-2012-0088",
  volume    = "5",
  pages     = "101--114",
  journal   = "Semantic Web",
  issn      = "1570-0844",
  publisher = "IOS Press",
  number    = "2",
}


@inbook{3fbc7f197cde4e729501efbdf251c04c,
  title     = "A Pragmatic Semantics for Identity in Linked Data",
  author    = "W.G.J. Beek and K.S. Schlobach and {van Harmelen}, F.A.H.",
  year      = "2014",
  booktitle = "The 14th International Conference on Principles of Knowledge Representation and Reasoning",
}


@inbook{23a86df0d2f94f6cac3242ea35fe3a44,
  title     = "A Web Observatory for the Machine Processability of Structured Data on the Web",
  author    = "W.G.J. Beek and P.T. Groth and K.S. Schlobach and R.J. Hoekstra",
  year      = "2014",
  pages     = "249--250",
  booktitle = "Proceedings of the 2014 ACM Conference on Web Science",
}


@article{fabc3899a46a41c5870f495c1aef750f,
  title   = "Benchmarking Linked Open Data Management Systems",
  author  = "{Angles Rojas}, R. and M.D. Pham and P.A. Boncz",
  year    = "2014",
  volume  = "96",
  pages   = "24--25",
  journal = "ERCIM NEWS",
  issn    = "0926-4981",
}


@misc{ab082add992348c7b7e737da24b5855f,
  title   = "Computing Healthcare Quality Indicators Automatically: Secondary Use of Patient Data and Semantic Interoperability",
  author  = "K. Dentler",
  note    = "Naam instelling promotie: VU Vrije Universiteit Naam instelling onderzoek: VU Vrije Universiteit",
  year    = "2014",
  school  = "Vrije Universiteit Amsterdam",
}


@article{104f96ec6d1b4495a7fe2d6887d1b81f,
  title     = "Conflict Resolution in Partially Ordered OWL DL Ontologies",
  abstract  = "Inconsistency handling in OWL DL ontologies is an important problem because an ontology can easily be inconsistent when it is generated or modified. Current approaches to dealing with inconsistent ontologies often assume that there exists a total order over axioms and use such an order to select axioms to remove. However, in some cases, such as ontology merging, a total order may not be available and we only have a partial order over axioms. In this paper, we consider a general notion of logical inconsistency and define the notion of conflict of an inconsistent ontology. We then propose a general approach to resolving inconsistency of a partially ordered ontology. We instantiate this approach by proposing two algorithms to calculate prioritized hitting sets for a set of conflicts. We implement the algorithms and provide evaluation results on the efficiency and effectiveness by considering both artificial and real-life data sets.",
  author    = "Q. Ji and Z. Gao and Z. Huang",
  note      = "Proceedings title: Proceedings of 21st European Conference on Artificial Intelligence (ECAI2014) Publisher: IOS Press",
  year      = "2014",
  doi       = "10.3233/978-1-61499-419-0-471",
  volume    = "263",
  pages     = "471--476",
  journal   = "Frontiers in Artificial Intelligence and Applications",
  issn      = "0922-6389",
  publisher = "IOS Press",
}


@inbook{044e7a11b506487bb4b1a03d7463bbe4,
  title     = "Constructing Provenance Cubes Based on Semantic Neuroimaging Data Provenances",
  author    = "J. Chen and J Feng and N. Zhong and Z. Huang",
  year      = "2014",
  booktitle = "Proceedings of 8th China Semantic Web Symposium & 3rd Web Science Conference (CSWS2014)",
}


@book{63278e6c72b044e885222949522971f2,
  title     = "Designing engines for data analysis",
  author    = "P.A. Boncz",
  note      = "Inaugurele rede - 2014-10-17",
  year      = "2014",
  publisher = "Vrije Universiteit Amsterdam",
}


@inbook{fbdbf7048b784b69abd61d45c084b6e8,
  title     = "Detecting Stay Areas from a User's Mobile Phone Data for Urban Computing",
  author    = "H. Wang and N. Zhong and Z. Huang and J. Huang and E. Zhou",
  year      = "2014",
  pages     = "336--346",
  booktitle = "The 2014 International Conference on Active Media Technology",
  publisher = "Springer",
}


@article{d8abe7a803604a129c1f71a9101754a3,
  title     = "Drug discovery FAQs: workflows for answering multidomain drug discovery questions",
  abstract  = "Modern data-driven drug discovery requires integrated resources to support decision-making and enable new discoveries. The Open PHACTS Discovery Platform (http://dev.openphacts.org) was built to address this requirement by focusing on drug discovery questions that are of high priority to the pharmaceutical industry. Although complex, most of these frequently asked questions (FAQs) revolve around the combination of data concerning compounds, targets, pathways and diseases. Computational drug discovery using workflow tools and the integrated resources of Open PHACTS can deliver answers to most of these questions. Here, we report on a selection of workflows used for solving these use cases and discuss some of the research challenges. The workflows are accessible online from myExperiment (http://www.myexperiment.org) and are available for reuse by the scientific community.",
  author    = "C. Chichester and D. Digles and R.M. Siebes and A. Loizou and P.T. Groth and L Harland",
  year      = "2014",
  doi       = "10.1016/j.drudis.2014.11.006",
  journal   = "Drug Discovery Today",
  issn      = "1359-6446",
  publisher = "Elsevier Limited",
}


@inbook{9700d89776784a4d81bfe8f2985049a3,
  title     = "Evidence-based clinical guidelines in SemanticCT",
  author    = "Q. Hu and Z. Huang and {van Harmelen}, F.A.H. and {ten Teije}, A.C.M. and Jinguang Gu",
  year      = "2014",
  booktitle = "8th China Semantic Web Symposium & 3rd Web Science Conference (CSWS2014)",
}


@article{45878f29b4ee405488c133a1c20bdaa2,
  title     = "Exploring killer ads: A terror management account of death in advertisements",
  abstract  = "Building on terror management theory, three experiments tested whether advertisements with a mortality reminder increase purchase intentions for products that provide an (un)important source of self-esteem. Study 1 tested the effects of mortality salience in advertisements (mortality reminder: yes vs. no) for art library and newspaper subscriptions. Study 2 used a 2 (mortality reminder: yes vs. no) × 2 (brand familiarity: low vs. high) between subjects design for newspaper subscriptions. Study 3 used a 2 (mortality reminder: yes vs. no) × 2 (product type: healthy vs. unhealthy) between subjects design for a beverage with importance of the product for consumers' self-esteem as a continuous moderator. Main dependent measures were mood; death-related thoughts; attitudes toward the ad; and purchase intentions. Across studies advertisements with a death reminder increased unconscious thoughts about death (Experiments 1-3), which, in turn, increased purchase intentions (Experiments 2 and 3). These effects occurred independent of mood, brand familiarity, product type, product relevance to self-esteem, and ad liking. Findings suggest that {"}killer ads{"} trigger unconscious consumer fears that may be alleviated by the urge to buy.",
  author    = "H.H.J. Das and R. Duiven and J.L. Arendsen and I.E. Vermeulen",
  year      = "2014",
  doi       = "10.1002/mar.20737",
  volume    = "31",
  pages     = "828--842",
  journal   = "Psychology & Marketing",
  issn      = "0742-6046",
  publisher = "Wiley-Liss Inc.",
  number    = "10",
}


@inbook{743083cf4b644e20a6146f1ac7d964c9,
  title     = "Feasibility Estimation for Clinical Trials",
  author    = "Z. Huang and {van Harmelen}, F.A.H. and {ten Teije}, A.C.M. and A. Dekker",
  year      = "2014",
  pages     = "68--77",
  booktitle = "Proceedings of the 7th International Conference on Health Informatics (HEALTHINF2014).",
  publisher = "scitepress digital library",
}


@article{d2f427f3875c41fe90b71a3fc5ba3dc4,
  title     = "Formalization and computation of quality measures based on electronic medical records",
  abstract  = "Objective: Ambiguous definitions of quality measures in natural language impede their automated computability and also the reproducibility, validity, timeliness, traceability, comparability, and interpretability of computed results. Therefore, quality measures should be formalized before their release. We have previously developed and successfully applied a method for clinical indicator formalization (CLIF). The objective of our present study is to test whether CLIF is generalizable- that is, applicable to a large set of heterogeneous measures of different types and from various domains. Materials and methods: We formalized the entire set of 159 Dutch quality measures for general practice, which contains structure, process, and outcome measures and covers seven domains. We relied on a web-based tool to facilitate the application of our method. Subsequently, we computed the measures on the basis of a large database of real patient data. Results: Our CLIF method enabled us to fully formalize 100% of the measures. Owing to missing functionality, the accompanying tool could support full formalization of only 86% of the quality measures into Structured Query Language (SQL) queries. The remaining 14% of the measures required manual application of our CLIF method by directly translating the respective criteria into SQL. The results obtained by computing the measures show a strong correlation with results computed independently by two other parties. Conclusions: The CLIF method covers all quality measures after having been extended by an additional step. Our web tool requires further refinement for CLIF to be applied completely automatically. We therefore conclude that CLIF is sufficiently generalizable to be able to formalize the entire set of Dutch quality measures for general practice.",
  author    = "K. Dentler and M.E. Numans and {ten Teije}, A.C.M. and R. Cornet and {de Keizer}, N.F.",
  year      = "2014",
  doi       = "10.1136/amiajnl-2013-001921",
  volume    = "21",
  pages     = "285--291",
  journal   = "Journal of the American Medical Informatics Association",
  issn      = "1067-5027",
  publisher = "Oxford University Press",
  number    = "2",
}


@inbook{6c0060454640430fb3856e1dd017e2f5,
  title     = "Generating Scientific Documentation for Computational Experiments Using Provenance",
  author    = "A. Wibisono and Peter Bloem and {de Vries}, G.K.D and P.T. Groth and A. Belloum and M. Bubak",
  year      = "2014",
  booktitle = "5th International Provenance and Annotation Workshop (IPAW'14)",
}


@inbook{990518b0126a4f4eaff2b7c30046ee4d,
  title     = "How to generate query parameters in RDF benchmarks?",
  author    = "A. Gubichev and {Angles Rojas}, R. and P.A. Boncz",
  year      = "2014",
  booktitle = "Proceedings of the Data Engineering meets the Semantic Web (DESWEB, 2014)",
}


@article{b73c2d7620a24b8da05009bf44940434,
  title     = "Influence of data quality on computed Dutch hospital quality indicators: a case study in colorectal cancer surgery.",
  abstract  = "Background: Our study aims to assess the influence of data quality on computed Dutch hospital quality indicators, and whether colorectal cancer surgery indicators can be computed reliably based on routinely recorded data from an electronic medical record (EMR). Methods. Cross-sectional study in a department of gastrointestinal oncology in a university hospital, in which a set of 10 indicators is computed (1) based on data abstracted manually for the national quality register Dutch Surgical Colorectal Audit (DSCA) as reference standard and (2) based on routinely collected data from an EMR. All 75 patients for whom data has been submitted to the DSCA for the reporting year 2011 and all 79 patients who underwent a resection of a primary colorectal carcinoma in 2011 according to structured data in the EMR were included. Comparison of results, investigating the causes for any differences based on data quality analysis. Main outcome measures are the computability of quality indicators, absolute percentages of indicator results, data quality in terms of availability in a structured format, completeness and correctness. Results: All indicators were fully computable based on the DSCA dataset, but only three based on EMR data, two of which were percentages. For both percentages, the difference in proportions computed based on the two datasets was significant.All required data items were available in a structured format in the DSCA dataset. Their average completeness was 86%, while the average completeness of these items in the EMR was 50%. Their average correctness was 87%. Conclusions: Our study showed that data quality can significantly influence indicator results, and that our EMR data was not suitable to reliably compute quality indicators. EMRs should be designed in a way so that the data required for audits can be entered directly in a structured and coded format. © 2014 Dentler et al.; licensee BioMed Central Ltd.",
  author    = "K. Dentler and R. Cornet and {ten Teije}, A.C.M. and P. Tanis and J. Klinkenbijl and K. Tytgat and {de Keizer}, N.F.",
  year      = "2014",
  doi       = "10.1186/1472-6947-14-32",
  volume    = "14",
  journal   = "BMC Medical Informatics and Decision Making",
  issn      = "1472-6947",
  publisher = "BioMed Central",
  number    = "32",
}


@inbook{f8b9e534ae1f4e73a2640aeaa020ba40,
  title     = "Integrating Brain Big Data with Depression Diagnostic Information by DataBrain Method",
  author    = "T. Kotake and J. Chen and N. Zhong and H. Zhong and J. Han and Z. Huang and Y. Yang",
  year      = "2014",
  booktitle = "Proceedings of 2014 International Conference on Brain Informatics and Health(BIH2014)",
  publisher = "Springer",
}


@book{61b369b6e8724971ae4f2327b65a7be5,
  title     = "Knowledge Engineering and Knowledge Management: 19th International Conference, EKAW 2014",
  author    = "K. Janowicz and K.S. Schlobach and P. Lambrix and E. Hyvönen",
  year      = "2014",
  publisher = "Springer",
}


@book{6d7510512e6a4df19cab7da812b79d6f,
  title     = "Knowledge Representation for Health Care (6th International Workshop, KR4HC 2014)",
  author    = "S Miksch and D Riano and {ten Teije}, A.C.M.",
  year      = "2014",
  doi       = "10.1007/978-3-319-13281-5",
  isbn      = "9783319132815",
  series    = "Lecture Notes CS",
  publisher = "Springer",
  number    = "Vol. 8903",
}


@inbook{9a5b54119a574d658275326369fbdd21,
  title     = "LDBC: benchmarks for graph and RDF data management",
  author    = "P.A. Boncz",
  year      = "2014",
  booktitle = "International Database Engineering and Applications Symposium (IDEAS 2014)",
}


@book{d291f7e9825642388f60aad7cb11fec6,
  title     = "Legal Knowledge and Information Systems - The Twenty Seventh Annual Conference (JURIX 2014)",
  author    = "R.J. Hoekstra",
  note      = "Gebeurtenis: JURIX 2014",
  year      = "2014",
  isbn      = "9781614994671",
  series    = "Frontiers in Artificial Intelligence",
  publisher = "IOS Press",
}


@article{66926faac42a487bb47bf6ec8a5f58b2,
  title     = "Let's {"}Downscale{"} Linked Data",
  abstract  = "Open data policies and linked data publication are powerful tools for increasing transparency, participatory governance, and accountability. The linked data community proudly emphasizes the economic and societal impact such technology shows. But a closer look proves that the design and deployment of these technologies leave out most of the world's population. The good news is that it will take small but fundamental changes to bridge this gap. Research agendas should be upated to design systems for small infrastructure, provide multimodal interfaces to data, and account better for locally relevant, contextualized data. Now is the time to act, because most linked data technologies are still in development.",
  author    = "C.D.M. Gueret and {de Boer}, V. and K.S. Schlobach",
  year      = "2014",
  doi       = "10.1109/MIC.2014.29",
  volume    = "18",
  pages     = "70--73",
  journal   = "IEEE Internet Computing",
  issn      = "1089-7801",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  number    = "2",
}


@misc{9fc0a0c2a1364e01ab31084f6674584e,
  title  = "Linking the STCN and Performing Big Data Queries in the Humanities",
  author = "W.G.J. Beek and R.J. Hoekstra and F. Maas and A. Merono and I.B. Leemans",
  note   = "Proceedings title: Digital Humanities Benelux Conference 2014 Place of publication: The Hague",
  year   = "2014",
}


@inbook{0ff21dea26bc4ae197cfe5235c110ca1,
  title     = "Linkitup: Semantic Publishing of Research Data",
  author    = "R.J. Hoekstra and P.T. Groth and M. Charalganov",
  year      = "2014",
  editor    = "V Presutti and M Stankovic and E Cambria",
  booktitle = "Proceedings of the Semantic Web Evaluation Challenge at ESWC 2014",
  publisher = "Springer",
}


@inbook{68bd3872218f425eb23543a970e6a3f2,
  title     = "LOD laundromat: A uniform way of publishing other people’s dirty data",
  author    = "W.G.J. Beek and L.J. Rietveld and H. Bazoobandi and J. Wielemaker and K.S. Schlobach",
  year      = "2014",
  pages     = "213--228",
  booktitle = "The Semantic Web–ISWC 2014",
  publisher = "Springer International Publishing",
}


@misc{29b151ebee5c48ddbb483e98b5ab48a3,
  title  = "LOD Laundromat: A Uniform Way of Publishing Other People's Dirty Data.",
  author = "{van Beek}, W. and L.J. Rietveld and H. Bazoobandi and J. Wielemaker and K.S. Schlobach",
  year   = "2014",
}


@inbook{2c868bd438b442baab7b42ba4cb4c98f,
  title     = "LOD Laundromat: A Uniform Way of Publishing Other People's Dirty Data.",
  author    = "W.G.J. Beek and L.J. Rietveld and H. Bazoubandi and J. Wielemaker and K.S. Schlobach",
  year      = "2014",
  booktitle = "The Semantic Web–ISWC 2014",
}


@inbook{f2635d22c7e040b9872801bbb75e9a13,
  title     = "LOD Laundromat: Extended Abstract",
  author    = "W.G.J. Beek and L.J. Rietveld and H. Bazoubandi and J. Wielemaker and K.S. Schlobach",
  year      = "2014",
  booktitle = "Proceedings of the Belgisch-Nederlandse Kunstmatige Intelligentie Conferentie (BNAIC) 2014",
}


@misc{181a1e36dced43c79026afd93cbb73a8,
  title  = "Man vs. Machine: Differences in SPARQL Queries.",
  author = "L.J. Rietveld and R.J. Hoekstra",
  year   = "2014",
}


@article{3bddd5d360c14fae949a6caef6ffe852,
  title     = "Measuring effectiveness of ontology debugging systems",
  author    = "J. Qiu and Z. Gao and Z. Huang and Man Zhu",
  year      = "2014",
  volume    = "11",
  journal   = "Knowledge-Based Systems",
  issn      = "0950-7051",
  publisher = "Elsevier",
}


@article{de979c20a4974979b1567ad4056b9cf9,
  title   = "MonetDB/RDF: Discovering and Exploiting the Emergent Schema of RDF Data",
  author  = "M.D. Pham and P.A. Boncz",
  year    = "2014",
  volume  = "96",
  pages   = "41--42",
  journal = "ERCIM NEWS",
  issn    = "0926-4981",
}


@inbook{796639283c6e4c59b8807f555a7a0ccb,
  title     = "Morsel-Driven Parallelism: A NUMA-Aware Query Evaluation Framework for the Many-Core Age",
  author    = "V. Leis and P.A. Boncz and T. Neumann and A. Kemper",
  year      = "2014",
  booktitle = "Proc. ACM SIGMOD",
}


@article{8343453179784d86a3dc916e67493551,
  title     = "On the formulation of performant sparql queries",
  abstract  = "Abstract The combination of the flexibility of RDF and the expressiveness of SPARQL provides a powerful mechanism to model, integrate and query data. However, these properties also mean that it is nontrivial to write performant SPARQL queries. Indeed, it is quite easy to create queries that tax even the most optimised triple stores. Currently, application developers have little concrete guidance on how to write {"}good{"} queries. The goal of this paper is to begin to bridge this gap. It describes 5 heuristics that can be applied to create optimised queries. The heuristics are informed by formal results in the literature on the semantics and complexity of evaluating SPARQL queries, which ensures that queries following these rules can be optimised effectively by an underlying RDF store. Moreover, we empirically verify the efficacy of the heuristics using a set of openly available datasets and corresponding SPARQL queries developed by a large pharmacology data integration project. The experimental results show improvements in performance across six state-of-the-art RDF stores.",
  author    = "A. Loizou and R. Angles and P.T. Groth",
  year      = "2014",
  doi       = "10.1016/j.websem.2014.11.003",
  journal   = "Journal of Web Semantics",
  issn      = "1570-8268",
  publisher = "Elsevier",
}


@inbook{79b478241fc6419187d67dee14a66d3e,
  title     = "Parameter Curation for Benchmark Queries",
  abstract  = "In this paper we consider the problem of generating parameters for benchmark queries so these have stable behavior despite being executed on datasets (real-world or synthetic) with skewed data distributions and value correlations. We show that uniform random sampling of the substitution parameters is not well suited for such benchmarks, since it results in unpredictable runtime behavior of queries. We present our approach of Parameter Curation with the goal of selecting parameter bindings that have consistently low-variance intermediate query result sizes throughout the query plan. Our solution is illustrated with IMDB data and the recently proposed LDBC Social Network Benchmark (SNB).",
  author    = "Andrey Gubichev and Peter Boncz",
  year      = "2014",
  doi       = "10.1007/978-3-319-15350-6_8",
  volume    = "8904",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "113--129",
  booktitle = "Performance Characterization and Benchmarking: Traditional to Big Data - 6th TPC Technology Conference, TPCTC 2014, Revised Selected Papers",
}


@article{602fdc1b78e94ddfaece272249d7b258,
  title     = "Querying neXtProt nanopublications and their value for insights on sequence variants and tissue expression",
  abstract  = "Understanding how genetic differences between individuals impact the regulation, expression, and ultimately function of proteins is an important step toward realizing the promise of personal medicine. There are several technical barriers hindering the transition of biological knowledge into the applications relevant to precision medicine. One important challenge for data integration is that new biological sequences (proteins, DNA) have multiple issues related to interoperability potentially creating a quagmire in the published data, especially when different data sources do not appear to be in agreement. Thus, there is an urgent need for systems and methodologies to facilitate the integration of information in a uniform manner to allow seamless querying of multiple data types which can illuminate, for example, the relationships between protein modifications and causative genomic variants. Our work demonstrates for the first time how semantic technologies can be used to address these challenges using the nanopublication model applied to the neXtProt data set, a curated knowledgebase of information about human proteins. We have applied the nanopublication model to demonstrate querying over several named graphs, including the provenance information associated with the curated scientific assertions from neXtProt. We show by way of use cases using sequence variations, post-translational modifications (PTMs) and tissue expression, that querying the neXtProt nanopublication implementation is a credible approach for expanding biological insight.",
  author    = "C. Chichester and P. Gaudet and R. Karch and P.T. Groth and L Lane and A Bairoch and B Mons and A. Loizou",
  year      = "2014",
  doi       = "10.1016/j.websem.2014.05.001",
  journal   = "Journal of Web Semantics",
  issn      = "1570-8268",
  publisher = "Elsevier",
}


@article{91cb952159614b12b8d6ff7fff464f22,
  title     = "Reports on the 2013 AAAI fall symposium series",
  abstract  = "The Association for the Advancement of Artificial Intelligence was pleased to present the 2013 Fall Symposium Series, held Friday through Sunday, November 15-17, at the Westin Arlington Gateway in Arlington, Virginia, near Washington, D.C., USA. The titles of the five symposia were Discovery Informatics: AI Takes a Science- Centered View on Big Data (FS-13-01); How Should Intelligence Be Abstracted in AI Research: MDPs, Symbolic Representations, Artificial Neural Networks, or - ? (FS-13-02); Integrated Cognition (FS-13-03); Semantics for Big Data (FS- 13-04); and Social Networks and Social Contagion: Web Analytics and Computational Social Science (FS-13-05). The highlights of each symposium are presented in this report.",
  author    = "Burns, {Gully A P C} and Yolanda Gil and Natalia Villanueva-Rosales and Yan Liu and Sebastian Risi and Joel Lehman and Jeff Clune and Christian Lebiere and Rosenbloom, {Paul S.} and {Van Harmelen}, Frank and Hendler, {James A.} and Pascal Hitzler and Krzysztof Janowicz and Samarth Swarup",
  year      = "2014",
  volume    = "35",
  pages     = "69--74",
  journal   = "The AI Magazine",
  issn      = "0738-4602",
  publisher = "AI Access Foundation",
  number    = "2",
}


@inbook{56aeef4da7c8498eac1532de8dfedce5,
  title     = "Rough set semantics for identity on the Web",
  abstract  = "Identity relations are at the foundation of many logic-based knowledge representations. We argue that the traditional notion of equality, is unsuited for many realistic knowledge representation settings. The classical interpretation of equality is too strong when the equality statements are re-used outside their original context. On the Semantic Web, equality statements are used to interlink multiple descriptions of the same object, using owl:sameAs assertions. And indeed, many practical uses of owl:sameAs are known to violate the formal Leibniz-style semantics. We provide a more flexible semantics to identity by assigning meaning to the subrelations of an identity relation in terms of the predicates that are used in a knowledge-base. Using those indiscernability-predicates, we define upper and lower approximations of equality in the style of rought-set theory, resulting in a quality-measure for identity relations.",
  author    = "Wouter Beek and Stefan Schlobach and {van Harmelen}, Frank",
  year      = "2014",
  pages     = "587--589",
  booktitle = "14th International Conference on the Principles of Knowledge Representation and Reasoning, KR 2014",
  publisher = "AAAI Press",
}


@inbook{193c4d80ef3242c096790fea37f8b36f,
  title     = "Scientific Lenses to Support Multiple Views over Linked Chemistry Data",
  author    = "C. Batchelor and C Brenninkmeijer and C. Chichester and M. Davies and D. Digles and I. Dunlop and C.T. Evelo and A. Gaulton and C.A. Goble and A.J.G Gray and P.T. Groth and L Harland and K. Karapetyan and A. Loizou and J.P. Overington and S. Pettifer and J. Steele and R. Stevens and V. Tkachenko and A Waagmeester and A.J. Williams and E.L Willighagen",
  year      = "2014",
  booktitle = "The Semantic Web–ISWC 2014",
}


@article{1165c9b0439c4faa87efa6098120a44a,
  title     = "Semantic Representation of Evidence-based Clinical Guidelines",
  author    = "Z. Huang and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H. and S. Ait-Mokhtar",
  note      = "Proceedings title: 6th International Workshop on Knowledge Representation for Health Care (KR4HC2014) Publisher: Springer ISBN: 978-3-319-13281-5 Editors: S. Miksch, D. Riano",
  year      = "2014",
  volume    = "8903",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@article{6570c360d0544bbdb575308926eb82cf,
  title    = "Semantic Representation of Evidence-based Clinical Guidelines",
  abstract = "Evidence-based Clinical Guidelines (EbCGs) are that the document or recommendation has been created using the best clinical research findings of the highest value to aid in the delivery of optimum clinical care to patients. In this paper, we propose a lightweight formal-ism of evidence-based clinical guidelines by introducing the Semantic Web Technology for it. With the help of the tools which have been de-veloped in the Semantic Web and Natural Language Processing (NLP), the generation of the formulations of evidence-based clinical guidelines become much easy. We will discuss several use cases of the semantic representation of EbCGs, and argue that it is potentially useful for the applications of the semantic web technology on the medical domain.",
  author   = "Zhisheng Huang and Harmelen, {Frank Van}",
  year     = "2014",
  doi      = "10.1007/978-3-319-13281-5_6",
  pages    = "78--94",
  journal  = "6th International Workshop on Knowledge Representation for Health Care {KR4HC}",
}


@inbook{a81fff84e674429290be451efa2fe20a,
  title     = "Simplifying RDF Data for Graph-Based Machine Learning.",
  abstract  = "From the perspective of machine learning and data mining applications, expressing data in RDF rather than a domain-specific for- mat can add complexity and obfuscate the internal structure. We in- vestigate and illustrate this issue with an example where bio-molecular graph datasets are expressed in RDF. We use this example to inspire pre- processing techniques which reverse some of the complications of adding semantic annotations, exposing those patterns in the data that are most relevant to machine learning. We test these methods in a number of clas- sification experiments and show that they can improve performance both for our example datasets and real-world RDF datasets.",
  author    = "P. Bloem and A. Wibisono and {de Vries}, G.K.D",
  year      = "2014",
  booktitle = "KNOW@ LOD",
}


@inbook{e309b7355486497ebd0952e62aff3230,
  title     = "Spatial Reasoning based Research of Road Network Intersection Patterns in Cities",
  author    = "D. Xu and Z. Huang and X. Zhang and D. Wang and Jiangli Zhang",
  year      = "2014",
  series    = "1",
  pages     = "31--35",
  booktitle = "Journal of Road Traffice and Safety",
}


@inbook{1a6e58e9007344c59c174b6547484d8f,
  title     = "Stay Point Identification of Mobile Phone Trajectory",
  author    = "R. Du and J. Huang and N. Zhong and Z. Huang",
  year      = "2014",
  series    = "8",
  pages     = "200--206",
  booktitle = "Journal of Frontiers of Computer Science and Technology,",
}


@article{29462648d9424241ae9f3c6d4eb31e12,
  title     = "Streaming the Web: Reasoning over dynamic data",
  abstract  = "In the last few years a new research area, called stream reasoning, emerged to bridge the gap between reasoning and stream processing. While current reasoning approaches are designed to work on mainly static data, the Web is, on the other hand, extremely dynamic: information is frequently changed and updated, and new data is continuously generated from a huge number of sources, often at high rate. In other words, fresh information is constantly made available in the form of streams of new data and updates. Despite some promising investigations in the area, stream reasoning is still in its infancy, both from the perspective of models and theories development, and from the perspective of systems and tools design and implementation. The aim of this paper is threefold: (i) we identify the requirements coming from different application scenarios, and we isolate the problems they pose; (ii) we survey existing approaches and proposals in the area of stream reasoning, highlighting their strengths and limitations; (iii) we draw a research agenda to guide the future research and development of stream reasoning. In doing so, we also analyze related research fields to extract algorithms, models, techniques, and solutions that could be useful in the area of stream reasoning. © 2014 Elsevier B.V. All rights reserved.",
  keywords  = "Complex Event Processing, Semantic Web, Stream processing, Stream reasoning, Survey",
  author    = "Alessandro Margara and Jacopo Urbani and {Van Harmelen}, Frank and Henri Bal",
  year      = "2014",
  doi       = "10.1016/j.websem.2014.02.001",
  volume    = "25",
  pages     = "24--44",
  journal   = "Journal of Web Semantics",
  issn      = "1570-8268",
  publisher = "Elsevier",
}


@misc{5f6168c39c024fbbb6af3a83c8c55cb2,
  title  = "Structural Properties as Proxy for Semantic Relevance in RDF Graph Sampling",
  author = "L.J. Rietveld and R.J. Hoekstra and K.S. Schlobach and C.D.M. Gueret",
  year   = "2014",
}


@misc{8bb5f0a8c46b420aa45deb4b4228781d,
  title   = "Supporting trial recruitment and design by automatically interpreting eligibility criteria",
  author  = "K. Milian",
  note    = "Naam instelling promotie: VU Vrije Universiteit Naam instelling onderzoek: VU Vrije Universiteit",
  year    = "2014",
  school  = "Vrije Universiteit Amsterdam",
}


@article{922fa661059e43edbb88dc10e0d02881,
  title     = "The Linked Data Benchmark Council: a Graph and RDF industry benchmarking effort",
  abstract  = "The Linked Data Benchmark Council (LDBC) is an EU project that aims to develop industry-strength benchmarks for graph and RDF data management systems. It includes the creation of a non-profit LDBC organization, where industry players and academia come together for managing the development of benchmarks as well as auditing and publishing official results. We present an overview of the project including its goals and organization, and describe its process and design methodology for benchmark development. We introduce so-called {"}choke-point{"} based benchmark development through which experts identify key technical challenges, and introduce them in the benchmark workload. Finally, we present the status of two benchmarks currently in development, one targeting graph data management systems using a social network data case, and the other targeting RDF systems using a data publishing case.",
  author    = "{Angles Rojas}, R. and P.A. Boncz and {Larriba Pey}, J. and I. Fundulaki and T. Neumann and O. Erling and P. Neubauer and V. Kotsev and I. Toma",
  year      = "2014",
  doi       = "10.1145/2627692.2627697",
  volume    = "43",
  pages     = "27--31",
  journal   = "ACM SIGMOD Record",
  issn      = "0163-5808",
  publisher = "Association for Computing Machinery (ACM)",
}


@inbook{fd6fc04a6c44499fa0a62ec0fe44c350,
  title     = "Towards a Conceptual Model for Enhancing Reasoning about Clinical Guidelines: A case-study on Comorbidity",
  abstract  = "Computer-Interpretable Guidelines (CIGs) are representations of Clinical Guidelines (CGs) in computer interpretable languages. CIGs have been pointed as an alternative to deal with the various limitations of paper based CGs to support healthcare activities. Although the improvements offered by existing CIG languages, the complexity of the medical domain requires advanced features in order to reuse, share, update, combine or personalize their contents. We propose a conceptual model for representing the content of CGs as a result from an iterative approach that take into account the content of real CGs, CIGs languages and foundational ontologies in order to enhance the reasoning capabilities required to address CIG use-cases. In particular, we apply our approach to the comorbidity use-case and illustrate the model with a realistic case study (Duodenal Ulcer and Transient Ischemic Attack) and compare the results against an existing approach.",
  author    = "{Carretta Zamborlini}, Veruska and {Da Silveira}, Marcos and Cedric Pruski and {ten Teije}, Annette and {van Harmelen}, Frank",
  note      = "Proceedings title: Proceedings of 6th International Workshop Knowledge Representation for Health-Care (KR4HC). Publisher: Springer Editors: S. Miksch, D. Riano, A. ten Teije",
  year      = "2014",
  doi       = "10.1007/978-3-319-13281-5_3",
  volume    = "8903",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "29--44",
  booktitle = "Knowledge Representation for Health Care - 6th International Workshop, KR4HC 2014, Revised Selected Papers",
}


@inbook{6d394da2e5c941c1990410c63950fa78,
  title     = "Using Semantic Web Technologies to Reproduce a Pharmacovigelance Case Study",
  author    = "M. Hildebrand and R.J. Hoekstra and {van Ossenbruggen}, J.R.",
  year      = "2014",
  editor    = "T Kauppinen and J Zhao and P.T. Groth and K. Kessler and L.C. Pouchard and C. Goble and Y. Gil and {van Erp}, M. and {van Ossenbruggen}, J.",
  booktitle = "3rd International Workshop on Linked Science 2013—Supporting Reproducibility, Scientific Investigations and Experiments (LISC2013)",
}


@inbook{9d75affb77e9494db1e5d55b95015c35,
  title     = "Validity Checking of Road Signs based on LarKC",
  author    = "D. Wang and Z. Huang and Q. Liu and X. Zhang and D. Xu",
  year      = "2014",
  pages     = "18--24",
  booktitle = "Journal of Road Traffic and Safety (ISSN 1008-2522)",
}


@book{29feaf635c0b476c8d1f0ff503fbeaeb,
  title     = "Web Information Systems Engineering - WISE 2013 Workshops",
  author    = "Z. Huang and Chenfei Liu and Jing He and Guangyan Huang",
  year      = "2014",
  series    = "1",
  publisher = "Springer",
  number    = "LNCS8182",
}


@inbook{1a922735da824990b12698b8ac46649b,
  title     = "WebQR: Building a Knowledge Representation Application on the Semantic Web",
  author    = "W.G.J. Beek and S. Latour and K.S. Schlobach",
  year      = "2014",
  pages     = "102--107",
  booktitle = "Proceedings of the ISWC Developers Workshop 2014",
}


@inbook{70e1f9ce1c6640c482c65181c2cad722,
  title     = "What is Linked Historical Data",
  author    = "A. Merono and R.J. Hoekstra",
  year      = "2014",
  editor    = "S. Schlobach and K. Janowicz",
  booktitle = "Proceedings of the 19th International Conference on Knowledge Engineering and Knowledge Management (EKAW 2014)",
  publisher = "Springer",
}


@inbook{e751cac771d04b3188bafa82120e2953,
  title     = "YASGUI: Feeling the Pulse of Linked Data",
  author    = "L.J. Rietveld and R.J. Hoekstra",
  year      = "2014",
  booktitle = "Proceedings of the 19th International Conference on Knowledge Engineering and Knowledge Management (EKAW 2014)",
}


@article{adb2dd43a92a42b28864166631fb7a95,
  title     = "Automated generation of patient-tailored electronic care pathways by translating computer-interpretable guidelines into hierarchical task networks",
  abstract  = "OBJECTIVE: This paper describes a methodology which enables computer-aided support for the planning, visualization and execution of personalized patient treatments in a specific healthcare process, taking into account complex temporal constraints and the allocation of institutional resources. To this end, a translation from a time-annotated computer-interpretable guideline (CIG) model of a clinical protocol into a temporal hierarchical task network (HTN) planning domain is presented.MATERIALS AND METHODS: The proposed method uses a knowledge-driven reasoning process to translate knowledge previously described in a CIG into a corresponding HTN Planning and Scheduling domain, taking advantage of HTNs known ability to (i) dynamically cope with temporal and resource constraints, and (ii) automatically generate customized plans. The proposed method, focusing on the representation of temporal knowledge and based on the identification of workflow and temporal patterns in a CIG, makes it possible to automatically generate time-annotated and resource-based care pathways tailored to the needs of any possible patient profile.RESULTS: The proposed translation is illustrated through a case study based on a 70 pages long clinical protocol to manage Hodgkin's disease, developed by the Spanish Society of Pediatric Oncology. We show that an HTN planning domain can be generated from the corresponding specification of the protocol in the Asbru language, providing a running example of this translation. Furthermore, the correctness of the translation is checked and also the management of ten different types of temporal patterns represented in the protocol. By interpreting the automatically generated domain with a state-of-art HTN planner, a time-annotated care pathway is automatically obtained, customized for the patient's and institutional needs. The generated care pathway can then be used by clinicians to plan and manage the patients long-term care.CONCLUSION: The described methodology makes it possible to automatically generate patient-tailored care pathways, leveraging an incremental knowledge-driven engineering process that starts from the expert knowledge of medical professionals. The presented approach makes the most of the strengths inherent in both CIG languages and HTN planning and scheduling techniques: for the former, knowledge acquisition and representation of the original clinical protocol, and for the latter, knowledge reasoning capabilities and an ability to deal with complex temporal and resource constraints. Moreover, the proposed approach provides immediate access to technologies such as business process management (BPM) tools, which are increasingly being used to support healthcare processes.",
  keywords  = "Antineoplastic Protocols, Artificial Intelligence, Critical Pathways, Decision Making, Computer-Assisted, Hodgkin Disease, Humans, Long-Term Care, Patient Care Planning, Pediatrics, Practice Guidelines as Topic, Workflow, Journal Article, Research Support, Non-U.S. Gov't",
  author    = "A. González-Ferrer and {ten Teije}, A.C.M. and J. Fdez-Olivares and K. Milian",
  note      = "Copyright © 2012 Elsevier B.V. All rights reserved.",
  year      = "2013",
  month     = "2",
  doi       = "10.1016/j.artmed.2012.08.008",
  volume    = "57",
  pages     = "91--109",
  journal   = "Artificial Intelligence in Medicine",
  issn      = "0933-3657",
  publisher = "Elsevier",
  number    = "2",
}


@inbook{07e286a48adc4ca2874c9d2568251155,
  title     = "A Monitoring System for the Safety of Building Structure Based on Semantic Technology",
  author    = "H. Wang and Z. Huang and N. Zhong",
  year      = "2013",
  booktitle = "the IEEE-CPS Proceedings of the 2013 International Symposium on Intelligent Building and Buliding Automation (ISBBA2013)",
}


@inbook{1bf777bcc6b84cceb91534c5e3bb7c21,
  title     = "Application of Semantic Technology in Rational Use of Antibacterial Agents",
  author    = "S. Yu and L. Xu and Z. Huang",
  year      = "2013",
  booktitle = "Proceedings of the 8th International Conference on Cooperation and Promotion of Information Resources in Science and Technology (COINFO 2013)",
  publisher = "China Science Press",
}


@inbook{a8c8c60fa6e84ca1bcbc1a8685d21fab,
  title     = "A Practical Query Language for Graph DBs",
  author    = "{Angles Rojas}, R. and P. Barcelo and G. Rios",
  year      = "2013",
  booktitle = "Alberto Mendelzon International Workshop on Foundations of Data Management",
}


@misc{fb48e56abc7544fcb0e24a476fbe8012,
  title  = "A Semantically-Enabled System for Clinical Trials",
  author = "Z. Huang and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H.",
  note   = "Proceedings title: 25th Benelux Conference on Artificial Intelligence Editors: K Hindriks, M de Weerdt, B van Riemsdijk, M Warnier",
  year   = "2013",
}


@inbook{3e982bf976e64511ada5e07ef08fa0c1,
  title     = "A Summary of the Workshop and Tutorial Program at ESWC 2013",
  author    = "J. Voelker and K.S. Schlobach",
  year      = "2013",
  booktitle = "The Semantic Web: ESWC 2013 Satellite Events",
}


@inbook{7ef24e6c9ca7439491e093b85893a3d9,
  title     = "Automatic Schema Design For Co-Clustered Tables",
  author    = "S. Baumann and P.A. Boncz and K.-U. Sattler",
  year      = "2013",
  booktitle = "Proceedings of International Workshop on Self-Managing Database Systems 2013 (SMDB )",
}


@article{87b9253386bb402d9f48f4a46df2e4dd,
  title     = "Barriers to the reuse of routinely recorded clinical data: a field report",
  abstract  = "Today, clinical data is routinely recorded in vast amounts, but its reuse can be challenging. A secondary use that should ideally be based on previously collected clinical data is the computation of clinical quality indicators. In the present study, we attempted to retrieve all data from our hospital that is required to compute a set of quality indicators in the domain of colorectal cancer surgery. We categorised the barriers that we encountered in the scope of this project according to an existing framework, and provide recommendations on how to prevent or surmount these barriers. Assuming that our case is not unique, these recommendations might be applicable for the design, evaluation and optimisation of Electronic Health Records.",
  keywords  = "Attitude of Health Personnel, Colorectal Neoplasms, Computer Literacy, Data Mining, Electronic Health Records, Humans, Medical Record Linkage, Netherlands, Prevalence, Journal Article",
  author    = "Kathrin Dentler and {ten Teije}, Annette and {De Keizer}, {Nicolette F.} and Ronald Cornet",
  year      = "2013",
  volume    = "192",
  pages     = "313--7",
  journal   = "Studies in Health Technology and Informatics",
  issn      = "0926-9630",
  publisher = "IOS Press",
}


@inbook{504cdc95cc514923848b587de36d3ae3,
  title     = "Barriers to the Reuse of Routinely Recorded Clinical Data: A Field Report",
  abstract  = "Today, clinical data is routinely recorded in vast amounts, but its reuse can be challenging. A secondary use that should ideally be based on previously collected clinical data is the computation of clinical quality indicators. In the present study, we attempted to retrieve all data from our hospital that is required to compute a set of quality indicators in the domain of colorectal cancer surgery. We categorised the barriers that we encountered in the scope of this project according to an existing framework, and provide recommendations on how to prevent or surmount these barriers. Assuming that our case is not unique, these recommendations might be applicable for the design, evaluation and optimisation of Electronic Health Records. © 2013 IMIA and IOS Press.",
  author    = "K. Dentler and {ten Teije}, A. and {De Keizer}, N. and R. Cornet",
  year      = "2013",
  doi       = "10.3233/978-1-61499-289-9-313",
  isbn      = "9781614992899",
  pages     = "313--317",
  editor    = "Lehmann, {C. U.}",
  booktitle = "MEDINFO 2013",
  publisher = "IOS",
}


@article{38c97fb87cdd44e5a169505f30375aa7,
  title     = "Barriers to the Reuse of Routinely Recorded Clinical Data: A Field Report",
  abstract  = "Today, clinical data is routinely recorded in vast amounts, but its reuse can be challenging. A secondary use that should ideally be based on previously collected clinical data is the computation of clinical quality indicators. In the present study, we attempted to retrieve all data from our hospital that is required to compute a set of quality indicators in the domain of colorectal cancer surgery. We categorised the barriers that we encountered in the scope of this project according to an existing framework, and provide recommendations on how to prevent or surmount these barriers. Assuming that our case is not unique, these recommendations might be applicable for the design, evaluation and optimisation of Electronic Health Records. © 2013 IMIA and IOS Press.",
  author    = "K. Dentler and {ten Teije}, A.C.M. and {de Keizer}, N.F. and R. Cornet",
  note      = "Proceedings title: Medinfo 2013 Publisher: Imia and IOS press",
  year      = "2013",
  doi       = "10.3233/978-1-61499-289-9-313",
  volume    = "192",
  journal   = "Studies in Health Technology and Informatics",
  issn      = "0926-9630",
  publisher = "IOS Press",
}


@inbook{b4b72d4b0e82496cb7d09ccf8044ae94,
  title     = "Benchmarking database systems for social network applications",
  author    = "{Angles Rojas}, R. and A. Prat-Pérez and D. Dominguez-Sal and Josep-Lluis Larriba-Pey",
  year      = "2013",
  booktitle = "Graph Data-management Experiences & Systems (GRADES)",
}


@article{83a2e1502b3840fc8556226509352170,
  title     = "Conceptual and Computational Analysis of the Role of Emotions and Social Influence in Learning",
  author    = "J. Treur and {van Wissen}, A.",
  note      = "Proceedings title: Proceedings of the Third World Conference on Learning, Teaching and Educational Leadership Publisher: Procedia Social and Behavioral Sciences, Elsevier",
  year      = "2013",
  doi       = "10.1016/j.sbspro.2013.09.220",
  volume    = "93",
  pages     = "449--467",
  journal   = "Procedia Social and Behavioral Sciences",
  issn      = "1877-0428",
  publisher = "Elsevier BV",
}


@inbook{11d7ed31bd004cd7823f7a011e56354b,
  title     = "DataHives: Triple Store Enhancement Using A Bee Calculus",
  abstract  = "The Linked Open Data (LOD) cloud is too big for efficient computation and too heterogeneous for standard materialization techniques to cope with. The purpose of the DataHives system is to solve both of these problems by utilizing swarm intelligence to enhance a curated dataset. The system spawns software agentsthat traverse the LOD cloud looking for extensions to the curated dataset that are relevant and trusted.",
  author    = "Pepijn Kroes and Wouter Beek and Stefan Schlobach",
  year      = "2013",
  pages     = "380--381",
  editor    = "{Koen Hindriks}, K. and {de Weerdt}, M. and {van Riemsdijk}, B. and M. Warnier",
  booktitle = "Proceedings of the 25th Belgium-Netherlands Artificial Intelligence Conference",
}


@inbook{3a9fa7d189134bf5842f7bd9a2ada835,
  title     = "Design and Implementation of Visualization Tools for Advanced Patient Data Generator",
  author    = "M. Zhang and Z. Huang and J. Gu",
  year      = "2013",
  booktitle = "Proceedings of 2013 International Workshop on Semantic Technology for eHealth (STeH2013)",
  publisher = "Springer",
}


@inbook{f3e837848031463dbc1cbd01d38ec25f,
  title     = "Detecting and Reporting Extensional Concept Drift in Statistical Linked Data",
  author    = "A. Merono and C.D.M. Gueret and R.J. Hoekstra and K.S. Schlobach",
  year      = "2013",
  booktitle = "Proceedings of the 1st International Workshop on Semantic Statistics (SemStats 2013), ISWC 2013",
  publisher = "CEUR",
}


@inbook{a038c99802254d429e5a77f38854da24,
  title     = "Developing a Brain Informatics Provenance Model",
  author    = "H. Zhong and J. Chen and J. Han and T. Kotake and N. Zhong and Z. Huang",
  year      = "2013",
  booktitle = "Proceedings of 2013 International Conference on Brain and Health Informatics, Special Session on Special Session on Human Centered Computing",
  publisher = "Springer",
}


@inbook{520b0a6a5ea64421bcc47926aa0b74ec,
  title     = "DynaLearn - An Intelligent Learning Environment for Learning Conceptual Knowledge",
  abstract  = "Articulating thought in computer-based media is a powerful means for\nhumans to develop their understanding of phenomena. We have created\nDynaLearn, an intelligent learning environment that allows learners to\nacquire conceptual knowledge by constructing and simulating qualitative\nmodels of how systems behave. DynaLearn uses diagrammatic\nrepresentations for learners to express their ideas. The environment is\nequipped with semantic technology components that are capable of\ngenerating knowledge-based feedback and virtual characters that enhance\nthe interaction with learners. Teachers have created course material,\nand successful evaluation studies have been performed. This article\npresents an overview of the DynaLearn system.",
  author    = "Bert Bredeweg and Jochem Liem and Wouter Beek and Floris Linnebank and Jorge Gracia and Esther Lozano and Michael Wissner and Rene Buehling and Paulo Salles and Richard Noble and Andreas Zitek and Petya Borisova and David Mioduser",
  year      = "2013",
  isbn      = "0738-4602",
  series    = "AI MAGAZINE",
  pages     = "46--65",
  booktitle = "AI MAGAZINE",
}


@article{900acaf85bd542f4bb17012e5751a16f,
  title     = "DynaLearn-An Intelligent Learning Environment for Learning Conceptual Knowledge",
  abstract  = "Articulating thought in computerbased media is a powerful means for humans to develop their understanding of phenomena. We have created DynaLearn, an intelligent learning environment that allows learners to acquire conceptual knowledge by constructing and simulating qualitative models of how systems behave. DynaLearn uses diagrammatic representations for learners to express their ideas. The environment is equipped with semantic technology components that are capable of generating knowledge- based feedback and virtual characters that enhance the interaction with learners. Teachers have created course material, and successful evaluation studies have been performed. This article presents an overview of the DynaLearn system. Copyright © 2013, Association for the Advancement of Artificial Intelligence. All rights reserved.",
  author    = "Bert Bredeweg and Jochem Liem and Wouter Beek and Floris Linnebank and Jorge Gracia and Esther Lozano and Michael Wißner and René Bühling and Paulo Salles and Richard Noble and Andreas Zitek and Petya Borisova and David Mioduser",
  year      = "2013",
  volume    = "34",
  pages     = "46--65",
  journal   = "The AI Magazine",
  issn      = "0738-4602",
  publisher = "AI Access Foundation",
  number    = "4",
}


@inbook{3160586e9d6846b9a8daa224e7d927e4,
  title     = "DynamiTE: Parallel Materialization of Dynamic RDF Data",
  author    = "J. Urbani and A. Margara and C.J.H. Jacobs and {van Harmelen}, F.A.H. and H.E. Bal",
  year      = "2013",
  pages     = "657--672",
  booktitle = "12th Int. Semantic Web Conference (ISWC 2013)",
  publisher = "Springer",
}


@inbook{3ad79a7f3a1440a4bc7e39cd22b55b99,
  title     = "Experiences With Virtuoso Cluster RDF Column Store",
  author    = "P.A. Boncz and O. Erling and {Minh Duc}, P.",
  year      = "2013",
  booktitle = "Linked Data Management: Principles and Techniques",
  publisher = "CRC Press",
}


@article{ce8b4c51717a46308eab9aa847a873c6,
  title     = "Formalization and computation of quality measures based on electronic medical records",
  abstract  = "Objective: Ambiguous definitions of quality measures in natural language impede their automated computability and also the reproducibility, validity, timeliness, traceability, comparability, and interpretability of computed results. Therefore, quality measures should be formalized before their release. We have previously developed and successfully applied a method for clinical indicator formalization (CLIF). The objective of our present study is to test whether CLIF is generalizable- that is, applicable to a large set of heterogeneous measures of different types and from various domains. Materials and methods: We formalized the entire set of 159 Dutch quality measures for general practice, which contains structure, process, and outcome measures and covers seven domains. We relied on a web-based tool to facilitate the application of our method. Subsequently, we computed the measures on the basis of a large database of real patient data. Results: Our CLIF method enabled us to fully formalize 100% of the measures. Owing to missing functionality, the accompanying tool could support full formalization of only 86% of the quality measures into Structured Query Language (SQL) queries. The remaining 14% of the measures required manual application of our CLIF method by directly translating the respective criteria into SQL. The results obtained by computing the measures show a strong correlation with results computed independently by two other parties. Conclusions: The CLIF method covers all quality measures after having been extended by an additional step. Our web tool requires further refinement for CLIF to be applied completely automatically. We therefore conclude that CLIF is sufficiently generalizable to be able to formalize the entire set of Dutch quality measures for general practice.",
  author    = "K. Dentler and M. Numans and {ten Teije}, A. and R. Cornet and {De Keizer}, N.",
  year      = "2013",
  doi       = "10.1136/amiajnl-2013-001921",
  journal   = "Journal of the American Medical Informatics Association",
  issn      = "1067-5027",
  publisher = "Oxford University Press",
}


@inbook{8764a7f95d8e46ca9410331e77bca45e,
  title     = "Generation of Semantic Data from Guidelines for Rational Use of Antibiotics",
  author    = "Q. Hu and Z. Huang and J. Gu",
  year      = "2013",
  booktitle = "Proceedings of 2013 International Workshop on Semantic Technology for eHealth (STeH2013)",
  publisher = "Springer",
}


@article{7131b396f2fe4f1cb4832082e4619c82,
  title     = "Hybrid reasoning on OWL RL",
  author    = "J. Urbani and R Piro and {van Harmelen}, F.A.H. and H.E. Bal",
  year      = "2013",
  journal   = "Semantic Web",
  issn      = "1570-0844",
  publisher = "IOS Press",
}


@inbook{bb18383308bf44539aa052e4b17a02b3,
  title     = "Identifying most relevant concepts to describe clinical trial eligibility criteria",
  author    = "K. Milian and A. Bucur and {van Harmelen}, F.A.H. and {ten Teije}, A.C.M.",
  year      = "2013",
  booktitle = "International Conference on Health Informatics (HEALTHINF 2013)",
}


@inbook{e9694ddfc6ff4d4ea071e344096fcf37,
  title     = "Identifying research talent using web-centric databases",
  author    = "A. Dumitrache and P.T. Groth and {van den Besselaar}, P.A.A.",
  note      = "Gebeurtenis: ACM Web Science Conference, Paris, May 2013",
  year      = "2013",
  isbn      = "9781450318891",
  pages     = "57--60",
  editor    = "H. Davis and H. Halpin and A. Pentland and M. Bernstein and L. Adamic and H. Alani and A. Monnin and R. Rogers",
  booktitle = "Web Science",
  publisher = "ACM",
}


@article{e18f608ad5c74b508b1e31aa1e6b77ae,
  title     = "Including Co-referent URIs in a SPARQL Query",
  author    = "C Brenninkmeijer and C.A. Goble and A.J.G Gray and P.T. Groth and A. Loizou and S Pettifier",
  note      = "Proceedings title: Fourth International Workshop on Consuming Linked Data, COLD 2013 Publisher: CEUR-WS.org",
  year      = "2013",
  volume    = "1034",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@inbook{a5224b05ee004298bbd93786f2316a8d,
  title     = "Incorporating Commercial and Private Data into an Open Linked Data Platform for Drug Discovery",
  author    = "C.A. Goble and A.J.G Gray and L Harland and K. Karapetyan and A. Loizou and I. Mikhailov and Y. Rankka and S. Senger and V. Tkachenko and A.J. Williams and E.L Willighagen",
  year      = "2013",
  booktitle = "12th Int. Semantic Web Conference (ISWC 2013)",
  publisher = "Springer",
}


@article{36955f7c0d4542a6af6f6a62c57eef05,
  title     = "Integration of Pattern-Based Debugging Approach into RaDON",
  author    = "Q. Ji and Z. Gao and Z. Huang",
  note      = "Proceedings title: Proceedings of 7th China Semantic Web Symposium & 2nd Web Science Conference (CSWS 2013) Publisher: Springer",
  year      = "2013",
  volume    = "406",
  pages     = "243--246",
  journal   = "Communications in Computer and Information Science",
  issn      = "1865-0929",
  publisher = "Springer Verlag",
}


@inbook{087d273978204bc993073b8626165e33,
  title     = "Intelligent Monitoring on Use of Antibacterial Agents based on Semantic Technology",
  author    = "X. Hua and C. Chen and Z. Huang",
  year      = "2013",
  pages     = "12--15",
  booktitle = "Journal of China Digital Medicine, Vol.8, No.4",
}


@inbook{ad639dcaf4fd4c6f955c66b55b188bfc,
  title     = "Interface Design of Semantic System for Road Sign Managemen",
  author    = "N. Li and Z. Huang and D. Xu",
  year      = "2013",
  booktitle = "Proceedings of 2013 International Workshop on Semantic Technology for Smart City",
  publisher = "Springer",
}


@article{371b66c6e07845768e482114ed32c7ca,
  title     = "Knowledge-based Patient Data Generation",
  author    = "Z. Huang and {van Harmelen}, F.A.H. and {ten Teije}, A.C.M. and K. Dentler",
  note      = "Proceedings title: Process Support and Knowledge Representation in Health Care Publisher: Springer Editors: D Riano, R Lenz, S Miksch, M Peleg, M Reichert, A.C.M. ten Teije",
  year      = "2013",
  volume    = "8268",
  pages     = "11--25",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@inbook{4db741f0dbc34ea68b003d1284249548,
  title     = "Linkitup: Link Discovery for Research Data",
  author    = "R.J. Hoekstra and P.T. Groth",
  year      = "2013",
  editor    = "G.A.P.C. Burns and Y. Gil and Y. Liu and N. Villanueva-Rosales",
  booktitle = "Proceedings of the AAAI Fall Symposium on Discovery Informatics",
  publisher = "AAAI",
}


@inbook{26d8ed12d1cd46fb913384699b55e5da,
  title     = "Longitudinal Queries over Linked Census Data",
  author    = "A. Merono and R.J. Hoekstra and A. Scharnhorst and C.D.M. Gueret and A. Ashkpour",
  year      = "2013",
  isbn      = "9783642382871",
  editor    = "P. Cimiano and O. Corcho and V. Presutti and L. Hollink and S. Rudolph",
  booktitle = "Proceedings of the 10th Extended Semantic Web Conference, ESWC 2013",
  publisher = "Springer-Verlag",
}


@article{0e159bf14aae49deb75a7a36761b1835,
  title     = "Making Web-Scale Semantic Reasoning More Service-Oriented: The Large Knowledge Collider,",
  author    = "A Cheptsov and Z. Huang",
  note      = "Proceedings title: Proceedings of the Web Information Systems Engineering - Combined WISE 2011 and WISE 2012 Workshops, 2013 Publisher: Springer",
  year      = "2013",
  volume    = "7652",
  pages     = "13--26",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@inbook{f121e2947bc34d62a40a3dc42d3cea78,
  title     = "Micro Adaptivity In Vectorwise",
  author    = "B. Raducanu and M. Zukowski and P.A. Boncz",
  year      = "2013",
  booktitle = "Proceedings of the ACM SIGMOD International Conference on Management of Data (SIGMOD, 2013",
  publisher = "ACM Press",
}


@article{88ca81af963648ed876694828c0a28df,
  title     = "Modelling Collective Decision Making in Groups and Crowds: Integrating Social Contagion and Interacting Emotions, Beliefs and Intentions",
  abstract  = "Collective decision making involves on the one hand individual mental states such as beliefs, emotions and intentions, and on the other hand interaction with others with possibly different mental states. Achieving a satisfactory common group decision on which all agree requires that such mental states are adapted to each other by social interaction. Recent developments in social neuroscience have revealed neural mechanisms by which such mutual adaptation can be realised. These mechanisms not only enable intentions to converge to an emerging common decision, but at the same time enable to achieve shared underlying individual beliefs and emotions. This paper presents a computational model for such processes. As an application of the model, an agent-based analysis was made of patterns in crowd behaviour, in particular to simulate a real-life incident that took place on May 4, 2010 in Amsterdam. From available video material and witness reports, useful empirical data were extracted. Similar patterns were achieved in simulations, whereby some of the parameters of the model were tuned to the case addressed, and most parameters were assigned default values. The results show the inclusion of contagion of belief, emotion, and intention states of agents results in better reproduction of the incident than non-inclusion. © 2012 The Author(s).",
  author    = "T. Bosse and M. Hoogendoorn and M.C.A. Klein and J. Treur and {van der Wal}, C.N. and {van Wissen}, A.",
  year      = "2013",
  doi       = "10.1007/s10458-012-9201-1",
  volume    = "27",
  pages     = "52--84",
  journal   = "Autonomous Agents and Multi-Agent Systems",
  issn      = "1387-2532",
  publisher = "Springer Netherlands",
  number    = "1",
}


@inbook{d70fe1f961a3455c9a922c53e7324714,
  title     = "Non-Temporal Orderings as Proxies for Extensional Concept Drift",
  author    = "A. Merono and K.S. Schlobach",
  year      = "2013",
  booktitle = "Proceedings of the 1st International Workshop on Semantic Statistics (SemStats 2013), challenge paper",
  publisher = "CEUR",
}


@article{66b924563bff4a9491ca7e35cec554ac,
  title     = "NoSQL Databases for RDF: An Empirical Evaluation",
  abstract  = "Processing large volumes of RDF data requires sophisticated tools. In recent years, much effort was spent on optimizing native RDF stores and on repurposing relational query engines for large-scale RDF processing. Concurrently, a number of new data management systems-regrouped under the NoSQL (for {"}not only SQL{"}) umbrella-rapidly rose to prominence and represent today a popular alternative to classical databases. Though NoSQL systems are increasingly used to manage RDF data, it is still difficult to grasp their key advantages and drawbacks in this context. This work is, to the best of our knowledge, the first systematic attempt at characterizing and comparing NoSQL stores for RDF processing. In the following, we describe four different NoSQL stores and compare their key characteristics when running standard RDF benchmarks on a popular cloud infrastructure using both single-machine and distributed deployments. © 2013 Springer-Verlag.",
  author    = "P. Cudré-Mauroux and I. Enchev and S. Fundatureanu and P.T. Groth and A. Haque and A. Harth and Keppmann, {F. L.} and D. Miranker and J. Sequeda and M. Wylot",
  note      = "Proceedings title: The Semantic Web – ISWC 2013 Publisher: Springer",
  year      = "2013",
  doi       = "10.1007/978-3-642-41338-4_20",
  volume    = "8219",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@inbook{15adffb873db429d833b0d85d14b7f17,
  title     = "NoSQL Databases for RDF: An Empirical Evaluation",
  author    = "S. Fundatureanu and P. Groth and M. Wylot and P. Cudré-Mauroux and I. Enchev and A. Haque and A. Harth and Keppmann, {F. L.} and D. Miranker and J. Sequeda",
  year      = "2013",
  booktitle = "12th Int. Semantic Web Conference (ISWC 2013)",
}


@inbook{fc0f30d4203f446994401e27c80f8617,
  title     = "On the Generation System of Mobile Data Based on Real User Behavior",
  author    = "R. Du and J. Huang and Z. Huang",
  year      = "2013",
  booktitle = "Proceedings of 2013 International Workshop on Big Web Data",
  publisher = "Springer",
}


@article{f6f84cd9bbf1493c8562e83be5a2f1b6,
  title     = "Order matters! Harnessing a world of orderings for reasoning over massive data.",
  abstract  = "More and more applications require real-time processing of massive, dynamically generated, ordered data; order is an essential factor as it reflects recency or relevance. Semantic technologies risk being unable to meet the needs of such applications, as they are not equipped with the appropriate instruments for answering queries over massive, highly dynamic, ordered data sets. In this vision paper, we argue that some data management techniques should be exported to the context of semantic technologies, by integrating ordering with reasoning, and by using methods which are inspired by stream and rank-aware data management.We systematically explore the problem space, and point both to problems which have been successfully approached and to problems which still need fundamental research, in an attempt to stimulate and guide a paradigm shift in semantic technologies. © IOS Press and the authors. All rights reserved.",
  author    = "{Della Valle}, E and K.S. Schlobach and M. Krötzsch and A. Bozzon and S. Ceri and I. Horrocks",
  year      = "2013",
  doi       = "10.3233/SW-2012-0085",
  volume    = "4",
  pages     = "219--231",
  journal   = "Semantic Web",
  issn      = "1570-0844",
  publisher = "IOS Press",
  number    = "2",
}


@inbook{97994a87bf844a8faf322b96ca9f4808,
  title     = "Pharmaceutical semantic database query mechanism Based on Key words",
  author    = "Y. Sun and J. Gu and Z. Huang",
  year      = "2013",
  pages     = "199--206",
  booktitle = "Proceedings of 7th China Semantic Web Symposium & 2nd Web Science Conference (CSWS 2013)",
  publisher = "Springer",
}


@inbook{8024c5ea075949189a0073d10a974dcb,
  title     = "Port city road sign management system based on semantic technology",
  author    = "Q. Liu and Z. Huang",
  year      = "2013",
  booktitle = "Journal of Industrial Instrumentation & Automation",
  publisher = "3",
}


@inbook{1f7367512d3b4a308b8aa657c69b5aaf,
  title     = "Pragmatic Semantics for the Web of Data",
  author    = "K.S. Schlobach and {van Beek}, W.",
  year      = "2013",
  booktitle = "2013 AAAI Fall Symposium Series",
}


@article{7595431b768a42b6bf547f977b25ca6b,
  title     = "Preface to the Advance Reasoning Technology for eScience (ART-2012) Workshop",
  author    = "Z. Huang and A Cheptsov and J. Pan",
  note      = "Proceedings title: Proceedings of the Web Information Systems Engineering - Combined WISE 2011 and WISE 2012 Workshops, 2013 Publisher: Springer",
  year      = "2013",
  volume    = "7652",
  pages     = "1--1",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@inbook{077adf66f46f4e71b5e7313557e4d413,
  title     = "Preliminary Construction of the Semantic Web Course",
  author    = "W. Hu and Y. Qu and Z. Huang",
  year      = "2013",
  pages     = "73--76",
  booktitle = "Journal of Computer Education, No.192(12)",
}


@book{fc39aa1538be40c6bf98dec9c9d5d5a3,
  title     = "Proceedings of the 2013 international workshop on web-scale knowledge representation, retrieval, and reasoning",
  author    = "Y. Zeng and S. Kotoulas and Z. Huang",
  year      = "2013",
  series    = "ACM International Conference Proceeding Series 2013",
  publisher = "ACM Press",
}


@book{7ca1ce14eb1f4ae09b7f730021a493fe,
  title     = "Proceedings of the 2013 international workshop on web-scale knowledge representation, retrieval, and reasoning",
  author    = "Y. Zeng and S. Kotoulas and Z. Huang",
  year      = "2013",
  series    = "ACM International Conference on Information and Knowledge Management",
  publisher = "ACM",
  number    = "22",
}


@book{cc4b89bb09e84e54aabf0ead6e0df179,
  title     = "Proceedings of the Web Information Systems Engineering - Combined WISE 2011 and WISE 2012 Workshops",
  author    = "A. Haller and G. Huang and Z. Huang and H. Paik and Q. Sheng",
  year      = "2013",
  isbn      = "9783642383328",
  series    = "LNCS",
  publisher = "Springer",
  number    = "7652",
}


@book{7959ba771c7543169bde997029a08d37,
  title     = "Process support and knowledge representation in health care",
  author    = "D Riano and R. Lenz and S Miksch and M Peleg and M. Reichert and {ten Teije}, A.C.M.",
  note      = "Gebeurtenis: KR4HC 2013/ProHealth 2013",
  year      = "2013",
  isbn      = "9783319039169",
  series    = "Lecture Notes CS",
  publisher = "Springer",
  number    = "8268",
}


@book{6d058c870939444cbb3e16b4489150dc,
  title     = "Process Support and Knowledge Representation in Health Care",
  author    = "R. Lenz and S Miksch and M Peleg and M. Reichert and D Riano and {ten Teije}, A.C.M.",
  year      = "2013",
  isbn      = "9783642364372",
  series    = "Lecture Notes in Computer Science",
  publisher = "Springer Berlin Heidelberg",
  number    = "7738",
}


@misc{fb9b94c634a94a3e86c09f8fbf31cbd6,
  title   = "Reasoning with Contexts in Description Logics",
  author  = "S. Klarman",
  note    = "Naam instelling promotie: VU Vrije Universiteit Naam instelling onderzoek: VU Vrije Universiteit",
  year    = "2013",
  school  = "Vrije Universiteit Amsterdam",
}


@inbook{02ea4f9400e8437aa29e69d9632eb044,
  title     = "Recycling In Pipelined Query Evaluation",
  author    = "F. Nagel and P.A. Boncz and S. Viglas",
  year      = "2013",
  booktitle = "Proceedings of the IEEE International Conference on Data Engineering (ICDE, 2013)",
  publisher = "IEEE",
}


@inbook{596fae6ddd35499db6748d9a0ca4da69,
  title     = "Redundant Elements in SNOMED CT Concept Definitions",
  author    = "K. Dentler and R. Cornet",
  year      = "2013",
  isbn      = "9783642383267",
  series    = "2013",
  publisher = "Springer",
  pages     = "186--195",
  editor    = "N. Peek and Morales, {R. M.} and M. Peleg",
  booktitle = "Artificial Intelligence in Medicine",
}


@article{c453ec4f76da4737bcf9e9eeeb83521f,
  title     = "Repurposing Benchmark Corpora for Reconstructing Provenance",
  author    = "S. Magliacane and P.T. Groth",
  note      = "Proceedings title: 3rd Workshop on Semantic Publishing Publisher: CEUR-WS.org",
  year      = "2013",
  volume    = "994",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@inbook{f88d8999f1f546cb8b262df18493ba40,
  title     = "Results of the Ontology Alignment Evaluation Initiative 2012",
  author    = "J.L. Aguirre and B.C Grau and K. Eckert and J. Euzenat and Ferrara Alfio and {van Hage}, Willem and L. Hollink and E. Jimenez-Ruiz and C. Meilicke and A. Nikolov and D. Ritze and F. Scharffe and P. Shvaiko and O. Sváb-Zamazal and C. Trojahn and B. Zapilko",
  year      = "2013",
  pages     = "73--115",
  booktitle = "7th ISWC workshop on ontology matching (OM)",
}


@inbook{efb46ed16f7047e2b077a25922736d00,
  title     = "Rough Set Semantics for Identity on the Web",
  author    = "{van Beek}, W. and K.S. Schlobach",
  year      = "2013",
  booktitle = "2013 AAAI Fall Symposium Series",
}


@inbook{d619971b9e074c95b786a3eca3b4ff1f,
  title     = "Rough Set Semantics for Identity on the Web",
  abstract  = "Identity relations are at the foundation of the Linked Open Data initiative and on the Semantic Web in general. They allow the interlinking of alternative descriptions of the same thing. However, many practical uses of owl:sameAs are known to violate its formal semantics. We propose a method that assigns meaning to (the subrelations of) an identity relation using the predicates of the dataset schema. Applications of this approach include automated suggestions for asserting/retracting identity pairs and quality assessment. We also describe an experimental design for this approach.",
  author    = "Wouter Beek and Stefan Schlobach and {van Harmelen}, Frank",
  year      = "2013",
  pages     = "10--13",
  editor    = "{van Harmelen}, F. and J. Hendler and P Hitzler and K Janowicz",
  booktitle = "AAAI Fall Symposium Semantics for Big Data",
  publisher = "AAAI",
}


@article{5c65a8ca57b84d3b8144d1238323ceca,
  title     = "Rule-based Formalization of Eligibility Criteria for Clinical Trials",
  author    = "Z. Huang and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H.",
  note      = "Proceedings title: Artificial Intelligence in Medicine Publisher: Springer Editors: N Peek, R Morales, M Peleg",
  year      = "2013",
  volume    = "7885",
  pages     = "38--47",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@inbook{7185f4a362534934bac69d8b0c40f420,
  title     = "Semantically Modeling Mobile Phone Data for Urban Computing",
  author    = "H. Wang and Z. Huang and N. Zhong",
  year      = "2013",
  booktitle = "Proceedings of the 2013 International Conference on Active Media Technology (AMT2013)",
  publisher = "Springer",
}


@inbook{7a0fd834f646426d8e921691cc2d7cab,
  title     = "Semantic and Qualitative Spatial Reasoning based Road Network Modeling",
  author    = "X. Zhang and Z. Huang",
  year      = "2013",
  booktitle = "Proceedings of 2013 International Workshop on Big Web Data",
  publisher = "Springer",
}


@inbook{71265264f4d14daab72cb8509624aa67,
  title     = "Semantic Approach for Rational Use of Antibiotics: A Perspective from Clinical Research",
  author    = "X. Hua and Z. Huang and R. Yang and Q. Hu",
  year      = "2013",
  booktitle = "Proceedings of 2013 International Workshop on Semantic Technology for eHealth (STeH2013)",
  publisher = "Springer",
}


@article{827ae4d020e343c4863bad421de8613c,
  title     = "SemanticCT: A Semantically-Enabled System for Clinical Trials",
  author    = "Z. Huang and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H.",
  note      = "Proceedings title: Process Support and Knowledge Representation in Health Care Publisher: Springer",
  year      = "2013",
  volume    = "8268",
  pages     = "11--25",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
  number    = "iss",
}


@inbook{47e6380dcef045b0a1e8b95e8729e4c0,
  title     = "Semantic Integration of Patient Data and Quality Indicators based on openEHR Archetypes",
  author    = "K. Dentler and {ten Teije}, A. and R. Cornet and {De Keizer}, N.",
  year      = "2013",
  isbn      = "9783642364389",
  series    = "2013",
  pages     = "85--97",
  booktitle = "Joint Workshop on Process-oriented Information Systems in Healthcare (ProHealth) and Knowledge Representation for Health Care (KRH4C)",
}


@inbook{a27bf65091054e30b0d10bdc36cd7812,
  title     = "Semantic Transformation and Generation for Guidelines of Rational Use of Antibiotics",
  author    = "J. Gu and Q. Hu and Z. Huang",
  year      = "2013",
  pages     = "20--24",
  booktitle = "Journal of China Digital Medicine, Vol.8, No.4",
}


@misc{499f192ce0124f7d8a23b831ffc4eaa4,
  title  = "SeSRUA: a Prototype for Intelligent Monitoring on Rational Use of Antibiotics",
  author = "J. Gu and Z. Huang",
  note   = "Proceedings title: Proceedings of the 3rd Joint International Semantic Technology Conference (JIST2013) Publisher: Springer",
  year   = "2013",
}


@inbook{f72142d4073d4b14be4b6cda23da0219,
  title     = "Stay Point Identification of Mobile Phone Trajectory",
  author    = "R. Du and J. Huang and H. Wang and N. Zhong and Z. Huang",
  year      = "2013",
  pages     = "26--30",
  booktitle = "Proceedings of 2013 China Web Intelligence Conference(CRSSC-CWI-CGrC2013),",
}


@inbook{e6d191f6fd8744bd9b476c79ba20be76,
  title     = "The Design and Implementation of Modern Column-Oriented Database Systems",
  author    = "D. Abadi and P.A. Boncz and S. Harizopoulos and S. Idreos and S. Madden",
  year      = "2013",
  series    = "5",
  publisher = "NOW Publishers",
  number    = "3",
  pages     = "297--280",
  booktitle = "Foundations and Trends in Databases",
}


@inbook{b14764c9d8b84f6693aeb52342126caa,
  title     = "The Linked Data Benchmark Council Project",
  author    = "P.A. Boncz and I. Fundulaki and A. Gubichev and {Larriba Pey}, J. and T. Neumann",
  year      = "2013",
  pages     = "121--129",
  booktitle = "Datenbank-Spektrum July 2013, Volume 13, Issue 2",
  publisher = "Springer Verlag",
}


@inbook{d5c0fdc5d7844aac9a5be1ed8b03bdf6,
  title     = "Towards automatic patient eligibility assessment: from free-text criteria to queries",
  abstract  = "The presented work contributes to bridging the representation of clinical trials and patient data. Our ultimate goal is to support the trial recruitment, by automating the process of formalizing eligibility criteria of clinical trials, starting from free text of criteria and leading to a computable representation. This paper discusses the final step in the pipeline i.e. generating queries from the structured representation consisting of detected patterns and semantic entities. The queries allow to evaluate patient eligibility for a given trial. To enable easy incorporation of semantic reasoning using medical ontologies, we built the queries in SPARQL and use the OWL representation of one the standards for patient data storage - openEHR archetypes and NCI ontology. The available public repository of archetypes and the expressivity of SPARQL allow to create template queries for the majority of patterns. © 2013 Springer-Verlag.",
  author    = "K. Milian and {ten Teije}, A.C.M.",
  year      = "2013",
  doi       = "10.1007/978-3-642-38326-7_12",
  isbn      = "9783642383250",
  pages     = "78--83",
  editor    = "N. Peek and M. Morales and M. Peleg",
  booktitle = "14th Conference on Artificial Intelligence in Medicine (AIME2013)",
  publisher = "Springer",
}


@inbook{567a5dbf15aa43a5bcc58535a8a38ed9,
  title     = "TPC-H Analyzed: Hidden Messages And Lessons Learned From An Influential Benchmark",
  author    = "P.A. Boncz and T. Neumann and O. Erling",
  year      = "2013",
  editor    = "M. Poess and R. Niambar",
  booktitle = "Proceedings of the TPC Technology Conference on Performance Evaluation $1amp; Benchmarking (TPCTC, 2013)",
  publisher = "Springer Verlag",
}


@inbook{4027a59b0e8c47a58608b9e0939ecf3c,
  title     = "Using Semantic Technology for Automatic Verification of Road Signs",
  author    = "D. Xu and Z. Huang and Q. Liu",
  year      = "2013",
  booktitle = "Proceedings of 2013 International Workshop on Semantic Technology for Smart City",
  publisher = "Springer",
}


@inbook{ea538ee5ab084d968ad1e30543d6d88a,
  title     = "Visual Interface Tools for Advanced Patient Data Generator",
  author    = "M. Zhang and Z. Huang and J. Gu",
  year      = "2013",
  pages     = "26--30",
  booktitle = "Journal of China Digital Medicine, Vol.8, No.5",
}


@inbook{67f3c7cdbb1b4f54a1bdd6c895722c11,
  title     = "Who are we talking about?: identifying scientific populations online",
  author    = "J.M. Birkholz and S. Wang and P.T. Groth and S. Magliacane",
  year      = "2013",
  booktitle = "Chinese Semantic Web Symposium & Web Science Conference Proceedings",
  publisher = "Springer Publications",
}


@inbook{27bfea5c82174089bb1ae573e8a27dbc,
  title     = "YASGUI: Not Just Another SPARQL Client",
  author    = "L.J. Rietveld and R. Hoekstra",
  year      = "2013",
  booktitle = "Proceedings of the ESWC2013 workshop on Services and Applications over Linked APIs and Data (SALAD 2013)",
}


@inbook{83f7e8f504a94918ad49af0d3b00ced4,
  title     = "YASGUI: Not Just Another SPARQL Client",
  author    = "L.J. Rietveld and R.J. Hoekstra",
  year      = "2013",
  editor    = "M Maleshkova and R. Verborgh and T. Steiner",
  booktitle = "Proceedings on the ESWC2013 Workshop on Services and Applications over Linked APIs and Data",
}


@article{34a3d04075e549dbac0a02e26910a7c4,
  title     = "Erratum: WebPIE: A Web-scale Parallel Inference Engine using MapReduce (Journal of Web Semantics (2012) 10 (59-75))",
  author    = "Jacopo Urbani and Spyros Kotoulas and Jason Maassen and {Van Harmelen}, Frank and Henri Bal",
  year      = "2012",
  month     = "12",
  doi       = "10.1016/j.websem.2012.09.005",
  volume    = "17",
  pages     = "44",
  journal   = "Journal of Web Semantics",
  issn      = "1570-8268",
  publisher = "Elsevier",
}


@article{456d66fe3a4346a6884c0eacd09cb67f,
  title     = "Reply to comment on {"}webPIE: A Web-scale parallel inference engine using MapReduce{"}",
  author    = "Jacopo Urbani and Spyros Kotoulas and J. Maassen and {Van Harmelen}, Frank and Henri Bal",
  year      = "2012",
  month     = "9",
  doi       = "10.1016/j.websem.2012.09.002",
  volume    = "15",
  pages     = "71--72",
  journal   = "Journal of Web Semantics",
  issn      = "1570-8268",
  publisher = "Elsevier",
}


@article{6acccbd7ec724673ad91a05f587f628c,
  title     = "A Legal Case OWL Ontology with an Instantiation of Popov v. Hayashi",
  abstract  = "The paper provides an OWL ontology for legal cases with an instantiation of the legal case Popov v. Hayashi. The ontology makes explicit the conceptual knowledge of the legal case domain, supports reasoning about the domain, and can be used to annotate the text of cases, which in turn can be used to populate the ontology. A populated ontology is a case base which can be used for information retrieval, information extraction, and case based reasoning. The ontology contains not only elements for indexing the case (e.g. the parties, jurisdiction, and date), but as well elements used to reason to a decision such as argument schemes and the components input to the schemes. We use the Protégé ontology editor and knowledge acquisition system, current guidelines for ontology development, and tools for visual and linguistic presentation of the ontology. © 2012 Springer Science+Business Media B.V.",
  author    = "A.Z. Wyner and R.J. Hoekstra",
  year      = "2012",
  doi       = "10.1007/s10506-012-9119-6",
  journal   = "Artificial Intelligence and Law",
  issn      = "0924-8463",
  publisher = "Springer Netherlands",
}


@inbook{416651e4d3bd4ac28a667b0dcda42a48,
  title     = "A method of contrastive reasoning with inconsistent ontologies,",
  author    = "J. Fang and Z. Huang and {van Harmelen}, F.",
  year      = "2012",
  pages     = "1--16",
  booktitle = "the Semantic Web",
  publisher = "Springer",
}


@inbook{e8dd6e2eebb1487d931f37587c36a5ea,
  title     = "An efficient approach to debugging ontologies based on patterns",
  author    = "Q. Ji and Z. Gao and Z. Huang and Man Zhu",
  year      = "2012",
  pages     = "425--433",
  booktitle = "the Semantic Web",
  publisher = "Springer",
}


@inbook{797686d3a7d84ebb9571f182d11ebfa3,
  title     = "An Introduction to Semantic Technology",
  author    = "Z. Huang and Jinshong Li",
  year      = "2012",
  isbn      = "9787308101295",
  pages     = "1--12",
  editor    = "Jinshong Li and Z Huang",
  booktitle = "Semantic Technology for Biomedical Science",
  publisher = "Zhejiang University Press",
}


@inbook{c248a58a8b534fe7b25aee70a2275a10,
  title     = "Annotating Evidence Based Clinical Guidelines -- A Lightweight Ontology",
  author    = "R.J. Hoekstra and {de Waard}, A. and R. Vdovjak",
  year      = "2012",
  editor    = "A Paschke and A Burger and P Roma",
  booktitle = "Proceedings of SWAT4LS 2012",
}


@book{c6171b88c2834c5988ce6f321165c8d2,
  title     = "A Semantic Web Primer",
  author    = "G. Antoniou and P.T. Groth and {van Harmelen}, F. and R.J. Hoekstra",
  year      = "2012",
  isbn      = "0262018284",
  publisher = "MIT Press",
}


@article{d72c731ae13c40f29035d695cedac026,
  title     = "Automated Evaluation of Annotators for Museum Collections using Subjective Logic",
  author    = "D. Ceolin and A. Nottamkandath and W.J. Fokkink",
  note      = "Proceedings title: Trust Management VI - 6th IFIP WG 11.11 International Conference Publisher: Springer ISBN: 978-3-642-29851-6 Editors: D. Patel, H. McKnight, R. Moona, T. Dimitrakos",
  year      = "2012",
  volume    = "374",
  pages     = "232--239",
  journal   = "IFIP",
  issn      = "1571-5736",
  publisher = "Springer New York",
}


@article{e6442fe4d9fc457fad3ecb8c9594355a,
  title     = "Automatic Extraction of Legal Concepts and Definitions",
  abstract  = "In this paper we present the results of an experiment in automatic concept and definition extraction from written sources of law using relatively simple natural language and standard semantic web technology. The software was tested on six laws from the tax domain. © 2012 The authors and IOS Press. All rights reserved.",
  author    = "R.G.F. Winkels and R.J. Hoekstra",
  note      = "Proceedings title: Legal Knowledge and Information Systems, Jurix 2012: the Twenty-Fifth Annual International Conference Publisher: IOS Press Place of publication: Amsterdam ISBN: 978-1-61499-166-3 Editors: B. Schäfer",
  year      = "2012",
  doi       = "10.3233/978-1-61499-167-0-157",
  volume    = "250",
  pages     = "157--166",
  journal   = "Frontiers in Artificial Intelligence and Applications",
  issn      = "0922-6389",
  publisher = "IOS Press",
}


@inbook{b3749adf98ef451195432044f472ea7b,
  title     = "Automatic Metadata Annotation through Reconstructing Provenance",
  author    = "P.T. Groth and S. Magliacane and Y Gil",
  year      = "2012",
  booktitle = "Third International Workshop on the role of Semantic Web in Provenance Management",
  publisher = "CEUR Proceedings",
}


@inbook{6f37074ad9ad438f9b763178fbc3b8a1,
  title     = "Building a Library of Eligibility Criteria to support design of clinical trials",
  author    = "K. Milian and A. Bucur and {van Harmelen}, F.",
  year      = "2012",
  isbn      = "9783642338755",
  pages     = "327--336",
  editor    = "{ten Teije}, A and J Völker and S Handschuh and H Stuckenschmidt and M d’Acquin and A Nikolov and N Aussenac-Gilles and N Hernandez",
  booktitle = "Knowledge Engineering and Knowledge Management",
  publisher = "Springer Link",
}


@article{96e6571946374e9cb3b98d46b8819c80,
  title     = "Capturing Common Knowledge about Tasks",
  abstract  = "Although to-do lists are a ubiquitous form of personal task management, there has been no work on intelligent assistance to automate, elaborate, or coordinate a users to-dos. Our research focuses on three aspects of intelligent assistance for to-dos. We investigated the use of intelligent agents to automate to-dos in an office setting. We collected a large corpus from users and developed a paraphrase-based approach to matching agent capabilities with to-dos. We also investigated to-dos for personal tasks and the kinds of assistance that can be offered to users by elaborating on them on the basis of substep knowledge extracted from the Web. Finally, we explored coordination of user tasks with other users through a to-do management application deployed in a popular social networking site. We discuss the emergence of Social Task Networks, which link users tasks to their social network as well as to relevant resources on the Web. We show the benefits of using common sense knowledge to interpret and elaborate to-dos. Conversely, we also show that to-do lists are a valuable way to create repositories of common sense knowledge about tasks. © 2012 ACM.",
  author    = "Y Gil and V Ratnakar and T Chklovski and P.T. Groth and D Vrandecic",
  year      = "2012",
  doi       = "10.1145/2362394.2362397",
  volume    = "2",
  pages     = "1--35",
  journal   = "ACM Transactions on Interactive Intelligent Systems",
  issn      = "2160-6455",
  publisher = "Association for Computing Machinery (ACM)",
  number    = "3",
}


@inbook{9fde87b3c1074321bc6bb454aacbedc0,
  title     = "Contagion of Habitual Behaviour in Social Networks: an Agent-Based Model",
  author    = "M.C.A. Klein and N.M. Mogles and J. Treur and {van Wissen}, A.",
  year      = "2012",
  booktitle = "Proceedings of the 4th International Conference on Social Computing",
  publisher = "IEEE Computer Society Press",
}


@article{61de931a71f5471f99b91c13c31fbca9,
  title     = "Dealing with the Messiness of the Web of Data",
  author    = "K.S. Schlobach and C.A. Knoblock",
  year      = "2012",
  doi       = "10.1016/j.websem.2012.05.004",
  volume    = "14",
  journal   = "Journal of Web Semantics",
  issn      = "1570-8268",
  publisher = "Elsevier",
  number    = "1",
}


@inbook{5cd7b28ffb8f45dca28c9969aebecff7,
  title     = "Efficient Execution of Top-K SPARQL Queries",
  author    = "S. Magliacane and A. Bozzon and {Della Valle}, E",
  year      = "2012",
  booktitle = "ISWC 2012",
}


@inbook{a4cf3abc5c9f4e67ad01a0e71b8aa1f4,
  title     = "Evolutionary and Swarm Computing for scaling up the Semantic Web",
  author    = "C.D.M. Gueret and K.S. Schlobach and K. Dentler and M.C. Schut",
  year      = "2012",
  booktitle = "Proceedings of the 24th Benelux Conference on Artificial Intelligence (BNAIC 2012)",
}


@inbook{fbacf84931b147dfaa711d0ec75dd8a0,
  title     = "Evolutionary and Swarm computing for the Semantic Web",
  abstract  = "The Semantic Web has become a dynamic and enormous network of typed links between data sets stored on different machines. These data sets are machine readable and unambiguously interpretable, thanks to their underlying standard representation languages. The expressiveness and flexibility of the publication model of Linked Data has led to its widespread adoption and an ever increasing publication of semantically rich data on the Web. This success however has started to create serious problems as the scale and complexity of information outgrows the current methods in use, which are mostly based on database technology, expressive knowledge representation formalism and high-performance computing. We argue that methods from computational intelligence can play an important role in solving these problems. In this paper we introduce and systemically discuss the typical application problems on the Semantic Web and argue that the existing approaches to address their underlying reasoning tasks consistently fail because of the increasing size, dynamicity and complexity of the data. For each of these primitive reasoning tasks we will discuss possible problem solving methods grounded in Evolutionary and Swarm computing, with short descriptions of existing approaches. Finally, we will discuss two case studies in which we successfully applied soft computing methods to two of the main reasoning tasks; an evolutionary approach to querying, and a swarm algorithm for entailment. © 2012 IEEE.",
  author    = "C.D.M. Gueret and K.S. Schlobach and K. Dentler and M.C. Schut and A.E. Eiben",
  year      = "2012",
  doi       = "10.1109/MCI.2012.2188583",
  series    = "2",
  pages     = "16--31",
  booktitle = "IEEE Computational Intelligence Magazine",
}


@inbook{ddcaf8c9de6e4303bd0c571783a015c7,
  title     = "Extending SPARQL algebra to support efficient evaluation of top-k SPARQL queries",
  author    = "A. Bozzon and {Della Valle}, E and S. Magliacane",
  year      = "2012",
  pages     = "143--156",
  booktitle = "Search Computing",
  publisher = "Springer",
}


@inbook{d7eda29c70c0423db2ba74dd4b550f25,
  title     = "Formalizalization of clinical trial eligibility criteria: Evaluation of a pattern-based approach",
  abstract  = "The semi-automatic evaluation of eligibility criteria can facilitate the recruitment for clinical trials, timely completion of studies and generation of clinical evidence about new approaches to treatment, prevention and diagnosis. Because eligibility criteria are represented as free text, automatically extracting their meaning and evaluating them for a particular patient is challenging. This paper presents our approach to the problem of automatic interpretation of criteria meaning. It is based on detecting in text semantic entities (diseases, treatment, measurements etc.) using ontology annotators and semantic taggers, and detecting predefined patterns providing the contextual information in which these entities occur. Evaluation of the approach is the main subject of the paper. It covers several aspects: precision and recall of the pattern detection algorithm and the assessment of the implications of using the identified patterns to find potential candidates. It was performed manually using a subset of patterns and randomly selected 33 trials from ClinicalTrials.gov. The average precision and recall of pattern detection algorithm calculated for selected patterns is 0.9 and 0.91, meaning that in most cases using the patterns can lead to correct interpretation of criteria and can support patient recruitment. © 2012 IEEE.",
  author    = "K. Milian and A. Bucur and {ten Teije}, A.C.M.",
  year      = "2012",
  doi       = "10.1109/BIBM.2012.6392733",
  pages     = "1--4",
  editor    = "J. Gao and R. Alhaij and W. Dubitzky and L. Ungar and C. Wu and A, Christianson and M. Liebman and X. Hu",
  booktitle = "IEEE International Conference of Bioinformatics and Biomedicine 2012",
}


@inbook{a63e490a3998468b91822633ee456c4a,
  title     = "Formalization of clinical trial eligibility criteria: Evaluation of a pattern-based approach",
  author    = "K. Milian and A. Bucur and {ten Teije}, A. and {van Harmelen}, F.",
  year      = "2012",
  pages     = "495--498",
  editor    = "J Gao and R Alhaij and W Dubitzky and L Ungar and C Wu and A Christianson and M Liebman and X Hu",
  booktitle = "2012 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)",
  publisher = "IEEE Computer Society Technical Committee on Bioinformatics",
}


@inbook{0cce0204a6f44e39a461a845757f99c3,
  title     = "Formal Verification of Data Provenance Records",
  author    = "S. Klarman and S. Schlobach and L. Serafini",
  year      = "2012",
  booktitle = "Proc. of the 11th International Semantic Web Conference (ISWC-12)",
}


@inbook{da8e355aef0f4fbfa360cea00b97cbb1,
  title     = "Formal Verification of Data Provenance Records",
  author    = "S. Klarman and K.S. Schlobach and L. Serafini",
  year      = "2012",
  booktitle = "Proceedings of the 11th International Semantic Web Conference (ISWC-12)",
  publisher = "Springer",
}


@article{bbff608b8aae4bfe9ef062f842c3d8de,
  title     = "From Cooperative Scans To Predictive Buffer Management",
  author    = "M. Switakowski and P.A. Boncz and M. Zukowski",
  year      = "2012",
  doi       = "10.14778/2367502.2367515",
  volume    = "5",
  pages     = "1748--1758",
  journal   = "PVLDB",
  issn      = "2150-8097",
  publisher = "Very Large Data Base Endowment Inc.",
  number    = "12",
}


@inbook{1b1dceaa33a744c2aba52356ecf4de1c,
  title     = "From X100 to Vectorwise: opportunities, challenges and things most researchers do not think about",
  author    = "M. Zukowski and P.A. Boncz",
  year      = "2012",
  booktitle = "Proc. ACM SIGMOD",
}


@inbook{56ccdaac43e642d8b3775ea52b8a0aee,
  title     = "Heuristics-based query optimisation for SPARQL",
  author    = "P. Tsialiamanis and L. Sidirourgos and V. Christophides and I. Fundulaki and P.A. Boncz",
  year      = "2012",
  booktitle = "Proc. International Conference on Extending Database Technology (EDBT)",
}


@article{ecad8c82529d43e5b56b5f2845e7fc7c,
  title     = "Human-Agent Team Formation in Dynamic Environments",
  abstract  = "Teamwork between humans and computer agents has become increasingly prevalent. This paper presents a behavioral study of fairness and trust in a heterogeneous setting comprising both computer agents and human participants. It investigates people's choice of teammates and their commitment to their teams in a dynamic environment in which actions occur at a fast pace and decisions are made within tightly constrained time frames, under conditions of uncertainty and partial information. In this setting, participants could form teams by negotiating over the division of a reward for the successful completion of a group task. Participants could also choose to defect from their existing teams in order to join or create other teams. Results show that when people form teams, they offer significantly less reward to agents than they offer to people. The most significant factor affecting people's decisions whether to defect from their existing teams is the extent to which they had successful previous interactions with other team members. Also, there is no significant difference in people's rate of defection from agent-led teams as compared to their defection from human-led teams. These results are significant for agent designers and behavioral researchers who study human-agent interactions. © 2011 Elsevier Ltd. All rights reserved.",
  author    = "{van Wissen}, A. and Y. Gal and B. Kamphorst and V. Dignum",
  year      = "2012",
  doi       = "10.1016/j.chb.2011.08.006",
  volume    = "28",
  pages     = "23--33",
  journal   = "Computers in Human Behavior",
  issn      = "0747-5632",
  publisher = "Pergamon",
  number    = "1",
}


@article{1f65eccd5a914b5eb02bb1b34493cc43,
  title     = "Instance-Based Ontology Matching by Instance Enrichment",
  abstract  = "The ontology matching (OM) problem is an important barrier to achieve true Semantic Interoperability. Instance-based ontology matching (IBOM) uses the extension of concepts, the instances directly associated with a concept, to determine whether a pair of concepts is related or not. While IBOM has many strengths it requires instances that are associated with concepts of both ontologies, (i.e) dually annotated instances. In practice, however, instances are often associated with concepts of a single ontology only, rendering IBOM rarely applicable. In this paper we discuss a method that enables IBOM to be used on two disjoint datasets, thus making it far more generically applicable. This is achieved by enriching instances of each dataset with the conceptual annotations of the most similar instances from the other dataset, creating artificially dually annotated instances. We call this technique instance-based ontology matching by instance enrichment (IBOMbIE). We have applied the IBOMbIE algorithm in a real-life use-case where large datasets are used to match the ontologies of European libraries. Existing gold standards and dually annotated instances are used to test the impact and significance of several design choices of the IBOMbIE algorithm. Finally, we compare the IBOMbIE algorithm to other ontology matching algorithms.",
  author    = "B. Schopman and S. Wang and A.H.J.C.A. Isaac and K.S. Schlobach",
  year      = "2012",
  doi       = "10.1007/s13740-012-0011-z",
  volume    = "1",
  pages     = "219--236",
  journal   = "Journal on Data Semantics",
  issn      = "1861-2032",
  publisher = "Springer International Publishing AG",
  number    = "4",
}


@inbook{5771d761d4394550bb4767e72775ee43,
  title     = "Introduction to Scalable Semantic Processing",
  author    = "Z. Huang and N. Zhong",
  year      = "2012",
  isbn      = "9787040362466",
  pages     = "1--13",
  booktitle = "Scalable Semantic Data Processing",
  publisher = "Higher Education Press",
}


@book{3af6b37551a14a2f93bac257cd0f7c54,
  title     = "Knowledge Representation for Health-Care",
  author    = "R. David and {ten Teije}, A.C.M. and S Miksch",
  year      = "2012",
  isbn      = "9783642276972",
  series    = "LNAI",
  publisher = "Springer",
  number    = "6924",
}


@article{027e5d8a21474d229828ee969ecc36e0,
  title     = "Large-Scale Storage and Reasoning for Semantic Data Using Swarms",
  abstract  = "Scalable, adaptive and robust approaches to store and analyze the massive amounts of data expected from Semantic Web applications are needed to bring the Web of Data to its full potential. The solution at hand is to distribute both data and requests onto multiple computers. Apart from storage, the annotation of data with machine-processable semantics is essential for realizing the vision of the Semantic Web. Reasoning on webscale data faces the same requirements as storage. Swarm-based approaches have been shown to produce near-optimal solutions for hard problems in a completely decentralized way. We propose a novel concept for reasoning within a fully distributed and self-organized storage system that is based on the collective behavior of swarm individuals and does not require any schema replication. We show the general feasibility and efficiency of our approach with a proof-of-concept experiment of storage and reasoning performance. Thereby, we positively answer the research question of whether swarm-based approaches are useful in creating a large-scale distributed storage and reasoning system. © 2012 IEEE.",
  author    = "H. Mühleisen and K. Dentler",
  year      = "2012",
  doi       = "10.1109/MCI.2012.2188586",
  volume    = "7",
  pages     = "32--44",
  journal   = "IEEE Computational Intelligence Magazine",
  issn      = "1556-603X",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  number    = "2",
}


@inbook{6040c5bcf28641bc9b2f1055204c96d3,
  title     = "Linked Humanities Data: The Next Frontier?",
  author    = "A. Merono and A. Ashkpour and L.J. Rietveld and R.J. Hoekstra and K.S. Schlobach",
  year      = "2012",
  editor    = "T. Kauppinen and L.C. Pouchard and C. Keßler",
  booktitle = "Proceedings of the Second International Workshop on Linked Science 2012 - Tackling Big Data",
  publisher = "CEUR Workshop Proceedings",
}


@inbook{61f81fe5ad9345acb630687bba332d76,
  title     = "Linked Life Data",
  author    = "Z. Huang",
  year      = "2012",
  isbn      = "9787308101295",
  pages     = "261--280",
  editor    = "Jinshong Li and Z Huang",
  booktitle = "Semantic Technology for Biomedical Science,",
  publisher = "Zhejiang University Press",
}


@inbook{5ded4b5197684d7c882c591a4a440581,
  title     = "Linked Life Data in LarKC,",
  author    = "Z. Huang",
  year      = "2012",
  isbn      = "9787040362466",
  pages     = "147--165",
  booktitle = "Scalable Semantic Data Processing",
  publisher = "Higher Education Press",
}


@inbook{f2522751004a4642a1017e98402512ab,
  title     = "Linked Stream Data Processing: Facts and Figures",
  author    = "Danh, {L. P.} and Minh, {D. T.} and {Minh Duc}, P. and P.A. Boncz and E Thomas and F. Michael",
  year      = "2012",
  booktitle = "Proc. International Semantic Web Conference (ISWC)",
}


@inbook{c5eefdde6c4d491e8b19a44a123e7025,
  title     = "Making Web-Scale Semantic Reasoning More Service-Oriented: The Large Knowledge Collider",
  author    = "A Cheptsov and Z. Huang",
  year      = "2012",
  booktitle = "Proceedings of the 2012 Workshop on Advanced Reasoning Technology for e-Science (ART2012)",
}


@article{f6c9ad61f7874efd8ba4faed5aefdfa4,
  title     = "Measuring Stress-Reducing Effects of Virtual Training Based on Subjective Response",
  abstract  = "Training to cope with negative emotions or stress is important for professionals with a high pressure job, such as police officers and military personnel. The work reported in this paper is part of a project that aims to develop a Virtual Reality based training environment for such professionals. As a first step in that direction, an experiment was performed to investigate the impact of virtual training on the subjects' experienced stress responses. A group of 10 participants was asked to rate the subjective emotional intensity of a set of affective pictures at two different time points, separated by six hours. Half of the group performed a session of virtual training in between, in which they were asked to actively apply reappraisal strategies as a form of emotion regulation, whereas the other half did not take part in this training session. The results indicate that the virtual training caused the participants in that group to give significantly lower ratings for the emotional intensity of the pictures. © 2012 Springer-Verlag.",
  author    = "T. Bosse and C. Gerritsen and {de Man}, J. and J. Treur",
  note      = "Proceedings title: Proceedings of the 19th International Conference on Neural Information Processing, ICONIP'12 Publisher: Springer Verlag",
  year      = "2012",
  doi       = "10.1007/978-3-642-34475-6_39",
  pages     = "322--330",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@inbook{ffe1a0ee4e494191b8a31eda54b63bb5,
  title     = "New Advances of Scalable Semantic Data Processing",
  author    = "Z. Huang",
  year      = "2012",
  booktitle = "Special Issue of CRSSC-CWI-CGrC2012,",
}


@inbook{4fcbde6b88ad4bf3a089118b8bf50129,
  title     = "Non-standard Reasoning of Semantic Data",
  author    = "J. Fang and Z. Huang",
  year      = "2012",
  isbn      = "9787040362466",
  pages     = "102--123",
  booktitle = "Scalable Semantic Data Processing",
  publisher = "Higher Education Press",
}


@inbook{bd10aee30c2545c885d4fbef216fa1d8,
  title     = "On the Impacts of Emotion on Learning in a Social Context: A Conceptual and Computational Analysis",
  author    = "J. Treur and {van Wissen}, A.",
  year      = "2012",
  booktitle = "Proceedings of the 12th International Conference on Intelligent Agent Technology",
  publisher = "IEEE Computer Society Press",
}


@article{ce09840e1ae44770af8624c102bd7e5b,
  title     = "Open PHACTS: Semantic interoperability for drug discovery",
  abstract  = "Open PHACTS is a public-private partnership between academia, publishers, small and medium sized enterprises and pharmaceutical companies. The goal of the project is to deliver and sustain an 'open pharmacological space' using and enhancing state-of-the-art semantic web standards and technologies. It is focused on practical and robust applications to solve specific questions in drug discovery research. OPS is intended to facilitate improvements in drug discovery in academia and industry and to support open innovation and in-house non-public drug discovery research. This paper lays out the challenges and how the Open PHACTS project is hoping to address these challenges technically and socially. © 2012 Elsevier Ltd.",
  author    = "A.J William and L Harland and P.T. Groth and S Pettifier and C Christine and E.L Willighagen and C.T. Evelo and N Blomberg and G. Ecker and C.A. Goble and B Mons",
  year      = "2012",
  doi       = "10.1016/j.drudis.2012.05.016",
  volume    = "17",
  pages     = "1188--1198",
  journal   = "Drug Discovery Today",
  issn      = "1359-6446",
  publisher = "Elsevier Limited",
  number    = "21-22",
}


@article{cdf20e92e72046a185939534c76f5b79,
  title     = "Paraconsistent Query Answering Over DL-Lite Ontologies",
  author    = "L. Zhou and H. Huang and G Qi and Y. Ma and Z. Huang",
  year      = "2012",
  volume    = "10",
  pages     = "19--31",
  journal   = "Web Intelligence and Agent Systems: An International Journal",
  issn      = "1570-1263",
  publisher = "IOS Press",
  number    = "1",
}


@inbook{ccdd46d6837d45ecb24d0ab9833af6da,
  title     = "Patterns of clinical trial eligibility criteria",
  author    = "K. Milian and {ten Teije}, A. and A. Bucur and F. Harmelen",
  year      = "2012",
  editor    = "D Riaño and {ten Teije}, A and S Miksch",
  booktitle = "Knowledge Representation for Health Care, LNAI6924, KR4HC'11",
}


@book{fc8022c3bb43442b9e930ea75c1df055,
  title     = "Proceedings 18th International Conference Knowledge Engineering and Knowledge Management; , EKAW 2012,",
  author    = "{ten Teije}, A.C.M. and J Volker and S. Handschuh and H. Stuckenschmidt and M. d'Acquin and A. Nikolov and N. Aussenac-Gilles and N. Hernandez",
  year      = "2012",
  isbn      = "9783642338762",
  series    = "LNAI",
  publisher = "Springer",
  number    = "7603",
}


@book{638088cef81e479e9c4c16b17fc976f4,
  title     = "Provenance and Annotation of Data and Processes: 4th International Provenance and Annotation Workshop, IPAW 2012",
  author    = "P.T. Groth and J Frew",
  year      = "2012",
  isbn      = "9783642342219",
  series    = "Lecture Notes in Computer Science",
  publisher = "Springer",
}


@inbook{277caa0ce2bc411f8722d8c2abad62cc,
  title     = "Query processing of pre-partitioned data using Sandwich Operators",
  author    = "P.A. Boncz and S. Baumann and K.-U. Sattler",
  year      = "2012",
  booktitle = "Proceedings of International Workshop on Business Intelligence for the Real Time Enterprise (BIRTE)",
}


@inbook{38e94887727d497296f24b971550c5ab,
  title     = "Reasoning and Deciding",
  author    = "Z. Huang",
  year      = "2012",
  isbn      = "9787040362466",
  pages     = "82--101",
  booktitle = "Scalable Semantic Data Processing,",
  publisher = "Higher Education Press",
}


@inbook{b1acf7831d544ea6bbd31aad17e3c76c,
  title     = "Recognition of Chinese Personal Names Based on CRFs and Law of Names",
  author    = "Z. Gao and Y Gui and M. Zhu and Z. Huang",
  year      = "2012",
  booktitle = "The First International Workshop on Information Extraction and Knowledge Base Building,",
}


@inbook{039fca9a9ceb4c9cb7056149fdb994a6,
  title     = "Reconstructing Provenance",
  author    = "S. Magliacane",
  year      = "2012",
  booktitle = "ISWC 2012",
}


@inbook{db7c73524b304f79a8e964cd5552fb2e,
  title     = "Replication for Linked Data",
  abstract  = "With the Semantic Web scaling up, and more triple-stores with update facilities being available, the need for higher levels of simultaneous triple-stores with identical information becomes more and more urgent. However, where such Data Replication approaches are common in the database community, there is no comprehensive approach for data replication for the Semantic Web. In this research proposal, we will discuss the problem space and scenarios of data replication in the Semantic Web, and explain how we plan on dealing with this issue",
  author    = "L.J. Rietveld",
  year      = "2012",
  booktitle = "International Semantic Web Conference, Doctoral Consortium",
}


@article{83d9aa607eb94f2e94aaa982ce6c9cc0,
  title     = "Requirements for Provenance on the Web",
  author    = "P.T. Groth and Y Gil and J Cheney and S Miles",
  year      = "2012",
  doi       = "10.2218/ijdc.v7i1.213",
  volume    = "7",
  journal   = "International Journal of Digital Curation",
  issn      = "1746-8256",
  publisher = "Digital Curation Centre",
  number    = "1",
}


@inbook{ed0db58a311e4694b6fa9f4a37a6c87e,
  title     = "Robust Runtime Optimization and Skew-Resistant Execution of Analytical SPARQL Queries on Pig",
  author    = "S. Kotoulas and J. Urbani and P.A. Boncz and P. Mika",
  year      = "2012",
  booktitle = "Proc. International Semantic Web Conference (ISWC)",
}


@inbook{cad36bab91c64dc4855fe93387c71d78,
  title     = "S3G2: a Scalable Structure-correlated Social Graph Generator",
  author    = "{Minh Duc}, P. and P.A. Boncz and O. Erling",
  year      = "2012",
  booktitle = "Transaction Processing Council Technical Conference (TPCTC)",
}


@inbook{1808b981189c4e999f5f72a016dc20e4,
  title     = "Scalable Semantic Data Processing and Gene Study",
  author    = "Z. Huang",
  year      = "2012",
  isbn      = "9787040362466",
  pages     = "183--206",
  booktitle = "Scalable Semantic Data Processing",
  publisher = "Higher Education Press",
}


@book{9d22f11179a2465ca027532a0a6ef323,
  title     = "Scalable Semantic Data Processing: Platform, Technology, and Application",
  author    = "Z. Huang and N. Zhong",
  year      = "2012",
  isbn      = "9787040362466",
  publisher = "Higher Education Press",
}


@inbook{ab50072485934d31a74464737f1d5563,
  title     = "Scientific Lenses over Linked Data: An approach to support task specific views of the data. A vision.",
  author    = "C Brenninkmeijer and C.T. Evelo and C.A. Goble and A.J.G Gray and P.T. Groth and S Pettifier and R. Stevens and A.J. Williams",
  year      = "2012",
  booktitle = "Proceedings of the Second International Workshop on Linked Science 2012 - Tackling Big Data",
  publisher = "CEUR Proceedings",
}


@inbook{4e374fd035be4c859bd3482dea1aa1ce,
  title     = "Semantic Precision and Recall for Evaluating Incoherent Ontology Mappings",
  author    = "Q. Ji and Z. Gao and Z. Huang and M. Zhu",
  year      = "2012",
  booktitle = "Proceedings of the 2012 International Conference on Active Media Technology (AMT2012)",
  publisher = "Springer",
}


@inbook{3831d8e29a9e4cc697624f395a137c3b,
  title     = "Semantic Search for Biomedical Science",
  author    = "Z. Huang",
  year      = "2012",
  isbn      = "9787040362466",
  pages     = "166--182",
  booktitle = "Scalable Semantic Data Processing",
  publisher = "Higher Education Press",
}


@inbook{7d115cf06848456f8a32bf4b9abee780,
  title     = "Semantic Technology and its Applications in Life Science",
  author    = "Z. Huang",
  year      = "2012",
  isbn      = "9787308101295",
  pages     = "344--365",
  editor    = "Jinshong Li and Z Huang",
  booktitle = "Semantic Technology for Biomedical Science",
  publisher = "Zhejiang University Press",
}


@book{5081d9bf0e7a46b0a88027c7dbe328b6,
  title     = "Semantic Technology for Biomedical Science",
  author    = "Jinshong Li and Z. Huang",
  year      = "2012",
  isbn      = "9787308101295",
  publisher = "Zhejiang University Press",
}


@inbook{8428d6b2421e4040a3a695fb33c3dfe9,
  title     = "Semantic Web in a Constrained Environment",
  abstract  = "The semantic web is intrinsically constrained by its environment. These constraints act as a bottlenecks and limit the performance of applications in various ways. Examples of such constraints are the limited availability of memory, disk space, or a limited network bandwidth.But how do these bounds inﬂuence Semantic Web applications? In this paper we propose to study the Semantic Web as part of a constrained environment. We discuss a framework where applications adapt to the constraints in its environment.",
  author    = "L.J. Rietveld and K.S. Schlobach",
  year      = "2012",
  booktitle = "Downscaling the Semantic Web",
}


@inbook{dd3a6156f02d4095b9da53ce1ba646a8,
  title     = "SemanticXO : connecting the XO with the World's largest information network",
  author    = "C.D.M. Gueret and K.S. Schlobach",
  year      = "2012",
  booktitle = "Proceedings of the First International Conference on e-Technologies and Networks for Development (ICeND2011)",
}


@inbook{8acd0d2b6ccd4d4eba78b442cb8119cd,
  title     = "Study of Ontology Debugging Approaches based on the Criterion Set BLUEI2CI,",
  author    = "Q. Ji and Z. Gao and Z. Huang",
  year      = "2012",
  booktitle = "Proceedings of the Joint Conference of the 6th Chinese Semantic Web Symposium and the 1st Chinese Web Science Conference (CSWS&CWSC)",
}


@article{8b60ca2b80794795a9a94cdbe03fd165,
  title     = "Subjective Logic Extensions for the Semantic Web",
  author    = "D. Ceolin and A. Nottamkandath and W.J. Fokkink",
  note      = "Proceedings title: Proceedings of the 8th International Workshop on Uncertainty Reasoning for the Semantic Web (URSW 2012) Publisher: CEUR-WS.org Place of publication: Web Editors: F. Bobillo, K. J. Laskey, T. Lukasiewicz, T. Martin, M. Nickles, M. Pool, R. Carvalho, P. C. G. da Costa, N. Fanizzi, K. B. Laskey",
  year      = "2012",
  volume    = "900",
  pages     = "27--38",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@article{c155d88268804291abe6b2702e6d5e54,
  title     = "The Altmetrics Collection",
  author    = "J Priem and P.T. Groth and D Taraborell",
  year      = "2012",
  doi       = "10.1371/journal.pone.0048753",
  volume    = "7",
  journal   = "PLoS One",
  issn      = "1932-6203",
  publisher = "Public Library of Science",
  number    = "11",
}


@article{44602747d9234130bed2a26f5c531e9b,
  title     = "The linked data benchmark council (LDBC)",
  author    = "Irini Fundulaki and Pey, {Josep Larriba} and David Dominguez-Sal and Ioan Toma and Dieter Fensel and Barry Bishop and Thomas Neumann and Orri Erling and Peter Neubauer and Paul Groth and {Van Harmelen}, Frank and Peter Boncz",
  year      = "2012",
  volume    = "877",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@inbook{b05eea47a87b4e1394dfb272205ee5c8,
  title     = "The Pharmacology Workspace: A Platform for Drug Discovery",
  author    = "A.J.G Gray and S Askjaer and C Brenninkmeijer and K. Burger and C. Chichester and J Eales and C.T. Evelo and C.A. Goble and P.T. Groth and L Harland and A. Loizou and S Pettifier and R Ramgolam and M. Thompson and A Waagmeester and A.J. Williams",
  year      = "2012",
  booktitle = "3rd International Conference on Biomedical Ontology",
  publisher = "CEUR Proceedings",
}


@inbook{5d0585225c88467393cdae3475843c3a,
  title     = "The Reproducibility of CLIF, a Method for Clinical Quality Indicator Formalisation",
  author    = "K. Dentler and R. Cornet and {ten Teije}, A. and K. Tytgat and J. Klinkenbijl and {De Keizer}, N.",
  year      = "2012",
  pages     = "113--117",
  editor    = "J. Mantas",
  booktitle = "Studies in health technology and informatics",
  publisher = "IOS Press",
}


@article{7e58a5f710a241c5aceed5980e327032,
  title     = "The Reproducibility of CLIF, a Method for Clinical Quality Indicator Formalisation",
  abstract  = "In order to be able to automatically calculate clinical quality indicators, we have proposed CLIF, a stepwise method for clinical quality indicator formalisation. Quality indicators are used for external accountability and hospital comparison. As clinical quality indicators are computed in a decentralised manner by the hospitals themselves, reproducibility of the formalisation method is essential to ensure the comparability of calculated values. Thus, we performed a case study to investigate the reproducibility of CLIF. Eight participants formalised the same sample quality indicator with the help of a web-based indicator-authoring tool that facilitates the application of CLIF. We analysed the results per step and concluded that the method itself leads to reproducible results. To further improve reproducibility, ambiguities in the indicator text must be clarified and trained experts are needed to encode clinical concepts and to specify the relations between concepts. © 2012 European Federation for Medical Informatics and IOS Press. All rights reserved.",
  author    = "K. Dentler and R. Cornet and {ten Teije}, A.C.M. and K. Tytgat and J. Klinkenbijl and {de Keizer}, N.",
  note      = "Publisher: IOS Press",
  year      = "2012",
  doi       = "10.3233/978-1-61499-101-4-113",
  pages     = "113--117",
  journal   = "Studies in Health Technology and Informatics",
  issn      = "0926-9630",
  publisher = "IOS Press",
}


@inbook{cbb45acfcf464a86a9045c699df24461,
  title     = "Towards a Unifying Approach to Representing and Querying Temporal Data in Description Logics",
  author    = "S. Klarman and V. Gutierrez-Basulto",
  year      = "2012",
  booktitle = "Proc. of Web Reasoning and Rule Systems (RR 2012)",
}


@inbook{9eddd832b19747448edf0eaedee69815,
  title     = "Towards Reconstructing the Provenance of Clinical Guidelines",
  author    = "S. Magliacane and P. Groth",
  year      = "2012",
  booktitle = "Proceedings of the 5th International Workshop on Semantic Web Applications and Tools for Life Sciences (SWAT4LS)",
}


@inbook{8d5c96518de044ed864cddaf0de45ee3,
  title     = "Towards the Automated Calculation of Clinical Quality Indicators",
  author    = "K. Dentler and {ten Teije}, A.C.M.",
  year      = "2012",
  pages     = "51--64",
  editor    = "R. David and {ten Teije}, A. and S. Miksch",
  booktitle = "Knowledge Representation for Health-Care; Aime 2011 workshop KR4HC 2011",
  publisher = "Springer",
}


@article{74e6c4d757484b72bbe7878623d59129,
  title     = "Trust Evaluation through User Reputation and Provenance Analysis",
  author    = "D. Ceolin and P. Groth and {van Hage}, W.R. and A. Nottamkandath and W.J. Fokkink",
  note      = "Proceedings title: Proceedings of the 8th International Workshop on Uncertainty Reasoning for the Semantic Web (URSW 2012) Publisher: CEUR-WS.org Place of publication: Web Editors: F. Bobillo, K. B. Laskey, K. J. Laskey, T. Lukasiewicz, T. Martin, M. Nickles, M. Pool, R. Carvalho, P. C. G. da Costa, N. Fanizzi",
  year      = "2012",
  volume    = "900",
  pages     = "15--26",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@inbook{1ca98b5391d940fe871c31f1d8a8292d,
  title     = "Uncovering impacts: a case study in using altmetrics tools",
  author    = "J Priem and C Parra and H Piwowar and P.T. Groth and A Waagmeester",
  year      = "2012",
  booktitle = "Proceedings of the 2nd Workshop on Semantic Publishing (SePublica 2012)",
  publisher = "CEUR Workshop Proceedings",
}


@inbook{3f4bc5f68c3b4ec7800ec3c919c5239a,
  title     = "Urban Computing and Road Sign Management",
  author    = "Z. Huang",
  year      = "2012",
  isbn      = "9787040362466",
  pages     = "231--247",
  booktitle = "Scalable Semantic Data Processing",
  publisher = "Higher Education Press",
}


@inbook{b1bede8a0a8e4365b0287c4dc360c24f,
  title     = "Vectorwise: a Vectorized Analytical DBMS",
  author    = "P.A. Boncz and M. Zukowski and {Van der Wiel}, M.",
  year      = "2012",
  booktitle = "Proc. IEEE International Conference on Data Engineering (ICDE)",
}


@article{0f3d11033f4d4b93b33e3cc80db43e98,
  title   = "Vectorwise: Beyond Column Stores",
  author  = "P.A. Boncz and M. Zukowski",
  year    = "2012",
  volume  = "35",
  pages   = "21--27",
  journal = "IEEE Data Engineering Bulletin",
  issn    = "1053-1238",
  number  = "1",
}


@article{34d46a307ff4440a8fb5efa26a3fb321,
  title    = "WebPIE: A web-scale parallel inference engine using MapReduce",
  abstract = "The large amount of Semantic Web data and its fast growth pose a significant computational challenge in performing efficient and scalable reasoning. On a large scale, the resources of single machines are no longer sufficient and we are required to distribute the process to improve performance. The article that we attach to our submission [1] tackles this problem proposing a methodology to perform inference materializing every possible consequence using the MapReduce programming model. We introduce a number of optimizations to address the issues that a naive implementation would raise and to improve the overall performance. We have implemented the presented techniques in a prototype called WebPIE and the evaluation shows that our approach is able to perform complex inference based on the OWL language over a very large input of about 100 billion triples. To the best of our knowledge, it is the only approach that demonstrates complex inference over an input of a hundred billion of triples.",
  author   = "Jacopo Urbani and Spyros Kotoulas and Jason Maassen and {Van Harmelen}, Frank and Henri Bal",
  year     = "2012",
  journal  = "Belgian/Netherlands Artificial Intelligence Conference",
  issn     = "1568-7805",
}


@inbook{2b52deb0aa844fd6a573ffa03b68a686,
  title     = "A Cognitive Science Perspective on Legal Ontologies",
  author    = "J.A. Breuker and R.J. Hoekstra",
  year      = "2011",
  series    = "Law, Governance and Technology",
  publisher = "Springer",
  number    = "1",
  pages     = "69--81",
  editor    = "G. Sartor and P. Casanovas and M. Biasiotti and M. Fernandez-Barrera",
  booktitle = "Approaches to Legal Ontologies, Theories, Domains, Methodologies",
}


@inbook{ce7b6c8a59d44805a6e093de3ba580ee,
  title     = "A data model for cross-domain data representation: The “Europeana Data Model” in the case of archival and museum data.",
  author    = "S. Hennicke and M. Olensky and {de Boer}, V. and A.H.J.C.A. Isaac and J. Wielemaker",
  year      = "2011",
  booktitle = "12th International Symposium on Information Science",
}


@inbook{2d6cb051d7ef4771b216f103235f613d,
  title     = "A Matrix Approach to Implicit Relationship Finding in Large-scale Knowledge Base",
  author    = "Y. Wang and Y. Zeng and N. Zhong and Z. Huang",
  year      = "2011",
  booktitle = "Proceedings of the 7th International Conference on Semantics, Knowledge and Grids (SKG2011),",
  publisher = "Springer",
}


@inbook{d9b82a785f9747408913b789dac6b852,
  title     = "A Method of Contrastive Reasoning with Inconsistent Ontologies",
  author    = "J. Fang and Z. Huang and {van Harmelen}, F.",
  year      = "2011",
  booktitle = "Proceedings of Joint International Semantic Technology Conference (JIST2011)",
  publisher = "Springer",
}


@inbook{0d502ebee0754406b601116aaf892665,
  title     = "An Efficient Approach to Debugging Ontologies based on Patterns",
  author    = "Qiu Ji and Z. Gao and Z. Huang and M. Zhu",
  year      = "2011",
  booktitle = "Proceedings of Joint International Semantic Technology Conference (JIST2011)",
  publisher = "Springer",
}


@inbook{6f7a3d1e56204e3fa5abf4f71befe0dd,
  title     = "Careflow planning: from time-annotated clinical guidelines to temporal hierarchical task networks",
  author    = "{ten Gonzalez-Ferrer}, A. and {ten Teije}, A.C.M. and J. Fdez-Olivares and K. Milian",
  year      = "2011",
  pages     = "265--275",
  booktitle = "The 13th European Conference on Artificial Intelligence in Medicine",
  publisher = "Springer",
}


@inbook{26372a47fbb442be9aeaa6d2fe04f1ea,
  title     = "Case Frames as Contextual Mappings to Case Law in BestPortal",
  author    = "R.J. Hoekstra and A.R. Lodder and {van Harmelen}, F.A.H.",
  year      = "2011",
  pages     = "393--394",
  editor    = "{De Causmaecker}, P. and J. Maervoet and T. Messelis and K. Verbeeck and T. Vermeulen",
  booktitle = "Proceedings of the 23rd Benelux Conference on Artificial Intelligence",
  publisher = "KAHO Sint-Lieven",
}


@misc{d8784ff19f7e4985b1f0ccdaa0c118f5,
  title  = "Classification of clinical trial eligibility criteria to support semantic linkage of research and clinical care data",
  author = "K. Milian and A. Bucur and {ten Teije}, A. and F. Harmelen",
  note   = "Publisher: AMIA Place of publication: Washington",
  year   = "2011",
}


@article{c28232c9ec1540a09e1e701082f8ca3b,
  title     = "Comparison of reasoners for large ontologies in the OWL 2 EL profile",
  abstract  = "This paper provides a survey to and a comparison of state-of-the-art Semantic Web reasoners that succeed in classifying large ontologies expressed in the tractable OWL 2 EL profile. Reasoners are characterized along several dimensions: The first dimension comprises underlying reasoning characteristics, such as the employed reasoning method and its correctness as well as the expressivity and worst-case computational complexity of its supported language and whether the reasoner supports incremental classification, rules, justifications for inconsistent concepts and ABox reasoning tasks. The second dimension is practical usability: whether the reasoner implements the OWL API and can be used via OWLlink, whether it is available as Protégé plugin, on which platforms it runs, whether its source is open or closed and which license it comes with. The last dimension contains performance indicators that can be evaluated empirically, such as classification, concept satisfiability, subsumption checking and consistency checking performance as well as required heap space and practical correctness, which is determined by comparing the computed concept hierarchies with each other. For the very large ontology SNOMED CT, which is released both in stated and inferred form, we test whether the computed concept hierarchies are correct by comparing them to the inferred form of the official distribution. The reasoners are categorized along the defined characteristics and benchmarked against well-known biomedical ontologies. The main conclusion from this study is that reasoners vary significantly with regard to all included characteristics, and therefore a critical assessment and evaluation of requirements is needed before selecting a reasoner for a real-life application. © 2011 - IOS Press and the authors. All rights reserved.",
  author    = "K. Dentler and R. Cornet and {ten Teije}, A.C.M. and {de Keizer}, N.F.",
  year      = "2011",
  doi       = "10.3233/SW-2011-0034",
  volume    = "2",
  pages     = "71--87",
  journal   = "Semantic Web",
  issn      = "1570-0844",
  publisher = "IOS Press",
  number    = "2",
}


@article{c358bd4277b145c48fe832a60e945ec2,
  title     = "Concept drift and how to identify it",
  abstract  = "This paper studies concept drift over time. We first define the meaning of a concept in terms of intension, extension and label. Then we study concept drift over time using two theories: one based on concept identity and one based on concept morphing. A qualitative toolkit for analysing concept drift is proposed to detect concept shift and stability when concept identity is available, and concept split and strength of morphing chain if using the morphing theory. We apply our framework in four case-studies: a political vocabulary in SKOS, the DBpedia ontology in RDFS, the LKIF-Core ontology in OWL and a few biomedical ontologies in OBO. We describe ways of identifying interesting changes in the meaning of concept within given application contexts. These case-studies illustrate the feasibility of our framework in analysing concept drift in knowledge organisation schemas of varying expressiveness. © 2011 Elsevier B.V. All rights reserved.",
  author    = "S. Wang and K.S. Schlobach and M.C.A. Klein",
  year      = "2011",
  doi       = "10.1016/j.websem.2011.05.003",
  volume    = "9",
  pages     = "247--265",
  journal   = "Journal of Web Semantics",
  issn      = "1570-8268",
  publisher = "Elsevier",
  number    = "3",
}


@inbook{870714b34b7e4a09a783958d1d7129ed,
  title     = "Contrastive Reasoning with Inconsistent Ontologies,",
  author    = "J. Fang and Z. Huang and {van Harmelen}, F.",
  year      = "2011",
  booktitle = "Proceedings of 2011 IEEE/WIC/ACM International Conference on Web Intelligence (WI-11)",
}


@book{6f9f09625cfa430887d4921aeda4e4a0,
  title     = "Diffusion d'informations sur un réseau P2P",
  author    = "C.D.M. Gueret",
  year      = "2011",
  publisher = "Editions Universitaires Européennes",
}


@article{634fbdc29fbd44dfa1f7dfa868005e9b,
  title     = "Estimating uncertainty of categorical Web data",
  author    = "D. Ceolin and {van Hage}, W.R. and W.J. Fokkink and A.Th. Schreiber",
  note      = "Proceedings title: Proceedings of the 7th International Workshop on Uncertainty Reasoning for the Semantic Web (URSW 2011) Publisher: CEUR-WS.org Place of publication: Web Editors: F. Bobillo, R. Carvalho, P. C. G. da Costa, C. D'Amato, M. Fanizzi, K. B. Laskey, K. J. Laskey, T. Lukasiewicz, T. Martin, T. Nickles, M. Pool",
  year      = "2011",
  volume    = "778",
  pages     = "15--26",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@inbook{80c2e23da4134deb83fa68924b4e5e75,
  title     = "Finding the Achilles Heel of the Web of Data using network analysis tools",
  author    = "C.D.M. Gueret and P.T. Groth and {van Harmelen}, F.A.H. and K.S. Schlobach",
  year      = "2011",
  booktitle = "The 23rd Benelux Conference on Artificial Intelligence (BNAIC 2011)",
}


@article{8e203ce30eef44d8b856fb6be2a52ce2,
  title     = "Integration of VectorWise with Ingres",
  author    = "D. Inkster and P.A. Boncz and M. Zukowski",
  year      = "2011",
  doi       = "10.1145/2070736.2070747",
  volume    = "40",
  journal   = "ACM SIGMOD Record",
  issn      = "0163-5808",
  publisher = "Association for Computing Machinery (ACM)",
  number    = "3",
}


@inbook{047727a958e548368f33e6c772a30d0b,
  title     = "Interest Logic and Its Application on the Web",
  author    = "Y. Zeng and Z. Huang and F. Liu and X. Ren and N. Zhong",
  year      = "2011",
  booktitle = "Proceedings of the 5th International Conference on Knowledge Science, Engineering, and Management (KSEM 2011)",
  publisher = "Springer",
}


@inbook{45a00c4388ad4aeaa219f1895870704e,
  title     = "Interleaving Reasoning and Selection with Knowledge Summarization",
  author    = "Y. Wang and Z. Huang and Y. Zeng and N. Zhong",
  year      = "2011",
  booktitle = "Proceedings of the 7th International Conference on Semantics, Knowledge and Grids (SKG2011),",
  publisher = "Springer",
}


@inbook{cfdea36ed5d64f14b3dcbccd3a13bb52,
  title     = "Is data sharing the privilege of a few ? Bringing Linked Data to those without the Web",
  author    = "C.D.M. Gueret and K.S. Schlobach and {de Boer}, V. and A.P.M. Bon and H. Akkermans",
  year      = "2011",
  booktitle = "Proceedings of ISWC2011 - {"}Outrageous ideas{"} track",
}


@misc{a2cbec6a47bd4ed49a3624f2daa969b7,
  title  = "Is data sharing the privilege of a few? Bringing Linked Data to those without the Web",
  author = "C.D.M. Guéret and K.S. Schlobach and {de Boer}, V. and A. Bon and J.M. Akkermans",
  note   = "Jury award winning paper 1st place Proceedings title: ISWC 2011 - Outrageous Ideas Track",
  year   = "2011",
}


@inbook{7e9dd006160747aa8af8896295057bdb,
  title     = "Knowledge Engineering Rediscovered: Towards Reasoning Patterns for the Semantic Web",
  author    = "{van Harmelen}, F.A.H. and {ten Teije}, A. and H. Wache",
  year      = "2011",
  isbn      = "9783642197963",
  pages     = "57--75",
  editor    = "D Fensel",
  booktitle = "Foundations for the Web of Information and Services - A Review of 20 Years of Semantic Web Research",
  publisher = "Springer Verlag",
}


@book{b6821d48dbda4a9c8182f553b2b527df,
  title     = "Knowledge Representation for Health-Care. Data, Processes and Guidelines (ECAI 2010 Workshop KR4HC 2010), Revised Selected Papers",
  author    = "D Riano and {ten Teije}, A.C.M. and S Miksch and M Peleg",
  year      = "2011",
  series    = "Lecture Notes in Computer Science",
  publisher = "Springer",
  number    = "6512",
}


@inbook{761f2b63531f48879f5696ec529fdecc,
  title     = "KR and Reasoning on the Semantic Web: Web-Scale Reasoning",
  abstract  = "Reasoning is a key element of the Semantic Web. For the Semantic Web to scale, it is required that reasoning also scales. This chapter focuses on two approaches to achieve this: The first deals with increasing the computational power available for a given task by harnessing distributed resources. These distributed resources refer to peer-to-peer networks, federated data stores, or cluster-based computing. The second deals with containing the set of axioms that need to be considered for a given task. This can be achieved by using intelligent selection strategies and limiting the scope of statements. The former is exemplified by methods substituting expensive web-scale reasoning with the cheaper application of heuristics while the latter by methods to control the quality of the provided axioms. Finally, future issues concerning information centralization and logics vs information retrieval-based methods, metrics, and benchmarking are considered.",
  author    = "Spyros Kotoulas and {van Harmelen}, Frank and Jesse Weaver",
  year      = "2011",
  doi       = "10.1007/978-3-540-92913-0_11",
  isbn      = "978-3-540-92912-3",
  series    = "Handbook of Semantic Web Technologies",
  pages     = "442--466",
  booktitle = "Handbook of Semantic Web Technologies",
}


@article{f6281e3513934e2a9e6fc7349fab3a3a,
  title     = "Learning Belief Connections in a Model for Situation Awareness",
  author    = "M.L. Gini and M. Hoogendoorn and {van Lambalgen}, R.M.",
  note      = "Proceedings title: Agents in Principle, Agents in Practice Publisher: Springer Verlag Editors: D. Kinny, D. Hsu, G. Governatori, A. Ghose",
  year      = "2011",
  volume    = "7047",
  pages     = "373--384",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@article{260aea1a6d2f4a229e918c7f80719ac4,
  title     = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics): Preface",
  author    = "David Riaño and {Ten Teije}, Annette and Silvia Miksch and Mor Peleg",
  year      = "2011",
  doi       = "10.1007/978-3-642-18050-7",
  volume    = "6512 LNAI",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@inbook{3a001315819a4c3cb5b89b486a9b7d45,
  title     = "Linked Data for Network Science",
  author    = "P.T. Groth and Y Gil",
  year      = "2011",
  editor    = "T. Kauppinen and L.C. Pouchard and C. Keßler",
  booktitle = "Proceedings of the First International Workshop on Linked Science 2011",
  publisher = "CEUR Workshop Proceedings",
}


@inbook{294d8978c7a04bb6af5f6ef414626e6d,
  title     = "LinkedDataLens: Linked Data as a Network of Networks",
  abstract  = "With billions of assertions and counting, the Web of Data represents the largest multi-contributor interlinked knowledge base that ever existed. We present a novel framework for analyzing and using the Web of Data based on extracting and analyzing thematic subsets of it. We view the Web of Data as a {"}network of networks{"} from which to extract meaningful subsets that can be converted them into self-contained networks to be further analyzed and reused. These extracted networks can then be analyzed through network analysis and discovery algorithms, and the results of these analyses can be published back on the Web of Data. We describe LinkedDataLens, an implementation of this framework that uses the Wings workflow system to represent multi-step network extraction and analysis processes. © 2011 Authors.",
  author    = "Y Gil and P.T. Groth",
  year      = "2011",
  doi       = "10.1145/1999676.1999720",
  isbn      = "978-1-4503-0396-8",
  pages     = "191--192",
  editor    = "M.A. Musen and Ó Corcho",
  booktitle = "Proceedings of the 6th International Conference on Knowledge Capture (K-CAP 2011)",
  publisher = "ACM",
}


@article{b8aa5a41727340e0830c4d3bdc1c4b7f,
  title    = "Multi-Scale Analysis of the Web of Data: a Challenge to the Complex System's Community",
  abstract = "The Web of Data (WoD) is an Internet-based network of data resources and their relations. It has recently taken flight and combines over a hundred interlinked data sources with more than 15 billion edges. A consequence of this recent success is that a paradigm shift has taken place: up to now the Web of Data could be studied, searched and maintained like a classical database; nowadays it has turned into a Complex System and needs to be studied as such. In this paper, we introduce the Web of Data as a challenging object of study and provide initial results on two network scales: the pure data-layer, and the global connection between groups data items. In this analysis, we show that the {"}official{"} abstract representation of the WoD does not fit the real distribution we derive from the lower scale. As interesting as these results are, bigger challenges for analysis await in the form of the highly dynamic character of the WoD, and the typed, and implicit, character of the edges which is, to the best of our knowledge, hitherto unstudied. © 2011 World Scientific Publishing Company.",
  author   = "C.D.M. Gueret and S. Wang and P.T. Groth and K.S. Schlobach",
  year     = "2011",
  doi      = "10.1142/S0219525911003153",
  volume   = "14",
  pages    = "587--609",
  journal  = "Advances in Complex Systems",
  issn     = "1294-8535",
  number   = "4",
}


@inbook{073256f42c8b4c91bcbb1bb59d8e42d4,
  title     = "oisy Semantic Data Processing in Seoul Road Sign Management System",
  author    = "Z. Huang and J. Fang and S Park and Tony Lee",
  year      = "2011",
  booktitle = "Proceedings of 2011 International Semantic Web Conference (ISWC2011)",
  publisher = "Springer",
}


@inbook{75ae4e2c28864fa5860a36f73bb53fff,
  title     = "Ontology Learning from Noisy Linked Data,",
  author    = "M. Zhu and Z. Gao and Z. Huang",
  year      = "2011",
  booktitle = "Proceedings of 2011 International Semantic Web Conference (ISWC2011)",
  publisher = "Springer",
}


@article{920b39b1d0104fcd8cadc165b775a17f,
  title     = "Performance Measures to enable Agent-Based Support in Demanding Circumstances",
  author    = "F. Both and M. Hoogendoorn and {van Lambalgen}, R.M. and R. Oorburg and {de Vos}, M.",
  note      = "Proceedings title: Foundations of Augmented Cognition. Directing the Future of Adaptive Systems Publisher: Springer Verlag Editors: D.D. Schmorrow, C.M. Fidopiastis",
  year      = "2011",
  volume    = "6780",
  pages     = "578--587",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@inbook{369c37b168844b9896cf5c533696c031,
  title     = "QueryPIE: Backward reasoning for OWL Horst over very large knowledge bases",
  author    = "J. Urbani and {van Harmelen}, F.A.H. and S. Schlobach and H.E. Bal",
  year      = "2011",
  booktitle = "10th Int. Semantic Web Conference (ISWC 2011)",
}


@inbook{1b72358e72524b75a5362548bf8c6089,
  title     = "Reasoning with Noisy Semantic Data",
  author    = "J. Qiu and Z. Gao and Z. Huang",
  year      = "2011",
  booktitle = "Proceeding of 8th Extended Semantic Web Conference (ESWC2011)",
  publisher = "Springer",
}


@article{337100b66eb84f6cb0a1918a68b3a104,
  title     = "Representing distributed systems using the Open Provenance Model",
  abstract  = "From the World Wide Web to supply chains and scientific simulations, distributed systems are a widely used and important approach to building computational systems. Tracking provenance within these systems is crucial for determining the trustworthiness of data they produce, troubleshooting problems, assigning responsibility for decisions, and improving performance. To facilitate such tracking, the Open Provenance Model (OPM) has been created to enable the interchange of provenance between a distributed system's components. However, to date, the ability of OPM to represent distributed systems has not been verified. In this work, we show how OPM can be used to represent a set of distributed systems' patterns. We present a profile that shows that these patterns are a specialisation of OPM. Finally, we define a contract that enables participants in a distributed system to ensure that their provenance can be integrated cohesively. © 2011 Elsevier B.V. All rights reserved.",
  author    = "P.T. Groth and L Moreau",
  year      = "2011",
  doi       = "10.1016/j.future.2010.10.001",
  volume    = "27",
  pages     = "757--765",
  journal   = "Future Generation Computer Systems",
  issn      = "0167-739X",
  publisher = "Elsevier",
  number    = "6",
}


@article{a34146e228e4413d9085c32a709e35ef,
  title     = "Research interests: their dynamics, structures and applications in unifying search and reasoning",
  abstract  = "Most scientific publication information, which may reflects scientists' research interests, is publicly available on the Web. Understanding the characteristics of research interests from previous publications may help to provide better services for scientists in the Web age. In this paper, we introduce some parameters to track the evolution process of research interests, we analyze their structural and dynamic characteristics. According to the observed characteristics of research interests, under the framework of unifying search and reasoning (ReaSearch), we propose interests-based unification of search and reasoning (I-ReaSearch). Under the proposed I-ReaSearch method, we illustrate how research interests can be used to improve literature search on the Web. According to the relationship between an author's own interests and his/her co-authors interests, social group interests are also used to refine the literature search process. Evaluation from both the user satisfaction and the scalability point of view show that the proposed I-ReaSearch method provides a user centered and practical way to problem solving on the Web. The efforts provide some hints and various methods to support personalized search, and can be considered as a step forward user centric knowledge retrieval on the Web. From the standpoint of the Active Media Technology (AMT) on the Wisdom Web, in this paper, the study on the characteristics of research interests is based on complex networks and human dynamics, which can be considered as an effort towards utilizing information physics to discover and explain the phenomena related to research interests of scientists. The application of research interests aims at providing scientific researchers best means and best ends in an active way for literature search on the Web. © 2010 Springer Science+Business Media, LLC.",
  author    = "Y. Zeng and E. Zhou and Y. Wang and X. Ren and Y. Qin and Z. Huang and N. Zhong",
  year      = "2011",
  doi       = "10.1007/s10844-010-0144-1",
  volume    = "37",
  pages     = "65--88",
  journal   = "Journal of Intelligent Information Systems",
  issn      = "0925-9902",
  publisher = "Springer Netherlands",
  number    = "1",
}


@inbook{87c998a3c6be4414b6209836d13550cc,
  title     = "SciBORQ: Scientific data management with Bounds On Runtime and Quality",
  author    = "L. Sidirourgos and P.A. Boncz and M.L. Kersten",
  year      = "2011",
  booktitle = "Proceedings of the biennial Conference on Innovative Data Systems Research",
}


@article{9c60ba7fc2a644829aece5398960c028,
  title     = "Special Section: The third provenance challenge on using the open provenance model for interoperability",
  abstract  = "The third provenance challenge was organized to evaluate the efficacy of the Open Provenance Model (OPM) in representing and sharing provenance with the goal of improving the specification. A data loading scientific workflow that ingests data files into a relational database for the Pan-STARRS sky survey project was selected as a candidate for collecting provenance. Challenge participants record provenance, run queries over it, and export/import provenance as OPM documents with other teams to verify interoperability. Fourteen teams participated in the challenge that concluded at a workshop in June 2009 in Amsterdam. The experiences of several participating teams are included in this special issue. In this editorial, we describe the challenge in detail, review its outcome, and introduce articles included in this special issue. © 2011 Elsevier B.V. All rights reserved.",
  author    = "Y Simmhan and P.T. Groth and L Moreau",
  year      = "2011",
  doi       = "10.1016/j.future.2010.11.020",
  volume    = "27",
  pages     = "737--742",
  journal   = "Future Generation Computer Systems",
  issn      = "0167-739X",
  publisher = "Elsevier",
  number    = "6",
}


@article{42b1bb74697748b9b0848e9b23147625,
  title     = "The Meaningful Use of Big Data: Four Perspectives - Four Challenges",
  author    = "C. Bizer and P.A. Boncz and E.L. Brodie and O. Erling",
  year      = "2011",
  doi       = "10.1145/2094114.2094129",
  volume    = "40",
  journal   = "ACM SIGMOD Record",
  issn      = "0163-5808",
  publisher = "Association for Computing Machinery (ACM)",
  number    = "4",
}


@article{11aa60e3935c46eda07822e577bc6ec1,
  title     = "The MetaLex Document Server - Legal Documents as Versioned Linked Data",
  author    = "R.J. Hoekstra",
  note      = "Proceedings title: The Semantic Web – ISWC 2011 Publisher: Springer Place of publication: Bonn Editors: L Aroyo, C Welty, H Alani, J Taylor, A Bernstein, L Kagal, N Noy, E Blomqvist",
  year      = "2011",
  doi       = "10.1007/978-3-642-25093-4_9",
  volume    = "7032",
  pages     = "128--143",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@article{740b3fb51bbb41328e0f280787c11ec6,
  title     = "The Open Provenance Model core specification (v1.1)",
  abstract  = "The Open Provenance Model is a model of provenance that is designed to meet the following requirements: (1) Allow provenance information to be exchanged between systems, by means of a compatibility layer based on a shared provenance model. (2) Allow developers to build and share tools that operate on such a provenance model. (3) Define provenance in a precise, technology-agnostic manner. (4) Support a digital representation of provenance for any {"}thing{"}, whether produced by computer systems or not. (5) Allow multiple levels of description to coexist. (6) Define a core set of rules that identify the valid inferences that can be made on provenance representation. This document contains the specification of the Open Provenance Model (v1.1) resulting from a community effort to achieve inter-operability in the Provenance Challenge series. © 2011 Elsevier B.V. All rights reserved.",
  author    = "L Moreau and B Clifford and J Freire and J Futrelle and Y Gill and P.T. Groth and N Kwasnikowska and S Miles and P Missier and J Myers and B Plale and Y Simmhan and E Stephan and {Van den Bussche}, J",
  year      = "2011",
  doi       = "10.1016/j.future.2010.07.005",
  volume    = "27",
  pages     = "743--756",
  journal   = "Future Generation Computer Systems",
  issn      = "0167-739X",
  publisher = "Elsevier",
  number    = "6",
}


@article{eb055c10bfe04551ad01d910be58b4b5,
  title     = "The value of data",
  abstract  = "Data citation and the derivation of semantic constructs directly from datasets have now both found their place in scientific communication. The social challenge facing us is to maintain the value of traditional narrative publications and their relationship to the datasets they report upon while at the same time developing appropriate metrics for citation of data and data constructs. © 2011 Nature America, Inc. All rights reserved.",
  author    = "B Mons and {van Haagen}, H. and C. Chichester and {'t Hoen}, P. and {den Dunnen}, J.T. and {van Ommen}, G and {van Mulligen}, E. and B. Singh and R. Hooft and M. Roos and J. Hammond and B. Kiesel and B. Giardine and J Velterop and P.T. Groth",
  year      = "2011",
  doi       = "10.1038/ng0411-281",
  volume    = "43",
  pages     = "281--283",
  journal   = "Nature Genetics",
  issn      = "1061-4036",
  publisher = "Nature Publishing Group",
}


@inbook{90f3f7ed70d8492fac789d5902138009,
  title     = "Top-k reasoning for the Semantic Web",
  author    = "K.S. Schlobach",
  year      = "2011",
  booktitle = "FIRST INTERNATIONAL WORKSHOP ON ORDERING AND REASONING",
}


@inbook{356a0ea1303e4e7f83bc87b5aaeab1b7,
  title     = "TripleCloud: An Infrastructure for Exploratory Querying over Web-Scale RDF Data",
  abstract  = "As the availability of large scale RDF data sets has grown, there has been a corresponding growth in researchers' and practitioners' interest in analyzing and investigating these data sets. However, given their size and messiness, there is significant overhead in setting up the infrastructure to store and query them. In this paper, we present TripleCloud, a system that aims to lower the entry cost to exploring Web-scale RDF data sets. The system takes advantage of existing cloud based key-value stores (e.g. BigTable, HBase) to both enable scalability as well as hide the complexities of infrastructure deployment and maintenance. It layers over these key-value stores a robust query engine able to return approximate answers. We test the scalability of the approach scaling to over 3 billion triples for complex queries. In addition to an implementation over HBase, TripleCloud runs over the Google App Engine, allowing us to perform a cost evaluation of the approach. © 2011 IEEE.",
  author    = "C.D.M. Gueret and S. Kotoulas and P.T. Groth",
  year      = "2011",
  doi       = "10.1109/WI-IAT.2011.166",
  booktitle = "IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technolog",
  publisher = "IEEE",
}


@inbook{56b3962acf0148999fc43261fe0c8ff0,
  title     = "Two-dimensional description logics for context-based semantic interoperability",
  author    = "S. Klarman and V. Gutierrez-Basulto",
  year      = "2011",
  booktitle = "Proc. of AAAI-11",
}


@inbook{100220a9d94e462797a1a55bcd09f814,
  title     = "Two-Dimensional Description Logics of Context",
  author    = "S. Klarman and V. Gutierrez-Basulto",
  year      = "2011",
  booktitle = "Proc. of the International Workshop on Description Logics (DL2011)",
}


@article{75d0ad4ba8a64d0c965b2a5938e6fb62,
  title     = "User-centric Query Refinement and Processing Using Granularity Based Strategies",
  abstract  = "Under the context of large-scale scientific literatures, this paper provides a user-centric approach for refining and processing incomplete or vague query based on cognitive- and granularity-based strategies. From the viewpoints of user interests retention and granular information processing, we examine various strategies for user-centric unification of search and reasoning. Inspired by the basic level for human problem-solving in cognitive science, we refine a query based on retained user interests. We bring the multi-level, multi-perspective strategies from human problem-solving to large-scale search and reasoning. The power/exponential law-based interests retention modeling, network statistics-based data selection, and ontology-supervised hierarchical reasoning are developed to implement these strategies. As an illustration, we investigate some case studies based on a large-scale scientific literature dataset, DBLP. The experimental results show that the proposed strategies are potentially effective. © 2010 Springer-Verlag London Limited.",
  author    = "Y. Zeng and N. Zhong and Y. Wang and Y. Qin and Z. Huang and H Zhou and Y Yao and {van Harmelen}, F.A.H.",
  year      = "2011",
  doi       = "10.1007/s10115-010-0298-8",
  volume    = "27",
  pages     = "419--450",
  journal   = "Knowledge and Information Systems",
  issn      = "0219-1377",
  publisher = "Springer London",
  number    = "3",
}


@article{4d5c775411c349ad90a924e932a85359,
  title     = "Using Provenance in the Semantic Web",
  author    = "Y Gil and P.T. Groth",
  year      = "2011",
  doi       = "10.1016/j.websem.2011.05.005",
  volume    = "9",
  pages     = "147--148",
  journal   = "Journal of Web Semantics",
  issn      = "1570-8268",
  publisher = "Elsevier",
  number    = "2",
}


@inbook{ff3a34d8d3584400a84dc17fe2244171,
  title     = "Vectorization vs. compilation in query execution",
  author    = "P.A. Boncz and J. Sompolski and M. Zukowski",
  year      = "2011",
  booktitle = "Proceedings of DAMON Workshop (colocated with ACM SIGMOD)",
}


@article{b5d6fab222b1402fa23b013753ace04e,
  title     = "WebPIE: A Web-scale parallel inference engine using MapReduce",
  author    = "J. Urbani and S. Kotoulas and J. Maassen and {van Harmelen}, F.A.H. and H.E. Bal",
  year      = "2011",
  journal   = "Journal of Web Semantics",
  issn      = "1570-8268",
  publisher = "Elsevier",
}


@article{985671543edc45e1adb02cb2f5543606,
  title     = "Wings: Intelligent Workflow-Based Design of Computational Experiments",
  abstract  = "The Wings intelligent workflow system assists scientists with designing computational experiments by automatically tracking constraints and ruling out invalid designs, letting scientists focus on their experiments and goals. © 2011 IEEE.",
  author    = "Y Gil and V Ratnakar and J. Kim and P.A. Gonzálex-Calero and P.T. Groth and J. Moody and E Deelman",
  year      = "2011",
  doi       = "10.1109/MIS.2010.9",
  volume    = "26",
  pages     = "62--72",
  journal   = "IEEE Intelligent Systems",
  issn      = "1541-1672",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  number    = "1",
}


@article{11589630c9e54c8cb7252a4a3c2e1213,
  title     = "Special issue on commonsense reasoning for the semantic web Preface",
  author    = "{van Harmelen}, Frank and Andreas Herzig and Pascal Hitzler and Guilin Qi",
  year      = "2010",
  month     = "2",
  doi       = "10.1007/s10472-010-9209-7",
  volume    = "58",
  pages     = "1--2",
  journal   = "Annals of Mathematics and Artificial Intelligence",
  issn      = "1012-2443",
  publisher = "Springer Netherlands",
  number    = "1-2",
}


@article{f557113ad6ec4bc99412963bab9f630d,
  title     = "ABox Abduction in the Description Logic ALC",
  abstract  = "Due to the growing popularity of Description Logics-based knowledge representation systems, predominantly in the context of Semantic Web applications, there is a rising demand for tools offering non-standard reasoning services. One particularly interesting form of reasoning, both from the user as well as the ontology engineering perspective, is abduction. In this paper we introduce two novel reasoning calculi for solving ABox abduction problems in the Description Logic ALC, i.e. problems of finding minimal sets of ABox axioms, which when added to the knowledge base enforce entailment of a requested set of assertions. The algorithms are based on regular connection tableaux and resolution with set-of-support and are proven to be sound and complete. We elaborate on a number of technical issues involved and discuss some practical aspects of reasoning with the methods. © 2010 Springer Science+Business Media B.V.",
  author    = "S. Klarman and U. Endriss and K.S. Schlobach",
  year      = "2010",
  doi       = "10.1007/s10817-010-9168-z",
  journal   = "Journal of Automated Reasoning",
  issn      = "0168-7433",
  publisher = "Springer Netherlands",
}


@inbook{5cce2011a9914d4bae046b80c1e502bf,
  title     = "ALC_ALC: a Context Description Logic",
  author    = "S. Klarman and V. Gutierrez-Basulto",
  year      = "2010",
  editor    = "T. Janhunen and I. Niemelä",
  booktitle = "Proceedings of the 12th European Conference on Logics in Artificial Intelligence (JELIA 2010)",
  publisher = "LNCS 6341, Springer",
}


@inbook{6fb9dd26e2ea477f807f494e0d1bf9fd,
  title     = "A nature-inspired model for the dissemination of information in P2P networks",
  author    = "C.D.M. Gueret",
  year      = "2010",
  isbn      = "9781848822283",
  editor    = "A. Abraham and A. Hassanien and V. Snášel",
  booktitle = "Computational Social Network Analysis: Trends, Tools and Research Advances",
  publisher = "Springer",
}


@article{4e25e1cebdc04855bf38e9865b7ac7bb,
  title     = "A reasonable Semantic Web",
  abstract  = "The realization of Semantic Web reasoning is central to substantiating the Semantic Web vision. However, current mainstream research on this topic faces serious challenges, which forces us to question established lines of research and to rethink the underlying approaches. We argue that reasoning for the Semantic Web should be understood as {"}shared inference,{"} which is not necessarily based on deductive methods. Model-theoretic semantics (and sound and complete reasoning based on it) functions as a gold standard, but applications dealing with large-scale and noisy data usually cannot afford the required runtimes. Approximate methods, including deductive ones, but also approaches based on entirely different methods like machine learning or nature-inspired computing need to be investigated, while quality assurance needs to be done in terms of precision and recall values (as in information retrieval) and not necessarily in terms of soundness and completeness of the underlying algorithms.",
  keywords  = "Automated reasoning, Formal semantics, Knowledge representation, Linked Open Data, Semantic Web",
  author    = "Pascal Hitzler and {Van Harmelen}, Frank",
  year      = "2010",
  doi       = "10.3233/SW-2010-0010",
  volume    = "1",
  pages     = "39--44",
  journal   = "Semantic Web",
  issn      = "1570-0844",
  publisher = "IOS Press",
  number    = "1-2",
}


@inbook{84494387efd44b49ae746e8a80d5ba57,
  title     = "A Trust Model to Estimate the Quality of Annotations using the Web",
  author    = "D. Ceolin and {van Hage}, W.R. and W.J. Fokkink",
  year      = "2010",
  booktitle = "Proceedings of the WebSci10: Extending the Frontiers of Society On-Line",
  publisher = "journal.webscience.org",
}


@inbook{201fd4178ba8487fa74831b96d411fcb,
  title     = "A Web-based Repository Service for Vocabularies and Alignments in the Cultural Heritage Domain",
  author    = "{van der Meij}, L. and A.H.J.C.A. Isaac and C. Zinn",
  year      = "2010",
  booktitle = "Proceedings of the 7th Extended Semantic Web Conference (ESWC 2010)",
}


@inbook{fe98236aa398497c952b466badd6df3f,
  title     = "A workbench for anytime reasoning by ontology approximation: With a case study on instance retrieval",
  abstract  = "Reasoning is computationally expensive. This is especially true for reasoning on the Web, where data sets are very large and often described by complex terminologies. One way to reduce this complexity is through the use of approximate reasoning methods which trade one computational property (eg. quality of answers) for others, such as time and memory. Previous research into approximation on the Semantic Web has been rather ad-hoc, and we propose a framework for systematically studying such methods. We developed a workbench which allows the structured combination of different algorithms for approximation, reasoning and measuring in one single framework. As a case-study we investigate an incremental method for instance retrieval through ontology approximation, and we use our workbench to study the computational behaviour of several approximation strategies.",
  keywords  = "Anytime Reasoning, Approximate Reasoning, Description Logics, Ontologies, Semantic Web",
  author    = "Gaston Tagni and Stefan Schlobach and {Ten Teije}, Annette and {Van Harmelen}, Frank and Giorgios Karafotias",
  year      = "2010",
  doi       = "10.3233/978-1-60750-676-8-328",
  isbn      = "9781607506751",
  volume    = "222",
  series    = "Frontiers in Artificial Intelligence and Applications",
  publisher = "IOS Press",
  pages     = "328--340",
  booktitle = "STAIRS 2010 Proceedings of the Fifth Starting AI Researchers' Symposium",
}


@inbook{5d8811ef900540389437ae642bad6f34,
  title     = "Calculating the Trust of Event Descriptions using Provenance",
  author    = "D. Ceolin and P.T. Groth and {van Hage}, W.R.",
  year      = "2010",
  editor    = "S.S. Satia and J. Zhao and P. Missier and J.M. Gomez-Perez",
  booktitle = "Proceedings of the Second International Workshop on the role of Semantic Web in Provenance Management (SWPM 2010)",
  publisher = "CEUR-WS.org",
}


@inbook{fe0bf63b9bbe45b2881141caa701e78f,
  title     = "Case Frames as Contextual Mappings to Case Law in BestPortal",
  author    = "R.J. Hoekstra and A.R. Lodder and {van Harmelen}, F.A.H.",
  year      = "2010",
  isbn      = "9781607506812",
  pages     = "77--86",
  editor    = "R.G.F. Winkels",
  booktitle = "Legal Knowledge and Information Systems - JURIX 2010: The Twenty-Third Annual Conference",
  publisher = "IOS Press",
}


@inbook{37963fc0a17c44709ef4a8923aad2db2,
  title     = "Description Logics for Relative Terminologies",
  author    = "S. Klarman",
  year      = "2010",
  editor    = "T. Icard and R. Muskens",
  booktitle = "ESSLLI 2008/2009 Student Sessions",
  publisher = "LNAI 6211 Springer",
}


@inbook{3ef0523ebbef4a1e94e52b238d5c8eac,
  title     = "Extensional mapping-chains for studying concept drift in political ontologies",
  author    = "S. Wang and J. Takens and {van Atteveldt}, W.H. and J. Kleinnijenhuis and K.S. Schlobach",
  year      = "2010",
  booktitle = "Proceedings of 2010 Conference of the International Communication Association",
}


@article{b12e115ad1d9474caa82e496bdfa0b59,
  title     = "Finding the Achilles Heel of the Web of Data : using network analysis for link-recommendation.",
  author    = "C.D.M. Gueret and P.T. Groth and {van Harmelen}, F.A.H. and S. Schlobach",
  note      = "Proceedings title: The Semantic Web - ISWC 2010 - 9th International Semantic Web Conference, ISWC 2010 Publisher: Springer ISBN: 978-3-642-17745-3 Editors: P.F. Patel-Schneider, Y. Pan, P Hitzler, P Mika, L Zhang, J.Z. Pan, I. Horrocks, B. Glimm",
  year      = "2010",
  volume    = "6496",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@inbook{f213fe4cb7c14367adc6cdb6b00765f2,
  title     = "Historical Event-based Access to Museum Collections",
  author    = "{van den Akker}, C. and L.M. Aroyo and A.K. Cybulska and {van Erp}, M.G.J. and P. Gorgels and L. Hollink and C. Jager and S. Legêne and {van der Meij}, L. and J. Oomen and {van Ossenbruggen}, J. and G. Schreiber and R. Segers and P.T.J.M. Vossen and B. Wielinga",
  year      = "2010",
  isbn      = "0074-624-1",
  pages     = "1--9",
  editor    = "Th. Winkler and A. Artikis and Y. Kompatsiaris and P. Mylonas",
  booktitle = "1st International Workshop {"}EVENTS 2010 - Recognising and tracking events on the Web and in real life{"} {"}(SETN2010)",
  publisher = "CEUR-WS (online)",
}


@inbook{3c6ae4a4742a48c89d720d87edebe3f6,
  title     = "Identifying disease-centric subdomains in very large medical ontologies: A case-study on breast cancer concepts in SNOMED CT. Or: Finding 2500 out of 300.000",
  abstract  = "Modern medical vocabularies can contain up to hundreds of thousands of concepts. In any particular use-case only a small fraction of these will be needed. In this paper we first define two notions of a disease-centric subdomain of a large ontology. We then explore two methods for identifying disease-centric subdomains of such large medical vocabularies. The first method is based on lexically querying the ontology with an iteratively extended set of seed queries. The second method is based on manual mapping between concepts from a medical guideline document and ontology concepts. Both methods include concept-expansion over subsumption and equality relations. We use both methods to determine a breast-cancer-centric subdomain of the SNOMED CT ontology. Our experiments show that the two methods produce a considerable overlap, but they also yield a large degree of complementarity, with interesting differences between the sets of concepts that they return. Analysis of the results reveals strengths and weaknesses of the different methods.",
  keywords  = "Disease related concepts, Identifying ontology subdomain, Mapping medical terminologies, Medical guidelines, Ontology subsetting, Seed queries",
  author    = "Krystyna Milian and Zharko Aleksovski and Richard Vdovjak and {Ten Teije}, Annette and {Van Harmelen}, Frank",
  year      = "2010",
  doi       = "10.1007/978-3-642-11808-1_5",
  isbn      = "3642118070",
  volume    = "5943 LNAI",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  pages     = "50--63",
  booktitle = "Knowledge Representation for Health-Care: Data, Processes and Guidelines, AIME 2009, Workshop KR4HC 2009, Revised Selected and Invited Papers",
}


@inbook{e97cea77ee104c0fad142910b7f4a715,
  title     = "Innovation is the vitality of software, an essay on the software industry in the Netherlands",
  author    = "Z. Huang and H Yin",
  year      = "2010",
  booktitle = "Journal of Programmers, 2010, no. 3",
}


@book{4a25d2a915ad4e02bd1b53b51edec3ed,
  title     = "Knowledge Representation for Health-Care. Data, Processes and Guidelines AIME 2009 Workshop KR4HC 2009",
  author    = "D Riano and {ten Teije}, A.C.M. and S Miksch and M Peleg",
  year      = "2010",
  doi       = "10.1007/978-3-642-11808-1",
  isbn      = "9783642118074",
  series    = "Lecture Notes in Computer Science",
  publisher = "Springer",
  number    = "5943",
}


@inbook{264f94decada4f8d83bde2e001d6bdd9,
  title     = "Large scale content analysis, ontology mapping, and concept drift",
  author    = "S. Wang and K.S. Schlobach and J. Takens and {van Atteveldt}, W.H.",
  year      = "2010",
  booktitle = "Proceedings of Het Etmaal van de Communicatiewetenschap 2010",
}


@article{6fdb8c2d749545d3b3cb5411f9cbe2a8,
  title     = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics): Preface",
  author    = "Lora Aroyo and Grigoris Antoniou and Eero Hyvönen and {Ten Teije}, Annette and Heiner Stuckenschmidt and Liliana Cabral and Tania Tudorache",
  year      = "2010",
  doi       = "10.1007/978-3-642-13489-0",
  volume    = "6089 LNCS",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
  number    = "PART 2",
}


@article{8535fcb026ee4e99b0ae16c409d94064,
  title     = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics): Preface",
  author    = "David Riaño and {Ten Teije}, Annette and Silvia Miksch and Mor Peleg",
  year      = "2010",
  volume    = "5943 LNAI",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@inbook{587af7c87e214671b646b241c0e02f32,
  title     = "Making Sense of Design Patterns",
  author    = "R.J. Hoekstra and J.A. Breuker",
  year      = "2010",
  editor    = "S Pinto and P Cimiano",
  booktitle = "Proceedings of EKAW 2010",
  publisher = "Springer",
}


@article{8d37ddc8fc3e4b35a8e9170a7486cd28,
  title     = "Measuring the dynamic bi-directional influence between content and social networks",
  author    = "S. Wang and P.T. Groth",
  note      = "Proceedings title: Proceedings of the 9th International Semantic Web Conference Publisher: Springer Place of publication: Shanghai",
  year      = "2010",
  volume    = "6496",
  pages     = "814--829",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@inbook{7543b43aa42a4da9bf053ac1d68e6834,
  title     = "Metadata and provenance management",
  author    = "E Deelman and G.B. Berriman and A.L. Chervenak and Ó Corcho and P.T. Groth and L Moreau",
  year      = "2010",
  isbn      = "978142006980",
  series    = "Computational Science Series",
  publisher = "CRC Press",
  pages     = "433--467",
  editor    = "A Shoshani and D Rotem",
  booktitle = "Semantic Data Management: challenges, technology, and deployment",
}


@inbook{643cb580cb714518bc44573f438ba71b,
  title     = "Normalized MEDLINE Distance in Context-Aware Life Science Literature Searches",
  author    = "Y. Wang and C. Wang and Y. Zeng and Z. Huang and V Momtchev and N. Zhong",
  year      = "2010",
  pages     = "709--715",
  booktitle = "Journal of Tsinghua Science and Technology, Vol 14, issue 6",
}


@inbook{4ad1c77d29c549bab4baeb9b9ab95ca6,
  title     = "OWL Reasoning with WebPIE: Calculating the Closure of 100 Billion Triples",
  author    = "J. Urbani and S. Kotoulas and J. Maassen and {van Harmelen}, F.A.H. and H.E. Bal",
  year      = "2010",
  booktitle = "The Semantic Web: Research and Applications, 7th Extended Semantic Web Conference, ESWC 2010",
  publisher = "Springer",
}


@article{6e0b72de47554e1abce2733e89c12c06,
  title     = "ProvenanceJS: Revealing the Provenance of Web Pages",
  author    = "P.T. Groth",
  note      = "Proceedings title: Third International Provenance and Annotation Workshop, IPAW 2010 Publisher: Springer ISBN: 978-3-642-17818-4 Editors: Deborah L. McGuinness, James Michaelis, Luc Moreau",
  year      = "2010",
  volume    = "6378",
  pages     = "283--285",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@book{03cc58acd3f54179974451c1eec5c8f7,
  title     = "Provenance XG Final Report",
  abstract  = "http://www.w3.org/2005/Incubator/prov/XGR-prov/",
  author    = "Y Gil and J Cheney and P.T. Groth and O Hartig and S Miles and L Moreau and {Pinheiro da Silva}, P",
  year      = "2010",
  series    = "W3C Incubator Group Reports",
  publisher = "W3C",
}


@inbook{fc5c2506a6404952ba440a0f8c84ef61,
  title     = "Reasoning with Inconsistent Ontologies",
  author    = "J. Fang and Z. Huang",
  year      = "2010",
  pages     = "687--691",
  booktitle = "Journal of Tsinghua Science and Technology, Vol 14, issue 6",
}


@inbook{3840ed3dc5c145559c19340b54b0462c,
  title     = "Representing Social Reality in OWL 2",
  author    = "R.J. Hoekstra",
  year      = "2010",
  editor    = "E Sirin and K Clark",
  booktitle = "Proceedings of OWL: Experiences and Directions (OWLED 2010)",
}


@misc{8a0016953e684eec962db6e6917c14bd,
  title   = "Scalable discovery of networked data",
  author  = "S. Kotoulas",
  note    = "Naam instelling promotie: VU Vrije Universiteit Naam instelling onderzoek: VU Vrije Universiteit",
  year    = "2010",
  school  = "Vrije Universiteit Amsterdam",
}


@inbook{627b85bea1fa45ce88c293df523fffa0,
  title     = "Scalable Reasoning for the Semantic Web",
  author    = "Z. Huang and J. Pan and G Qi",
  year      = "2010",
  series    = "8",
  pages     = "19--25",
  booktitle = "Communication of China Computer Federation",
}


@inbook{2db06fed731a469493d3800d7fd99085,
  title     = "Similarity features, and their role in concept alignment learning",
  author    = "S. Wang and G Englebienne and C.D.M. Guéret and K.S. Schlobach and A.H.J.C.A. Isaac and M.C. Schut",
  year      = "2010",
  isbn      = "9781612080000",
  pages     = "1--6",
  booktitle = "Proceedings of the Fourth International Conference on Advances in Semantic Processing",
}


@inbook{ed45c3e9147f427ebd2e1600fac1be20,
  title     = "Social Task Networks : Personal and Collaborative Task Formulation and Management in Social Networking Sites.",
  author    = "Y Gil and P.T. Groth and V Ratnakar",
  year      = "2010",
  booktitle = "AAAI Fall Symposium Series on Proactive Assistant Agents",
  publisher = "AAAI",
}


@inbook{365cbc9675204092b674b5e72e0bba01,
  title     = "SPARQL Query Answering on a Shared-Nothing Architecture",
  author    = "S. Kotoulas and J. Urbani",
  year      = "2010",
  booktitle = "Proceedings of the SemData Workshop at VLDB2010",
}


@article{655822bf1e534aa98f6a74083d43a5c3,
  title     = "Spatial Planning on the Semantic Web",
  author    = "R.J. Hoekstra and R.G.F. Winkels and E. Hupkes",
  year      = "2010",
  doi       = "10.1111/j.1467-9671.2010.01188.x",
  volume    = "14",
  journal   = "Transactions in GIS",
  issn      = "1361-1682",
  publisher = "Wiley-Blackwell",
  number    = "2",
}


@inbook{72e0de107ef040a5ae837ca013594727,
  title     = "Studying Scientific Discourse on the Web using Bibliometrics: A Chemistry Blogging Case Study",
  author    = "P.T. Groth and T Gurney",
  year      = "2010",
  booktitle = "Proceedings of the WebSci10: Extending the Frontiers of Society On-Line",
  publisher = "Web Science Trust",
}


@article{9502357d7ebb49faabaea77514b8557c,
  title     = "The anatomy of a nanopublication",
  abstract  = "As the amount of scholarly communication increases, it is increasingly difficult for specific core scientific statements to be found, connected and curated. Additionally, the redundancy of these statements in multiple fora makes it difficult to determine attribution, quality and provenance. To tackle these challenges, the Concept Web Alliance has promoted the notion of nanopublications (core scientific statements with associated context). In this document, we present a model of nanopublications along with a Named Graph/RDF serialization of the model. Importantly, the serialization is defined completely using already existing community-developed technologies. Finally, we discuss the importance of aggregating nanopublications and the role that the Concept Wiki plays in facilitating it. © 2010 - IOS Press and the authors. All rights reserved.",
  author    = "P.T. Groth and A Gibson and J Velterop",
  year      = "2010",
  doi       = "10.3233/ISU-2010-0613",
  volume    = "30",
  pages     = "51--56",
  journal   = "Information Services and Use",
  issn      = "0167-5265",
  publisher = "IOS Press",
}


@inbook{11b8620b3a1049c9ae7e6bbc7a93e690,
  title     = "The Application of Cloud Computing to the Creation of Image Mosaics and Management of Their Provenance",
  author    = "G.B. Berriman and E Deelman and P.T. Groth and J. Gideon",
  year      = "2010",
  editor    = "N. Radziwill and A. Bridger",
  booktitle = "SPIE Conference 7740: Software and Cyberinfrastructure for Astronomy",
  publisher = "SPIE",
}


@book{02a3638d894c4544930da1995b5ee84d,
  title     = "The Semantic Web: Research and Applications 7th Extended Semantic Web Conference, ESWC 2010 Proceedings",
  author    = "L.M. Aroyo and G. Antoniou and E. Hyvonen and {ten Teije}, A.C.M. and H. Stuckenschmidt",
  year      = "2010",
  series    = "Lecture Notes in Computer Science",
  publisher = "Springer",
  number    = "6089",
}


@article{aa7f74a475544d4682336fef34ef6c16,
  title    = "Towards expressive stream reasoning",
  abstract = "Stream Data processing has become a popular topic in database research addressing the challenge of efficiently answering queries over continuous data streams. Meanwhile data streams have become more and more important as a basis for higher level decision processes that require complex reasoning over data streams and rich background knowledge. In previous work the foundation for complex reasoning over streams and background knowledge was laid by introducing technologies for wrapping and querying streams in the RDF data format and by supporting simple forms of reasoning in terms of incremental view maintenance. In this paper, we discuss how this existing technologies should be extended toward richer forms of reasoning using Sensor Networks as a motivating example.",
  author   = "Heiner Stuckenschmidt and Stefano Ceri and {Della Valle}, Emanuele and {van Harmelen}, Frank and {di Milano}, P.",
  year     = "2010",
  pages    = "1--14",
  journal  = "Proceedings of the Dagstuhl Seminar on Semantic Aspects of Sensor Networks",
  issn     = "1862-4405",
}


@article{79033e74edba476c86439d24d39e8e39,
  title     = "Web Scale Reasoning: Scalable, Tolerant, and Dynamic (Guest Editors' Introduction)",
  author    = "Z. Huang and U. Keller and N. Zhong and F. Fischer",
  year      = "2010",
  volume    = "4",
  pages     = "1--2",
  journal   = "Information Services and Use",
  issn      = "0167-5265",
  publisher = "IOS Press",
  number    = "1",
}


@inbook{da72ccccaf0d4671adafb3ebe78d0f9e,
  title     = "What is concept drift and how to measure it?",
  author    = "S. Wang and K.S. Schlobach and M.C.A. Klein",
  year      = "2010",
  pages     = "241--256",
  editor    = "P. Cimiano and H.S. Pinto",
  booktitle = "Proceedings of the 17th International Conference on Knowledge Engineering and Knowledge Management",
}


@article{3206aea94c0a4d6a9944c76dbd01c493,
  title     = "It's a streaming world! Reasoning upon rapidly changing information",
  abstract  = "Stream reasoning, an unexplored yet high impact research area, is a new multidisciplinary approach that can provide the abstractions, foundations, methods, and tools required to integrate data streams, the Semantic Web, and reasoning systems, thus providing a way to answer our initial questions and many others. Stream reasoning can benefit numerous areas, including traffic monitoring and traffic pattern detection that appear to provide a natural application area. Dealing with users' stream of experience, mobile applications must reason on what part of the streaming information is relevant and what its meaning is. Stream reasoning requires continuous processing, because queries are normally registered and remain continuously active while data streams into the stream-reasoning system. Streams can appear in multiple forms, ranging from relation data over binary messaging protocols, such as data streams originated by sensor networks, to text streams over Web protocols, such as blogs and microblogs.",
  author    = "{Della Valle}, Emanuele and Stefano Ceri and {Van Harmelen}, Frank and Dieter Fensel",
  year      = "2009",
  month     = "11",
  volume    = "24",
  pages     = "83--89",
  journal   = "IEEE Intelligent Systems",
  issn      = "1541-1672",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  number    = "6",
}


@inbook{7ef22ad68eb141e1bb95070751f659e5,
  title     = "A Distance-based Operator to Revising Ontologies in DL SHOQ,",
  author    = "F. Yang and G Qi and Z. Huang",
  year      = "2009",
  booktitle = "Proceedings of The 10th European Conference on Symbolic and Quantitative Approaches to Reasoning with Uncertainty (ECSQARU 2009).",
}


@article{688f6d582689411e85f0e592b79990e4,
  title     = "A model of process documentation to determine provenance in mash-ups",
  abstract  = "Through technologies such as RSS (Really Simple Syndication), Web Services, and AJAX (Asynchronous JavaScript and XML), the Internet has facilitated the emergence of applications that are composed from a variety of services and data sources. Through tools such as Yahoo Pipes, these “mash-ups” can be composed in a dynamic, just-in-time manner from components provided by multiple institutions (i.e., Google, Amazon, your neighbor). However, when using these applications, it is not apparent where data comes from or how it is processed. Thus, to inspire trust and confidence in mash-ups, it is critical to be able to analyze their processes after the fact. These trailing analyses, in particular the determination of the provenance of a result (i.e., the process that led to it), are enabled by process documentation, which is documentation of an application's past process created by the components of that application at execution time. In this article, we define a generic conceptual data model that supports the autonomous creation of attributable, factual process documentation for dynamic multi-institutional applications. The data model is instantiated using two Internet formats, OWL and XML, and is evaluated with respect to questions about the provenance of results generated by a complex bioinformatics mash-up.",
  author    = "P.T. Groth and S Miles",
  year      = "2009",
  doi       = "10.1145/1462159.1462162",
  volume    = "9",
  journal   = "ACM Transactions on Internet Technology",
  issn      = "1533-5399",
  publisher = "Association for Computing Machinery (ACM)",
  number    = "1",
}


@inbook{d183e344c9de45d58f57876c3f4396cc,
  title     = "Analyzing the Gap between Workflows and their Natural Language Descriptions",
  abstract  = "Scientists increasingly use workflows to represent and share their computational experiments. Because of their declarative nature, focus on pre-existing component composition and the availability of visual editors, workflows provide a valuable start for creating user-friendly environments for end user scientists. However, there is still work to be done to create even more user-friendly environments. In this paper, we aim to identify key constructs that intelligent workflow systems could support to allow for more natural workflow representations. These constructs are identified through a comparison of bioinformatics workflows and their associated natural language descriptions obtained from the virtual research environment myExperiment.",
  author    = "P.T. Groth and Y Gil",
  year      = "2009",
  isbn      = "9780769537085",
  pages     = "299--305",
  booktitle = "2009 IEEE Congress on Services, Part I, SERVICES I 2009",
  publisher = "IEEE Computer Society",
}


@inbook{1c0209f2b3814577b44664e33a727f8a,
  title     = "A Pipeline-centric provenance model",
  abstract  = "In this paper we propose a new provenance model which is tailored to a class of workflow-based applications. We motivate the approach with use cases from the astronomy community. We generalize the class of applications the approach is relevant to and propose a pipeline-centric provenance model. Finally, we evaluate the benefits in terms of storage of the approach when applied to an astronomy application.",
  author    = "P.T. Groth and E Deelman and G Juve and G Mehta and G.B. Berriman",
  year      = "2009",
  isbn      = "9781605587172",
  editor    = "E Deelman and I Taylor",
  booktitle = "Proceedings of the 4th Workshop on Workflows in Support of Large-Scale Science, WORKS 2009",
  publisher = "ACM",
}


@inbook{7b8bc67c91d14e9895c178459e8d93b2,
  title     = "A scalable architecture for peer privacy on the Web",
  author    = "S. Kotoulas and R.G.M. Stegers",
  year      = "2009",
  booktitle = "Proceedings of the ESWC2009 Workshop on Trust and Privacy on the Social and Semantic Web (SPOT2009)",
  publisher = "CEUR",
}


@inbook{02aa874ae1d047d5ad0984794e71e32b,
  title     = "A Scientific Workflow Construction Command Line",
  abstract  = "Workflows have emerged as a common tool for scientists toexpress their computational analyses. While there are amultitude of visual data flow editors for workflow construction, to date there are none that support the input ofworkflows using natural language. This work presents thedesign of a hybrid system that combines natural languageinput through a command line with a visual editor. The design of the system is scoped by an extensive analysis of a corpus of workflow descriptions.",
  author    = "P.T. Groth and Y Gil",
  year      = "2009",
  isbn      = "9781605581682",
  pages     = "132--147",
  editor    = "C Conati and M Bauer and N Oliver and D.S. Weld",
  booktitle = "Proceedings of the 13th international conference on Intelligent user interfaces",
  publisher = "ACM",
}


@inbook{9ec628ec4f0843258d85d9f463f49380,
  title     = "BestPortal: Lessons learned in lightweight semantic access to court proceedings",
  author    = "R.J. Hoekstra",
  year      = "2009",
  editor    = "G. Governatori",
  booktitle = "Proceedings of the 22nd International Conference on Legal Knowledge and Information Systems (JURIX 2009)",
  publisher = "IOS Press",
}


@inbook{a052b1f3e7544588ba692f38cd1a0788,
  title     = "Context-aware SKOS vocabulary mappings in OWL 2",
  author    = "R.J. Hoekstra",
  year      = "2009",
  editor    = "R.J. Hoekstra and P.F. Patel-Schneider",
  booktitle = "Proceedings of OWL: Experiences and Directions 2009",
  publisher = "CEUR",
}


@article{2b3e46d8359f423b81e20265cbd6c583,
  title     = "Determining the Trustworthiness of New Electronic Contracts",
  abstract  = "Expressing contractual agreements electronically potentially allows agents to automatically perform functions surrounding contract use: establish- ment, fulfilment, renegotiation etc. For such automation to be used for real busi- ness concerns, there needs to be a high level of trust in the agent-based system. While there has been much research on simulating trust between agents, there are areas where such trust is harder to establish. In particular, contract proposals may come from parties that an agent has had no prior interaction with and, in competitive business-to-business environments, little reputation information may be available. In human practice, trust in a proposed contract is determined in part from the content of the proposal itself, and the similarity of the content to that of prior contracts, executed to varying degrees of success. In this paper, we argue that such analysis is also appropriate in automated systems, and to provide it we need systems to record salient details of prior contract use and algorithms for as- sessing proposals on their content.We use provenance technology to provide the former and detail algorithms for measuring contract success and similarity for the latter, applying them to an aerospace case study.",
  author    = "P.T. Groth and S Miles and S Modgil and O Oren and M. Luck and Y Gil",
  note      = "Proceedings title: Proceedings of the 10th Annual International Workshop Engineering Societies in the Agents' World (ESAW'09) Publisher: Springer ISBN: 978-3-642-10202-8 Editors: D Aldewereld, V Dignum, G Picard",
  year      = "2009",
  volume    = "5881",
  pages     = "132--147",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@inbook{47139bbb3ba14c0b82d7cca420a125b6,
  title     = "ERDF: Live Discovery for the Web of Data",
  author    = "C.D.M. Gueret and P.T. Groth and K.S. Schlobach",
  year      = "2009",
  booktitle = "the Billion Triple Challenge 2009",
}


@inbook{f4bbde95184c4fb8a9630669f358f175,
  title     = "Evaluating thesaurus alignments for semantic interoperability in the library domain",
  author    = "A.H.J.C.A. Isaac and S. Wang and C. Zinn and H. Matthezing and {van der Meij}, L. and K.S. Schlobach",
  year      = "2009",
  isbn      = "issn: 1541-1672",
  number    = "24 (2)",
  pages     = "76--86",
  booktitle = "IEEE Intelligent Systems",
}


@article{2e738a2d83f04f519e78e5384ac4c861,
  title     = "Evaluating Thesaurus Alignments for Semantic Interoperability in the Library Domain",
  author    = "A.H.J.C.A. Isaac and S. Wang and C. Zinn and H. Matthezing and {van der Meij}, L. and K.S. Schlobach",
  year      = "2009",
  doi       = "10.1109/MIS.2009.26",
  volume    = "24",
  pages     = "76--86",
  journal   = "IEEE Intelligent Systems",
  issn      = "1541-1672",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  number    = "2",
}


@article{f895358bec9a435b8c65cd4e5177cf64,
  title     = "Exploiting thesauri knowledge in medical guideline formalization",
  abstract  = "Objectives: As in software product lifecycle, the effort spent in maintaining medical knowl edge in guidelines can be reduced, if modularization, formalization and tracking of domain knowledge are employed across the guideline development phases. Methods: We propose to exploit and combine knowledge templates with medical background knowledge from existing thesauri in order to produce reusable building blocks used in guideline development. These tem- plates enable easier guideline formalization, by describing how chunks of medical knowledge can be combined into more complex ones and how they are linked to a textual representation. Results: By linking our ontology used in guideline formalization with existing thesauri, we can use compilations of thesauri knowledge as building blocks for modeling and maintaining the content of a medical guideline. Conclusions: Our paper investigates whether medical knowledge acquired from several medical thesauri can be molded on a guideline pattern, such that it supports building of executable models of guidelines. © 2009 Schattauer.",
  author    = "R.C. Serban and {ten Teije}, A.C.M.",
  year      = "2009",
  doi       = "10.3414/ME0629",
  volume    = "48",
  pages     = "468--474",
  journal   = "Methods of Information in Medicine",
  issn      = "0026-1270",
  publisher = "Schattauer GmbH",
  number    = "5",
}


@inbook{d938c841f8ba468fb1ffaa8b64f66218,
  title     = "Exposing Privacy Obligation Policies in Social Networking Sites",
  abstract  = "Increasingly, web-based applications are created through thecomposition of multiple functional components provided bydifferent institutions. These so called ''mash-ups'' are an effectivemeans to rapidly develop new applications. However,when these mash-ups are embedded within social networkingsites that aggregate and expose personal data, such as Facebook,MySpace, or LinkedIn, serious privacy issues arise becausepersonal data can be transmitted outside the applicationshosting institution. In this paper, we describe an initial architecture and implementation to address these privacy concernsthrough the exposure of privacy obligation policies to the user using a workflow-based representation of mash-ups.",
  author    = "P.T. Groth",
  year      = "2009",
  booktitle = "2009 AAAI Spring Symposium on Social Semantic Web",
  publisher = "The AAAI Press",
}


@inbook{3cfbdd1e9bf84a77a561097f4b3d43da,
  title     = "Expressive Reusable Workflow Templates",
  abstract  = "Workflow systems can manage complex scientific applications with distributed data processing. Although some workflow systems can represent collections of data with very compact abstractions and manage their execution efficiently, there are no approaches to date to manage collections of application components required to express some scientific applications. We present an approach to handle collections of components and data alike in expressive workflow templates whose basic structure is reusable. We also present an algorithm that can elaborate abstract compact workflow templates into execution-ready workflows that enumerate all computations to be carried out. We implemented the proposed approach in the Wings workflow system. Our work is motivated by real-world complex scientific applications that require handling of nested collections of both components and data.",
  author    = "Y Gil and P.T. Groth and V Ratnakar and C Fritz",
  year      = "2009",
  isbn      = "9780769538778",
  pages     = "334--351",
  booktitle = "2009 Fifth IEEE International Conference on e-Science",
  publisher = "IEEE Computer Society",
}


@inbook{6afa9d08c6c64c3f805dd44713141541,
  title     = "Extensions to the Relational Paths Based Learning Approach RPBL",
  author    = "Z. Gao and Z. Zhang and Z. Huang",
  year      = "2009",
  booktitle = "Proceedings of First Asian Conference on Intelligent Information and Database Systems",
}


@inbook{8634b9e67293479b90d66b5c54bd1110,
  title     = "Identifying Disease-Centric Subdomains in Very Large Medical Ontologies: A Case-Study on Breast Cancer Concepts in SNOMED CT. Or: Finding 2500 Out of 300.000",
  author    = "K. Milian and Z. Aleksovski and R. Vdovjak and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H.",
  year      = "2009",
  pages     = "50--63",
  editor    = "D. Riano and {ten Teije}, A.C.M. and S. Miksch and M. Peleg",
  booktitle = "Knowledge Representation for Health-Care: Data, Processes and Guidelines, AIME 2009 Workshop KR4HC 2009",
}


@inbook{76b03bd74ff44365a419e715fdd7a63d,
  title     = "Knowledge engineering rediscovered: towards reasoning patterns for the semantic web",
  abstract  = "The extensive work on Knowledge Engineering in the 1990s has resulted in a systematic analysis of task-types, and the corresponding problem solving methods that can be deployed for different types of tasks. That anal- ysis was the basis for a sound and widely accepted methodology for building knowledge-based systems, and has made it is possible to build libraries of reusable models, methods and code.In this paper, we make a first attempt at a similar analy- sis for Semantic Web applications. We will show that it is possible to identify a relatively small number of task- types, and that, somewhat surprisingly, a large set of Semantic Web applications can be described in this ty- pology. Secondly, we show that it is possible to decom- pose these task-types into a small number of primitive (“atomic”) inference steps. We give semi-formal defini- tions for both the task-types and the primitive inference steps that we identify. We substantiate our claim that our task-types are sufficient to cover the vast majority of Semantic Web applications by showing that all en- tries of the Semantic Web Challenges of the last 3 years can be classified in these task-types.",
  author    = "{van Harmelen}, F.A.H. and {ten Teije}, A.C.M. and H. Wache",
  year      = "2009",
  isbn      = "9781605586588",
  pages     = "81--88",
  editor    = "Y Gil and N Fridman",
  booktitle = "Proceedings of the 5th International Conference on Knowledge Capture (K-CAP 2009)",
  publisher = "ACM",
}


@inbook{0a5ed06a13da48baa1d268b48b101356,
  title     = "Learning Relations by Path Finding and Simultaneous Covering",
  author    = "Z. Gao and Z. Zhang and Z. Huang",
  year      = "2009",
  booktitle = "Proceedings of the WRI World Congress on Computer Science and Information Engineering",
}


@inbook{d5015aeacb9d4b6a845f89c78b406a34,
  title     = "Legal simcity: Legislative maps and semantic web supporting conflict resolution",
  author    = "R.J. Hoekstra and Rob Peters and {van Engers}, T.M. and E. Hupkes",
  year      = "2009",
  editor    = "{van Loenen}, B. and J.W.J. Besemer and J.A. Zevenbergen",
  booktitle = "SDI Convergence – Research, Emerging Trends, and Critical Assessment",
}


@inbook{8b4f701478a44a3c99aa7fde09e423dd,
  title     = "Leveraging Social Networking Sites to Acquire Rich Task Structure",
  author    = "Y Gil and P.T. Groth and V Ratnakar",
  year      = "2009",
  booktitle = "Proceedings of the IJCAI 2009 Workshop on User-Contributed Knowledge and Artificial Intelligence: An Evolving Synergy (WikiAI09)",
}


@inbook{3de347c95b6d4570b5cc447fe2295e5a,
  title     = "LKIF Core: Principled ontology development for the legal domain",
  author    = "R.J. Hoekstra and J.A. Breuker and {Di Bello}, M. and A.W.F. Boer",
  year      = "2009",
  series    = "Frontiers of Artificial Intelligence and Applications",
  publisher = "IOS Press",
  number    = "188",
  editor    = "J.A. Breuker and P. Casanovas and M. Klein and E. Francesconi",
  booktitle = "Law, Ontologies and the Semantic Web",
}


@inbook{2a3c5f2d7d14446a80a9c3e7678e37e0,
  title     = "Mapping-chains for studying concept shift in political ontologies",
  author    = "S. Wang and K.S. Schlobach and J. Takens and {van Atteveldt}, W.H.",
  year      = "2009",
  isbn      = "issn: 1613-0073",
  series    = "CEUR WS",
  number    = "551",
  booktitle = "Proceedings of the third International Workshop on Ontology Matching",
}


@inbook{66ece8203c9a4bdfba19f741474bcfdd,
  title     = "Mapping-Chains for studying Concept Shift in Political Ontologies",
  author    = "S. Wang and K.S. Schlobach and J. Takens and {van Atteveldt}, W.H.",
  year      = "2009",
  booktitle = "proceedings of the 3rd International Workshop on Ontology Matching",
}


@inbook{dabfb007e9524066a6c95e7dbff6fde9,
  title     = "MaRVIN: A platform for large-scale analysis of Semantic Web data",
  author    = "E. Oren and S. Kotoulas and G. Anadiotis and R.M. Siebes and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H.",
  year      = "2009",
  booktitle = "Proceedings of the WebSci09: Society On-Line",
}


@article{dcf9139489264b0793a7e4646dea2a19,
  title     = "MARVIN: Distributed reasoning over large-scale Semantic Web data",
  abstract  = "Many Semantic Web problems are difficult to solve through common divide-and-conquer strategies, since they are hard to partition. We present Marvin, a parallel and distributed platform for processing large amounts of RDF data, on a network of loosely coupled peers. We present our divide-conquer-swap strategy and show that this model converges towards completeness. Within this strategy, we address the problem of making distributed reasoning scalable and load-balanced. We present SpeedDate, a routing strategy that combines data clustering with random exchanges. The random exchanges ensure load balancing, while the data clustering attempts to maximise efficiency. SpeedDate is compared against random and deterministic (DHT-like) approaches, on performance and load-balancing. We simulate parameters such as system size, data distribution, churn rate, and network topology. The results indicate that SpeedDate is near-optimally balanced, performs in the same order of magnitude as a DHT-like approach, and has an average throughput per node that scales with sqrt(i) for i items in the system. We evaluate our overall Marvin system for performance, scalability, load balancing and efficiency. © 2009 Elsevier B.V. All rights reserved.",
  author    = "E. Oren and S. Kotoulas and G. Anadiotis and R.M. Siebes and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H.",
  year      = "2009",
  doi       = "10.1016/j.websem.2009.09.002",
  volume    = "7",
  journal   = "Journal of Web Semantics",
  issn      = "1570-8268",
  publisher = "Elsevier",
  number    = "4",
}


@inbook{5cb1c19988f84674b22bf96e6354200b,
  title     = "Massively Scalable Web Service Discovery",
  author    = "G. Anadiotis and S. Kotoulas and H Lausen and R.M. Siebes",
  year      = "2009",
  booktitle = "IEEE Advanced Information Networking and Applications",
}


@inbook{b5da14791cd942ebaf6991c4e7043a48,
  title     = "Matching multi-lingual subject vocabularies",
  author    = "S. Wang and A.H.J.C.A. Isaac and B.A.C. Schopman and K.S. Schlobach and {van der Meij}, L.",
  year      = "2009",
  isbn      = "issn: 0302-9743",
  series    = "Lecture Notes in Computer Science",
  publisher = "Springer Verlag",
  number    = "5714",
  pages     = "125--137",
  booktitle = "Proceedings of the 13th European Conference on Digital Libraries",
}


@inbook{272904821f1741a0a015c57cc48d7101,
  title     = "Matching multi-lingual subject vocabularies",
  author    = "S. Wang and A.H.J.C.A. Isaac and B.A.C. Schopman and K.S. Schlobach and {van der Meij}, L.",
  year      = "2009",
  pages     = "125--137",
  booktitle = "Proceedings of the 13th European Conference on Digital Libraries",
}


@inbook{830f8dd9e23945c792035249a2cd8063,
  title     = "Measuring Inconsistency Degrees of DL-Lite Ontologies",
  author    = "L. Zhou and H. Huang and G Qi and Y. Ma and Z. Huang and Y. Qu",
  year      = "2009",
  booktitle = "Proceedings of 2009 IEEE/WIC/ACM International Conference on Web Intelligence (WI-09)",
}


@inbook{fe8fa2e193da488bbbd10d7b994d18e4,
  title     = "Ontology Management",
  author    = "S. Bloehdorn and P. Haase and Z. Huang and Y. Sure and J Volker and {van Harmelen}, F. and R. Studer",
  year      = "2009",
  pages     = "3--20",
  booktitle = "Semantic Knowledge Management",
  publisher = "Springer",
}


@inbook{2e5cd892f2954ab4bbf2ed2c3c7029e7,
  title     = "owards Scalable Semantic Data Processing by Knowledge Summarization",
  author    = "Z. Zhang and Z. Huang and S. Gao and Xiaoru Zhang and Xiaofei. Zhang and A. Shi",
  year      = "2009",
  booktitle = "Proceedings of the 1st Asian Workshop on Scalable Semantic Data Processing",
  publisher = "ASWC2009 Workshop",
}


@inbook{471d3fccf5bf495d93d28a2abb55ae0a,
  title     = "Paraconsistent Query Answering Over DL-Lite Ontologies",
  author    = "L. Zhou and H. Huang and Y. Ma and G Qi and Z. Huang",
  year      = "2009",
  booktitle = "Proceedings of the Third Chinese Semantic Web Symposium (CSWS2009)",
}


@inbook{f98679b520294b28bf556a6437d8749e,
  title     = "Reasoning about repairability of workflows at design time",
  abstract  = "This paper describes an approach for reasoning about the repairability of workflows at design time. We propose a heuristic-based analysis of a workflow that aims at evaluating its definition, considering different design aspects and characteristics that affect its repairability (called repairability factors), in order to determine if the workflow schema supports repairable executions of its activities through the application of repair actions. The analysis intents to identify and evaluate the impact of critical design flaws affecting the repairability of workflows. The results of this analysis are fed back to the workflow designer and used to improve the repairability of the workflow by making appropriate changes to its definition.",
  keywords  = "Repairability of Workflows, Self-healing Web services, Web Service Composition, Workflow Design",
  author    = "Gaston Tagni and {Ten Teije}, Annette and {Van Harmelen}, Frank",
  year      = "2009",
  doi       = "10.1007/978-3-642-00328-8_46",
  isbn      = "9783642003271",
  volume    = "17 LNBIP",
  series    = "Lecture Notes in Business Information Processing",
  publisher = "Springer/Verlag",
  pages     = "455--467",
  booktitle = "Business Process Management Workshops - BPM 2008 International Workshops - Revised Papers",
}


@inbook{1be1fbf8e8f04bf2a04daeebb0e1bcb5,
  title     = "Reasoning with spatial plans on the semantic web",
  author    = "R.J. Hoekstra and R.G.F. Winkels and E. Hupkes",
  year      = "2009",
  editor    = "T. Calders and K. Tuyls",
  booktitle = "Proceedings of the 21st Benelux Conference on Artificial Intelligence (BNAIC 2009)",
}


@inbook{943234fad0bb4801a3c23e6d55263a55,
  title     = "Reasoning with spatial plans on the semantic web",
  author    = "R.J. Hoekstra and R.G.F. Winkels and E. Hupkes",
  year      = "2009",
  booktitle = "Proceedings of the Twelfth International Conference on Artificial Intelligence and Law (ICAIL 2009)",
  publisher = "ACM Press",
}


@article{77eaa2dc56bc4060a03f1a0c683f8ec3,
  title     = "Recording Process Documentation for Provenance",
  abstract  = "Scientific and business communities are adopting large-scale distributed systems as a means to solve a wide range of resource-intensive tasks. These communities also have requirements in terms of provenance. We define the provenance of a result produced by a distributed system as the process that led to that result. This paper describes a protocol for recording documentation of a distributed system's execution. The distributed protocol guarantees that documentation with characteristics suitable for accurately determining the provenance of results is recorded. These characteristics are confirmed through a number of proofs based on an abstract state machine formalization.",
  author    = "P.T. Groth and L Moreau",
  year      = "2009",
  doi       = "10.1109/TPDS.2008.215",
  volume    = "20",
  pages     = "1246--1259",
  journal   = "IEEE Transactions on Parallel and Distributed Systems",
  issn      = "1045-9219",
  publisher = "IEEE Computer Society",
  number    = "9",
}


@inbook{4f9963700bea41feb0b8e0a1889e915e,
  title     = "Relativizing Concept Descriptions to Comparison Classes in ALC",
  author    = "S. Klarman and K.S. Schlobach",
  year      = "2009",
  editor    = "{Bernardo Cuenca Grau} and {Ian Horrocks} and {Boris Motik} and {Ulrike Sattler}",
  booktitle = "Proceedings of the 22nd International Workshop on Description Logics (DL 2009)",
}


@article{36022f6e57c7436da7b0d0e61f5e799a,
  title     = "Research chapters in the area of stream reasoning",
  abstract  = "Data streams occur in a variety of modern applications. Specialized Stream Database Management Systems proved to be an optimal solution for on the y analysis of data streams, but they cannot perform complex reasoning tasks that requires to combine the streaming data with less time variant knowledge. At the same time, while reasoners are year after year scaling up in the classical, time invariant domain of ontological knowledge, reasoning upon rapidly changing information has been neglected or forgotten. We hereby propose stream reasoning - an unexplored, yet high impact, research area - as the new multi-disciplinary approach which will provide the abstractions, foundations, methods, and tools required to integrate data streams and reasoning systems. In particular the focus of this paper is to sketch the research chapters of Stream Reasoning.",
  author    = "{Della Valle}, E. and S. Ceri and Daniele Braga and I. Celino and D. Frensel and {Van Harmelen}, F. and G. Unel",
  year      = "2009",
  volume    = "466",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@inbook{6d6ebfafab104f6e80ea982771de9239,
  title     = "Results of the Ontology Alignment Evaluation and Initiative",
  author    = "J. Euzenat and A. Ferrara and L. Hollink and A.H.J.C.A. Isaac and C. Joslyn and V. Malaisé and C. Meilicke and A. Nikolov and J. Pane and M. Sabou and F. Scharffe and P. Shvaiko and V. Spiliopoulos and H. Stuckenschmidt and X. Vojtchsvtek and {Trojahn dos Santos}, C. and G. Vouros and S. Wang",
  year      = "2009",
  booktitle = "4th International Workshop on Ontology Matching",
}


@inbook{d0140681ce5a45cea1d9578ccdf03dc9,
  title     = "Scaffolding Instructions to Learn Procedures from Users",
  abstract  = "Humans often teach procedures through tutorial instruction to other humans. For computers, learning from natural human instruction remains a challenge as it is plagued with incompleteness and ambiguity. Instructions sre often given out of order and are not always consistent. Moreover, humans assume that the learnerhas a wealth of knowledge and skills, which computersdo not always have. Our goal is to develop an electronic student that can be made increasingly capable through research to learn from human tutorial instruction. Initially, we will provide our student with human understandable instruction that is extended with many scaffolding statements that supplement the limited initialcapabilities of the student. Over time, improvements to the system are driven and quantified by the removal of scaffolding instructions that are not consideredto be natural for users to provide humans. This paper describes our initial design and implementationof this system, how it learns from scaffolded instruction in two different domains, and the initial relaxations of scaffolding that the system supports.",
  author    = "P.T. Groth and Y Gil",
  year      = "2009",
  booktitle = "AAAI 2009 Spring Symposium on Agents that Learn from Human Teachers",
  publisher = "The AAAI Press",
}


@inbook{be965766aadc48ae9e446885e3e8fd49,
  title     = "Scalable Distributed Reasoning Using MapReduce",
  author    = "J. Urbani and S. Kotoulas and E. Oren and {van Harmelen}, F.A.H.",
  year      = "2009",
  booktitle = "The Semantic Web - ISWC 2009",
  publisher = "Springer",
}


@inbook{196c12f730fb4161ad7e761183cc77e7,
  title     = "Semantic Web Reasoning by Swarm Intelligence",
  author    = "K. Dentler and C.D.M. Gueret and K.S. Schlobach",
  year      = "2009",
  booktitle = "International Workshop on Scalable Semantic Web Knowledge Base Systems (SSWS)",
}


@book{7feb5098da3a4e45baea3a73b738b445,
  title     = "SKOS Simple Knowledge Organization System Primer",
  author    = "A.H.J.C.A. Isaac and E Summers",
  year      = "2009",
  publisher = "W3C",
}


@book{74e234374d3a460992ec7a25c3139a62,
  title     = "SKOS Use Cases and Requirements",
  author    = "A.H.J.C.A. Isaac and J. Phipps",
  year      = "2009",
  publisher = "W3C",
}


@inbook{88f14d5ee4c544d4a7999be214d01d30,
  title     = "Social Relation based Scalable Semantic Search Refinement",
  author    = "Y. Zeng and X. Ren and N. Zhong and Z. Huang and Y. Wang",
  year      = "2009",
  booktitle = "Proceedings of the 1st Asian Workshop on Scalable Semantic Data Processing,ASWC2009",
}


@inbook{7329cd50c7c94763b7a08ff4bb4156ec,
  title     = "Spatial planning on the Semantic Web",
  author    = "R.G.F. Winkels and R.J. Hoekstra and E. Hupkes",
  year      = "2009",
  editor    = "Dave Kolas and Nancy Wiegand and Gary Berg-Cross",
  booktitle = "Proceedings of the Terra Cognita 2009 Workshop",
  publisher = "CEUR",
}


@inbook{bdacb05355614467aa87cd49f4c5bdcd,
  title     = "The Free Speech Engine: Conversational Web Service Compatibility for Free",
  author    = "R.G.M. Stegers and {van Harmelen}, F.A.H. and {ten Teije}, A.C.M.",
  year      = "2009",
  isbn      = "1601321309",
  pages     = "53--59",
  editor    = "H.R. Arabnia and A. Marsh",
  booktitle = "Proceedings of the 2009 International Conference on Semantic Web & Web Services, SWWS 2009",
  publisher = "CSREA Press",
}


@inbook{d165896df0bc4d299b153e8d2d007ab5,
  title     = "The NoTube Beancounter: Aggregating User Data for television Programme Recommendation",
  author    = "{van Aart}, C.J. and L.M. Aroyo and Y. Raimond and D.A. Brickley and A.Th. Schreiber and M. Minno and L. Miller and D. Palmisano and M. Mostarda and R.M. Siebes and V. Buser",
  year      = "2009",
  booktitle = "Workshop Social Data on the Web",
}


@book{26d44dbb34d84c769ee0c588c3180191,
  title     = "The Semantic Web: Research and Apllications",
  author    = "L.M. Aroyo and P. Traverso and F. Ciravegna and P. Cimiano and T. Heath and E. Hyvonen and R. Mizoguchi and E. Oren and M. Sabou and {Paslaru Bontas Simperl}, E.",
  year      = "2009",
  publisher = "Springer",
}


@inbook{4a98671935a5416aae29af39f9455bf6,
  title     = "Towards nature-inspired communication in Peer-to-Peer networks",
  author    = "C.D.M. Gueret",
  year      = "2009",
  booktitle = "Proceedings of the 21st Benelux Conference on Artificial Intelligence (BNAIC 2009)",
}


@inbook{0335b6d0f5274febadb6bc63f7c7543c,
  title     = "Unifying Web-scale Search and Reasoning from the Viewpoint of Granularity",
  author    = "Y. Zeng and Y. Wang and Z. Huang and N. Zhong",
  year      = "2009",
  booktitle = "Proceedings of the 2009 International Conference on Active Media Technology (AMT'09)",
}


@article{d0f279ece3764d0db4671a586709882a,
  title     = "Using model checking for critiquing based on clinical guidelines",
  abstract  = "Objective: Medical critiquing systems compare clinical actions performed by a physician with a predefined set of actions. In order to provide useful feedback, an important task is to find differences between the actual actions and a set of 'ideal' actions as described by a clinical guideline. In case differences exist, the critiquing system provides insight into the extent to which they are compatible. Methods and material: We propose a computational method for such critiquing, where the ideal actions are given by a formal model of a clinical guideline, and where the actual actions are derived from real world patient data. We employ model checking to investigate whether a part of the actual treatment is consistent with the guideline. Results: We show how critiquing can be cast in terms of temporal logic, and what can be achieved by using model checking. Furthermore, a method is introduced for off-line computing relevant information which can be exploited during critiquing. The method has been applied to a clinical guideline of breast cancer in conjunction with breast cancer patient data. © 2008 Elsevier B.V. All rights reserved.",
  author    = "P. Groot and A. Hommersom and P.F. Lucas and R. Merk and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H. and R.C. Serban",
  year      = "2009",
  doi       = "10.1016/j.artmed.2008.07.007",
  volume    = "46",
  pages     = "19--36",
  journal   = "Artificial Intelligence in Medicine",
  issn      = "0933-3657",
  publisher = "Elsevier",
  number    = "1",
}


@inbook{5c47bf4a3cfd4aea958cb165b25e097a,
  title     = "Using Semantic Distances for Reasoning with Inconsistent Ontolgies (Extended Abstract)",
  author    = "Z. Huang and {van Harmelen}, F.",
  year      = "2009",
  booktitle = "Proceedings of The 21st Benelux Conference on Artificial Intelligence (BNAIC 2009)",
}


@article{0930585942dc452e8573ed79204a43e3,
  title    = "Using semantic distances for reasoning with inconsistent ontologies",
  abstract = "Re-using and combining multiple ontologies on the Web is bound to lead to inconsistencies between the combined vocabularies. Even many of the ontologies that are in use today turn out to be inconsistent once some of their implicit knowledge is made explicit. However, robust and efficient methods to deal with inconsistencies are lacking from current Semantic Web reasoning systems, which are typically based on classical logic. In earlier papers, we have proposed the use of syntactic relevance functions as a method for reasoning with inconsistent ontologies. In this paper, we extend that work to the use of semantic distances. We show how Google distances can be used to develop semantic relevance functions to reason with inconsistent ontologies. In essence we are using the implicit knowledge hidden in theWeb for explicit reasoning purposes. We have implemented this approach as part of the PION reasoning system. We report on experiments with several realistic ontologies. The test results show that a mixed syntactic/semantic approach can significantly improve reasoning performance over the purely syntactic approach. Furthermore, our methods allow to trade-off computational cost for inferential completeness. Our experiment shows that we only have to give up a little quality to obtain a high performance gain.",
  author   = "Zhisheng Huang and {van Harmelen}, Frank",
  year     = "2009",
  pages    = "329--330",
  journal  = "Belgian/Netherlands Artificial Intelligence Conference",
  issn     = "1568-7805",
}


@inbook{11ce36e84f2f4e75982a76946a96f34b,
  title     = "Vocabulary Matching for Book Indexing Suggestion in Linked Libraries - A Prototype Imlementation & Evaluation",
  author    = "A.H.J.C.A. Isaac and D. Kramer and {van der Meij}, L. and S. Wang and K.S. Schlobach and J. Stapel",
  year      = "2009",
  booktitle = "Proceedings of the 8th International Semantic Web Conference",
}


@inbook{9ae5a8de784c4e96bed688e83a9d99cc,
  title     = "Vocabulary matching for book indexing suggestion in linked libraries: A prototype implementation & evaluation",
  author    = "A.H.J.C.A. Isaac and D. Kramer and {van der Meij}, L. and S. Wang and K.S. Schlobach and J. Stapel",
  year      = "2009",
  isbn      = "issn: 0302-9743",
  series    = "Lecture Notes in Computer Science",
  publisher = "Springer Verlag",
  number    = "5823",
  pages     = "843--859",
  booktitle = "The Semantic Web - ISWC 2009",
}


@inbook{7554b346f45f412a88538a905b4bf13b,
  title     = "Vocabulary Matching for Book Indexing Suggestion in Linked Libraries – A Prototype Implementation & Evaluation",
  author    = "A.H.J.C.A. Isaac and D. Kramer and {van der Meij}, L. and S. Wang and K.S. Schlobach and J. Stapel",
  year      = "2009",
  booktitle = "International Semantic Web Conference (ISWC 2009)",
}


@article{1849fcb892e14cb2aea6e787e31111ab,
  title     = "Expertise-based peer selection in Peer-to-Peer networks",
  abstract  = "Peer-to-Peer systems have proven to be an effective way of sharing data. Modern protocols are able to efficiently route a message to a given peer. However, determining the destination peer in the first place is not always trivial. We propose a model in which peers advertise their expertise in the Peer-to-Peer network. The knowledge about the expertise of other peers forms a semantic topology. Based on the semantic similarity between the subject of a query and the expertise of other peers, a peer can select appropriate peers to forward queries to, instead of broadcasting the query or sending it to a random set of peers. To calculate our semantic similarity measure, we make the simplifying assumption that the peers share the same ontology. We evaluate the model in a bibliographic scenario, where peers share bibliographic descriptions of publications among each other. In simulation experiments complemented with a real-world field experiment, we show how expertise-based peer selection improves the performance of a Peer-to-Peer system with respect to precision, recall and the number of messages.",
  keywords  = "Ontologies, P2P, Routing, Semantic overlays",
  author    = "Peter Haase and Ronny Siebes and {van Harmelen}, Frank",
  year      = "2008",
  month     = "4",
  doi       = "10.1007/s10115-006-0055-1",
  volume    = "15",
  pages     = "75--107",
  journal   = "Knowledge and Information Systems",
  issn      = "0219-1377",
  publisher = "Springer London",
  number    = "1",
}


@inbook{185e12b86cd14bd38f795f1eb1a7d59c,
  title     = "A Kernel Revision Operator for Terminologies",
  author    = "G Qi and P. Haase and Z. Huang and Q. Ji and J. Pan and J. Voelker",
  year      = "2008",
  booktitle = "Proceedings of the 7th International Semantic Web Conference (ISWC2008),",
  publisher = "Springer",
}


@inbook{0f751c46de4c4041b9ba31f1f504ab3f,
  title     = "Analyzing Distribution and Evolution of Research Interests by Term Extraction and Ontology Learning",
  author    = "Z. Gao and W Zhu and Y. Qu and Z. Huang",
  year      = "2008",
  booktitle = "Proceedings of the Ninth International Conference on Web-Age Information Management (WAIM'2008)",
}


@inbook{55c06a578abd4e88942de11c9bebff4c,
  title     = "An Evolutionary Perspective on Approximate RDF Query Answering",
  author    = "C.D.M. Gueret and E. Oren and K.S. Schlobach and M.C. Schut",
  note      = "mcs-Gueret2008",
  year      = "2008",
  series    = "Lecture Notes in Computer Science",
  publisher = "Springer",
  pages     = "215--228",
  editor    = "S. Greco and T. Lukasiewicz",
  booktitle = "Scalable Uncertainty Management",
}


@inbook{9c3bfdd495c4487eb902713ef59389d6,
  title     = "An Integrated Approach for Automatic Construction of Bilingual Chinese-English WordNet",
  author    = "R. Xu and Z. Gao and Y. Pan and Y. Qu and Z. Huang",
  year      = "2008",
  booktitle = "Proceedings of 2008 Asian Semantic Web Conference (ASWC2008)",
}


@inbook{bb3d543b65c34a02af620c61e01ee230,
  title     = "Challenging the Internet of the Future with Urban Computing",
  author    = "{Della Valle}, E and I. Celino and K Kim and Z. Huang and V. Tresp and W. Hauptmann and Y. Huang",
  year      = "2008",
  booktitle = "Proceedings of the 1st International Workshop on Blending Physical and Digital Spaces on the Internet(OneSpace 2008)",
}


@inbook{d164f9a305d14844a2ee4f8e479f5fba,
  title     = "Chapter 21 The Semantic Web: Webizing Knowledge Representation",
  abstract  = "The World Wide Web opens up new opportunities for the use of knowledge representation: a formal description of the semantic content of Web pages can allow better processing by computational agents. Further, the naming scheme of the Web, using Universal Resource Indicators, allows KR systems to avoid the ambiguities of natural language and to allow linking between semantic documents. These capabilities open up a raft of new possibilities for KR, but also present challenges to some traditional KR assumptions.",
  author    = "Jim Hendler and {van Harmelen}, Frank",
  year      = "2008",
  doi       = "10.1016/S1574-6526(07)03021-0",
  isbn      = "9780444522115",
  volume    = "3",
  series    = "Foundations of Artificial Intelligence",
  pages     = "821--839",
  booktitle = "Handbook of Knowledge Representation",
}


@inbook{9ffce07e39894dc7bc2c12c15265120e,
  title     = "Dynamic Aspects of OPJK Legal Ontology",
  author    = "Z. Huang and S. Schlobach and {van Harmelen}, F. and N. Casellas and P. Casanovas",
  year      = "2008",
  booktitle = "Computational Models of the Law",
  publisher = "Springer",
}


@article{b4f4bbd7e19846dc990e034e83ad845f,
  title     = "Fuzzy Reasoning Based on First-Order Modal Logic,",
  abstract  = "As an extension of traditional modal logics, this paper proposes a fuzzy first-order modal logic based on believable degree, and gives out a description of the fuzzy first-order modal logic based on constant domain semantics. In order to make the reasoning procedure between the fuzzy assertions efficiently, the notion of the fuzzy constraint is considered. A fuzzy constraint is an expression in which both syntax ingredient and semantics information are contained. By using the notion of the constraint, the reasoning procedure between the fuzzy assertions can be directly considered in the semantics environment, thus a fuzzy reasoning formal system which contains fuzzy constraint as its basic element is developed. As a main work of the paper, the relationship between the validity of the new assertion and the satisfiability of the fuzzy constraints is analyzed, and reasoning rules of the fuzzy reasoning formal system based on first order modal logic are given out. Further work could be done by considering the soundness and completeness of the formal system, and by building an efficient mechanism of reasoning procedure. The results have potential application in the areas of artificial intelligence and computer science. © 2008 by Journal of Software.",
  author    = "Xiaoru Zhang and Z. Zhang and Y. Sui and Z. Huang",
  year      = "2008",
  doi       = "10.3724/sp.j.1001.2008.03170",
  volume    = "19",
  pages     = "3170--3178",
  journal   = "Journal of Software",
  issn      = "1796-217X",
  publisher = "Academy Publisher",
  number    = "12",
}


@inbook{bf4d9dc0db0f47759bf872c849fc8b69,
  title     = "Models of interaction as a grounding for Peer to peer knowledge sharing",
  abstract  = "Most current attempts to achieve reliable knowledge sharing on a large scale have relied on pre-engineering of content and supply services. This, like traditional knowledge engineering, does not by itself scale to large, open, peer to peer systems because the cost of being precise about the absolute semantics of services and their knowledge rises rapidly as more services participate. We describe how to break out of this deadlock by focusing on semantics related to interaction and using this to avoid dependency on a priori semantic agreement; instead making semantic commitments incrementally at run time. Our method is based on interaction models that are mobile in the sense that they may be transferred to other components, this being a mechanism for service composition and for coalition formation. By shifting the emphasis to interaction (the details of which may be hidden from users) we can obtain knowledge sharing of sufficient quality for sustainable communities of practice without the barrier of complex meta-data provision prior to community formation.",
  author    = "David Robertson and Adam Barker and Paolo Besana and Alan Bundy and Chen-Burger, {Yun Heh} and David Dupplaw and Fausto Giunchiglia and {Van Harmelen}, Frank and Fadzil Hassan and Spyros Kotoulas and David Lambert and Guo-chao Li and Jarred McGinnis and Fiona McNeill and Nardine Osman and {De Pinninck}, {Adrian Perreau} and Ronny Siebes and Carles Sierra and Chris Walton",
  year      = "2008",
  doi       = "10.1007/978-3-540-89784-2_4",
  isbn      = "3540897836",
  volume    = "4891 LNCS",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  pages     = "81--129",
  booktitle = "Advances in Web Semantics I - Ontologies, Web Services and Applied Semantic Web",
}


@inbook{31ea4763a7934f2992fcc15a8a2a686a,
  title     = "Open knowledge coordinating knowledge sharing through peer-to-peer interaction",
  abstract  = "The drive to extend the Web by taking advantage of automated symbolic reasoning (the so-called Semantic Web) has been dominated by a traditional model of knowledge sharing, in which the focus is on task-independent standardisation of knowledge. It appears to be difficult, in practice, to standardise in this way because the way in which we represent knowledge is strongly influenced by the ways in which we expect to use it. We present a form of knowledge sharing that is based not on direct sharing of {"}true{"} statements about the world but, instead, is based on sharing descriptions of interactions. By making interaction specifications the currency of knowledge sharing we gain a context to interpreting knowledge that can be transmitted between peers, in a manner analogous to the use of electronic institutions in multi-agent systems. The narrower notion of semantic commitment we thus obtain requires peers only to commit to meanings of terms for the purposes and duration of the interactions in which they appear. This lightweight semantics allows networks of interaction to be formed between peers using comparatively simple means of tackling the perennial issues of query routing, service composition and ontology matching. A basic version of the system described in this paper has been built (via the OpenKnowledge project); all its components use established methods; many of these have been deployed in substantial applications; and we summarise a simple means of integration using the interaction specification language itself.",
  author    = "Dave Robertson and Fausto Giunchiglia and {Van Harmelen}, Frank and Maurizio Marchese and Marta Sabou and Marco Schorlemmer and Nigel Shadbolt and Ronnie Siebes and Carles Sierra and Chris Walton and Srinandan Dasmahapatra and Dave Dupplaw and Paul Lewis and Mikalai Yatskevich and Spyros Kotoulas and {De Pinninck}, {Adrian Perreau} and Antonis Loizou",
  year      = "2008",
  doi       = "10.1007/978-3-540-85058-8_1",
  isbn      = "3540850570",
  volume    = "5118 LNAI",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  pages     = "1--18",
  booktitle = "Languages, Methodologies and Development Tools for Multi-Agent Systems - First International Workshop, LADS 2007, Revised Selected and Invited Papers",
}


@inbook{5875181a1c2d4f28ac393beb7615ee96,
  title     = "Retrieval of Case Law to provide laymen with information about liability: Preliminary Results of the BEST-Project",
  abstract  = "This paper describes the experiments carried out in the context of the BEST-project, an interdisciplinary project with researchers from the Law faculty and the AI department of the VU University Amsterdam. The aim of the project is to provide laymen with information about their legal position in a liability case, based on retrieved case law. The process basically comes down to (1) analyzing the input of a layman in terms of a layman ontology, (2) mapping this ontology to a legal ontology, (3) retrieve relevant case law based, and finally (4) present the results in a comprehensible way to the layman. This paper describes the experiments undertaken regarding step 4, and in particular step 3.",
  author    = "E.M. Uijttenbroek and A.R. Lodder and M.C.A. Klein and {van Harmelen}, F.A.H. and G. Wildeboer and R.L.L. Sie",
  year      = "2008",
  doi       = "10.1007/978-3-540-85569-9_19",
  series    = "Lecture Notes in Computer Science",
  publisher = "Springer",
  pages     = "291--310",
  editor    = "P. Casanovas and G. Sartor and N. Casellas and R. Rubino",
  booktitle = "Computable Models of the Law",
}


@inbook{228407fb7d844ee49d8c8a52bfe35170,
  title     = "Semantic Web Technologies as the Foundation for the Information Infrastructure",
  abstract  = "The Semantic Web is arising over the pas few years as a realistic option for a world wide Information Infrastructure, with its promises of semantic interoperability and serendipitous reuse. In this paper we will analyse the essential ingredients of semantic technologies, what makes them suitable as the foundation for the Information Infrastructure, and what the alternatives to semantic technologies would be as foundations for the Information Infrastructure. We will make a survey of the most important achievements on semantic technologies in the past few years, and point to the most important challenges that remain to be solved. 3.1 Historical trend towards increasing demands on interoperability When Thomas Watson, the founder of IBM, was asked for his estimate of how many computers would be needed worldwide, his reply is widely claimed to have been: 'about five'",
  keywords  = "everythingimport",
  author    = "{Van Oosterom}, Peter and S. Zlatanova and {Van Harmelen}, Frank",
  year      = "2008",
  series    = "Creating Spatial Information Infrastructures Towards the Spatial Semantic Web",
  publisher = "CRC Press",
  editor    = "{Van Oosterom}, Peter and S Zlatanova",
  booktitle = "Creating Spatial Information Infrastructures Towards the Spatial Semantic Web",
}


@inbook{89593f9cdf4a47b2a87ef115dc5b84f0,
  title     = "Studies in Health Technology and Informatics: Preface",
  author    = "{Ten Teije}, Annette and Silvia Miksch and Lucas, {Peter J F}",
  year      = "2008",
  isbn      = "9781586038731",
  volume    = "139",
  booktitle = "Computer-based Medical Guidelines and Protocols: A Primer and Current Trends",
}


@article{5b117ee42b20420f95c0f0e98f9f5876,
  title     = "SWI-Prolog and the Web",
  abstract  = "Prolog is an excellent tool for representing and manipulating data written in formal languages as well as natural language. Its safe semantics and automatic memory management make it a prime candidate for programming robust Web services. Although Prolog is commonly seen as a component in a Web application that is either embedded or communicates using a proprietary protocol, we propose an architecture where Prolog communicates to other components in a Web application using the standard HTTP protocol. By avoiding embedding in external Web servers, development and deployment become much easier. To support this architecture, in addition to the transfer protocol, we must also support parsing, representing and generating the key Web document types such as HTML, XML and RDF. This article motivates the design decisions in the libraries and extensions to Prolog for handling Web documents and protocols. The design has been guided by the requirement to handle large documents efficiently. The described libraries support a wide range of Web applications ranging from HTML and XML documents to Semantic Web RDF processing. The benefits of using Prolog for Web-related tasks are illustrated using three case studies. © 2008 Cambridge University Press.",
  author    = "J. Wielemaker and Z. Huang and {van der Meij}, L.",
  year      = "2008",
  doi       = "10.1017/S1471068407003237",
  volume    = "8",
  pages     = "363--392",
  journal   = "Theory and practice of logic programming",
  issn      = "1471-0684",
  publisher = "Cambridge University Press",
  number    = "3",
}


@inbook{11c3609efb2848aa99fe2d3b2696fea1,
  title     = "Towards LarKC: A platform for Web-scale reasoning",
  abstract  = "Current Semantic Web reasoning systems do not scale to the requirements of their hottest applications, such as analyzing data from millions of mobile devices, dealing with terabytes of scientific data, and content management in enterprises with thousands of knowledge workers. In this paper, we present our plan of building the Large Knowledge Collider, a platform for massive distributed incomplete reasoning that will remove these scalability barriers. This is achieved by (i) enriching the current logic-based Semantic Web reasoning methods, (ii) employing cognitively inspired approaches and techniques, and (iii) building a distributed reasoning platform and realizing it both on a high-performance computing cluster and via {"}computing at home{"}. In this paper, we will discuss how the technologies of LarKC would move beyond the state-of-the-art of Web-scale reasoning.",
  author    = "Dieter Fensel and {Van Harmelen}, Frank and Bo Andersson and Paul Brennan and Hamish Cunningham and Valle, {Emanuele Delia} and Florian Fischer and Zhisheng Huang and Atanas Kiryakov and Lee, {Tony Kyung Il} and Lael Schooler and Volker Tresp and Stefan Wesner and Michael Witbrock and Ning Zhong",
  year      = "2008",
  doi       = "10.1109/ICSC.2008.41",
  isbn      = "9780769532790",
  pages     = "524--529",
  booktitle = "Proceedings - IEEE International Conference on Semantic Computing 2008, ICSC 2008",
}


@inbook{f5e16d95265f4bcca85de2c23e0603d9,
  title     = "Towards LarKC: a Platform forWeb-scale Reasoning",
  author    = "D. Fensel and {van Harmelen}, F. and B. Andersson and P. Brennan and H. Cunningham and {Della Valle}, E and F. Fischer and Z. Huang and A. Kiryakov and Tony Lee and L. Schooler and V. Tresp and S. Wesner and M. Witbrock and N. Zhong",
  year      = "2008",
  booktitle = "Proceedings of the Second IEEE International Conference on Semantic Computing (IEEE-ICSC2008)",
  publisher = "IEEE Press",
}


@inbook{f0875887aba345d0b477778099093add,
  title     = "Two Variations on Ontology Alignment Evaluation: Methodological Issues",
  author    = "L. Hollink and {van Assem}, M.F.J. and S. Wang and A.H.J.C.A. Isaac and A.T. Schreiber",
  note      = "Hollink08a",
  year      = "2008",
  series    = "LNCS",
  publisher = "Springer-Verlag",
  pages     = "388--401",
  booktitle = "The Semantic Web -- ESWC 2008, Tenerife, Spain",
}


@inbook{f6ebc6bcf3524ef6a9bba64fd69e25bd,
  title     = "Urban Computing: a challenging problem for Semantic Technologies,",
  author    = "{Della Valle}, E and I. Celino and K Kim and Z. Huang and V. Tresp and W. Hauptmann and Y. Huang",
  year      = "2008",
  booktitle = "Proceedings of the 2008 Workshop of New Forms of Reasoning for the Semantic Web (NEFORS08),",
}


@misc{ac157507808c4b649dab081c2d3106a9,
  title    = "Using multiple ontologies as background knowledge in ontology matching",
  abstract = "Using ontology as a background knowledge in ontology match- ing is being actively investigated. Recently the idea attracted attention because of the growing number of available ontologies, which in turn opens up new opportunities, and reduces the problem of nding candi- date background knowledge. Particularly interesting is the approach of using multiple ontologies as background knowledge, which we explore in this paper. We report on an experimental study conducted using real-life ontologies published online. The rst contribution of this paper is an exploration about how the matching performance behaves when multiple background ontologies are used cumulatively. As a second contribution, we analyze the impact that dierent types of background ontologies have to the matching perfor- mance. With respect to the precision and recall, more background knowl- edge monotonically increases the recall, while the precision depends on the quality of the added background ontology, with high quality tending to increase, and the low quality tending to decrease the precision.",
  author   = "Zharko Aleksovski and {Ten Kate}, Warner and {Van Harmelen}, Frank",
  year     = "2008",
}


@inbook{cc8f64d2961f44f081297913c4d35da5,
  title     = "Using Semantic Distances for Reasoning with Inconsistent Ontolgies",
  author    = "Z. Huang and {van Harmelen}, F.",
  year      = "2008",
  booktitle = "Proceedings of the 7th International Semantic Web Conference (ISWC2008)",
}


@inbook{ac98cb7e00ea41e7acba662a998d85e0,
  title     = "Explaining the Relevance of Court Decisions to Laymen",
  abstract  = "In the context of intelligent disclosure of case law, we report on our findings with respect to the presentation of relevant court decisions back to the laymen users. For this presentation we first localize the relevant legal concepts in the cases using shallow NLP techniques. Hereafter we investigated the use of techniques from the field of recommender systems, i.e. keyword style explanation and influence style explanation, to present the cases to the user in an understandable way. In order to find out if we succeeded in that respect, we conducted a small user satisfaction research. It shows promising results, and gives us some directions for future research.",
  author    = "Wildeboer, {Gwen R.} and Klein, {Michel C. A.} and Uijttenbroek, {Elisabeth M.}",
  year      = "2007",
  month     = "12",
  series    = "Frontiers in Artificial Intelligence and Applications",
  publisher = "IOS Press",
  pages     = "129--138",
  editor    = "Lodder, {Arno R.} and Laurens Mommers",
  booktitle = "The 20th Anniversary International Conference on Legal Knowledge and Information Systems, JURIX 2007",
}


@article{f1ad47c1ddaf494dbee0e6b53873d059,
  title     = "Where is the Web in the Semantic Web?",
  author    = "{van Harmelen}, Frank and Michael Uschold",
  year      = "2007",
  month     = "12",
  doi       = "10.1016/j.websem.2007.09.003",
  volume    = "5",
  pages     = "225--226",
  journal   = "Journal of Web Semantics",
  issn      = "1570-8268",
  publisher = "Elsevier",
  number    = "4",
}


@article{3171cb825d2044b994361c28e94d7dc7,
  title     = "Rough Description Logics for Modeling Uncertainty in Instance Unification",
  abstract  = "Instance-unification is a prime example for uncertainty on the Semantic Web, as it is not always possible to automatically determine with absolute certainty whether two references denote the same object or not. In this paper, we present openacademia, a semantics-based system for the management of distributed bibliographic information collected from the Web, in which the Instance Unification problem is ubiquitous. Our tentative solution is Rough DL, a simple extension of classical Description Logics, which allows for approximations of vague concept. This shows that already a simple formalism for dealing with uncertain information in a qualitative way can provide an elegant solution to practical problems on the Semantic Web.",
  author    = "Klein, {Michel C. A.} and Peter Mika and Stefan Schlobach",
  year      = "2007",
  month     = "11",
  volume    = "327",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@article{70984b20c73d4635895bc4a4c0493911,
  title     = "Reasoning and change management in modular ontologies",
  abstract  = "The benefits of modular representations are well known from many areas of computer science. While in software engineering modularization is mainly a vehicle for supporting distributed development and re-use, in knowledge representation, the main goal of modularization is efficiency of reasoning. In this paper, we concentrate on the benefits of modularization in the context of ontologies, explicit representations of the terminology used in a domain. We define a formal representation for modular ontologies based on the notion of Distributed Description Logics and introduce an architecture that supports local reasoning by compiling implied axioms. We further address the problem of guaranteeing the correctness and completeness of compiled knowledge in the presence of changes in different modules. We propose a heuristic for analyzing changes and their impact on compiled knowledge and guiding the process of updating compiled information that can often reduce the effort of maintaining a modular ontology by avoiding unnecessary re-compilation.",
  keywords  = "Change management, Distributed Knowledge Representation, Ontologies, Reasoning",
  author    = "Heiner Stuckenschmidt and Michel Klein",
  year      = "2007",
  month     = "11",
  doi       = "10.1016/j.datak.2007.02.001",
  volume    = "63",
  pages     = "200--223",
  journal   = "Data and Knowledge Engineering",
  issn      = "0169-023X",
  publisher = "Elsevier",
  number    = "2",
}


@article{13f19f42684b4319a665bedb844b5cda,
  title     = "Debugging incoherent terminologies",
  abstract  = "In this paper we study the diagnosis and repair of incoherent terminologies. We define a number of new nonstandard reasoning services to explain incoherence through pinpointing, and we present algorithms for all of these services. For one of the core tasks of debugging, the calculation of minimal unsatisfiability preserving subterminologies, we developed two different algorithms, one implementing a bottom-up approach using support of an external description logic reasoner, the other implementing a specialized tableau-based calculus. Both algorithms have been prototypically implemented. We study the effectiveness of our algorithms in two ways: we present a realistic case study where we diagnose a terminology used in a practical application, and we perform controlled benchmark experiments to get a better understanding of the computational properties of our algorithms in particular and the debugging problem in general.",
  keywords  = "Debugging, Description logics, Diagnosis",
  author    = "Stefan Schlobach and Zhisheng Huang and Ronald Cornet and {Van Harmelen}, Frank",
  year      = "2007",
  month     = "10",
  doi       = "10.1007/s10817-007-9076-z",
  volume    = "39",
  pages     = "317--349",
  journal   = "Journal of Automated Reasoning",
  issn      = "0168-7433",
  publisher = "Springer Netherlands",
  number    = "3",
}


@article{283379e3418b40d8b9471f6e8e144a1a,
  title     = "Extraction and use of linguistic patterns for modelling medical guidelines",
  abstract  = "Objective: The quality of knowledge updates in evidence-based medical guidelines can be improved and the effort spent for updating can be reduced if the knowledge underlying the guideline text is explicitly modelled using the so-called linguistic guideline patterns, mappings between a text fragment and a formal representation of its corresponding medical knowledge. Methods and material: Ontology-driven extraction of linguistic patterns is a method to automatically reconstruct the control knowledge captured in guidelines, which facilitates a more effective modelling and authoring of medical guidelines. We illustrate by examples the use of this method for generating and instantiating linguistic patterns in the text of a guideline for treatment of breast cancer, and evaluate the usefulness of these patterns in the modelling of this guideline. Results: We developed a methodology for extracting and using linguistic patterns in guideline formalization, to aid the human modellers in guideline formalization and reduce the human modelling effort. Using automatic transformation rules for simple linguistic patterns, a good recall (between 72% and 80%) is obtained in selecting the procedural knowledge relevant for the guideline model, even though the precision of the guideline model generated automatically covers only between 20% and 35% of the human-generated guideline model. These results indicate the suitability of our method as a pre-processing step in medical guideline formalization. Conclusions: Modelling and authoring of medical texts can benefit from our proposed method. As pre-requisites for generating automatically a skeleton of the guideline model from the procedural part of the guideline text, to aid the human modeller, the medical terminology used by the guideline must have a good overlap with existing medical thesauri and its procedural knowledge must obey linguistic regularities that can be mapped into the control constructs of the target guideline modelling language.",
  keywords  = "Knowledge engineering, Medical guideline formalization, Ontologies, Semantic mark-up",
  author    = "Radu Serban and {ten Teije}, Annette and {van Harmelen}, Frank and Mar Marcos and Cristina Polo-Conde",
  year      = "2007",
  month     = "2",
  doi       = "10.1016/j.artmed.2006.07.012",
  volume    = "39",
  pages     = "137--149",
  journal   = "Artificial Intelligence in Medicine",
  issn      = "0933-3657",
  publisher = "Elsevier",
  number    = "2",
}


@misc{5160bf1935694a7ebbe55cd90ecf505f,
  title   = "Agent-Based Mediated Service Negotiation",
  author  = "D.G.A. Mobach",
  note    = "Mobach2007 Naam instelling promotie: Vrije Universiteit Amsterdam Naam instelling onderzoek: Vrije Universiteit Amsterdam",
  year    = "2007",
  school  = "Vrije Universiteit Amsterdam",
}


@inbook{a78c5e01371f479abdbc2a83e460d202,
  title     = "Analyzing differences in operational disease definitions using ontological modeling",
  abstract  = "In medicine, there are many diseases which cannot be precisely characterized but are considered as natural kinds. In the communication between health care professionals, this is generally not problematic. In biomedical research, however, crisp definitions are required to unambiguously distinguish patients with and without the disease. In practice, this results in different operational definitions being in use for a single disease. This paper presents an approach to compare different operational definitions of a single disease using ontological modeling. The approach is illustrated with a case-study in the area of severe sepsis.",
  author    = "Linda Peelen and Klein, {Michel C.A.} and Stefan Schlobach and {De Keizer}, {Nicolette F.} and Niels Peek",
  year      = "2007",
  doi       = "10.1007/978-3-540-73599-1_40",
  isbn      = "3540735984",
  volume    = "4594",
  pages     = "297--302",
  booktitle = "Artificial Intelligence in Medicine - 11th Conference on Artificial Intelligence in Medicine, AIME 2007, Proceedings",
  publisher = "Springer/Verlag",
}


@article{e648354f2c7c42ea84dd9e30a3b23574,
  title     = "Anytime classification by ontology approximation",
  abstract  = "Reasoning with large or complex ontologies is one of the bottle-necks of the Semantic Web. In this paper we present an anytime algorithm for classification based on approximate subsumption. We give the formal definitions for approximate subsumption, and show its monotonicity and soundness; we show how it can be computed in terms of classical subsumption; and we study the computational behaviour of the algorithm on a set of realistic benchmarks. The most interesting finding is that anytime classification works best on ontologies where classical subsumption is hardest to compute.",
  author    = "S. Schlobach and E. Blaauw and {El Kebir}, M. and {Ten Teije}, A. and {Van Harmelen}, F. and S. Bortoli and M.C. Hobbelman and K. Millian and Y. Ren and S. Stam and P. Thomassen and {Van Het Schip}, R. and {Van Willigem}, W.",
  year      = "2007",
  volume    = "291",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@inbook{16dd252158ee4c6d8b0bccb52b505242,
  title     = "A Specification Language for Coordination in Agent Systems",
  author    = "T. Bosse and M. Hoogendoorn and R.C. Serban and J. Treur",
  note      = "IAT07-coordlanguage",
  year      = "2007",
  pages     = "252--256",
  editor    = "Lin, {Tsau Young} and Bradshaw, {Jeffrey M.} and Matthias Klusch and Chengqi Zhang and Andrei Broder and ward Ho",
  booktitle = "Proceedings of the Seventh IEEE/WIC/ACM International Conference on Intelligent Agent Technology (IAT'07)",
  publisher = "IEEE Computer Society Press",
}


@inbook{f8ae4cb58c704fbda43b91cb8b62b1ed,
  title     = "A Survey and Categorization of Ontology-Matching Use Cases",
  author    = "Z. Aleksovski and {van Hage}, W.R. and A.H.J.C.A. Isaac",
  note      = "aleksovski2007a-survey",
  year      = "2007",
  booktitle = "Proceedings of the International Workshop on Ontology Matching (OM-2007)",
}


@inbook{ab77811a4b954f9a992b4347c8b7bd11,
  title     = "Case law retrieval by concept search and visualization",
  abstract  = "The BEST-project (BATNA Establishment using Semantic web Technology, http://best-project.nl) strives to provide disputing parties with information about their legal position in a liability case. Our assumption is that through intelligent disclosure of Dutch Tort Law cases, laymen can estimate their chances: information derived from previous court decisions can help to obtain insight into BATNAs (Best Alternative to a Negotiated Agreement), alternatives a party has if negotiation fails. Information BATNAs also contributes to determining the room left for negotiation.",
  author    = "Uijttenbroek, {Elisabeth M.} and Klein, {Michel C.A.} and Lodder, {Arno R.} and {Van Harmelen}, Frank",
  year      = "2007",
  doi       = "10.1145/1276318.1276336",
  isbn      = "1595936807",
  pages     = "95--96",
  booktitle = "Proceedings of the Eleventh International Conference on Artificial Intelligence and Law",
}


@article{e05d559e4df64d6894cb558978b44a3b,
  title    = "Description logics with approximate definitions precise modeling of vague concepts",
  abstract = "We extend traditional Description Logics (DL) with a simple mechanism to handle approximate concept definitions in a qualitative way. Often, for example in medical applications, concepts are not definable in a crisp way but can fairly exhaustively be constrained through a particular sub- and a particular super-concept. We introduce such lower and upper approximations based on rough-set semantics, and show that reasoning in these languages can be reduced to standard DL satisfiability. This allows us to apply Rough Description Logics in a study of medical trials about sepsis patients, which is a typical application for precise modeling of vague knowledge. The study shows that Rough DL-based reasoning can be done in a realistic use case and that modeling vague knowledge helps to answer important questions in the design of clinical trials.",
  author   = "Stefan Schlobach and Michel Klein and Linda Peelen",
  year     = "2007",
  pages    = "557--562",
  journal  = "IJCAI International Joint Conference on Artificial Intelligence",
  issn     = "1045-0823",
}


@inbook{a0f178afe3e64189b27e076b91f617bd,
  title     = "Dynamic aspects of OPJK legal ontology",
  abstract  = "The OPJK (Ontology of Professional Judicial Knowledge) is a legal ontology developed to map questions of junior judges to a set of stored frequently asked questions. In this paper, we investigate dynamic and temporal aspects of one of the SEKT legal ontologies, by subjecting the ontology OPJK to MORE, a multi-version ontologies reasoning Sys-tem. MORE is based on a temporal logic approach. We show how the temporal logic approach can be used to obtain a better understanding of dynamic and temporal evolution of legal ontologies.",
  author    = "Zhisheng Huang and Stefan Schlobach and {Van Harmelen}, Frank and Núria Casellas and Pompeu Casanovas",
  year      = "2007",
  doi       = "10.1007/978-3-540-85569-9-8",
  isbn      = "3540855688",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  pages     = "113--129",
  booktitle = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
}


@inbook{a06de2367d074723b9a3facc6af646e3,
  title     = "Maintaining formal models of living guidelines efficiently",
  abstract  = "Translating clinical guidelines into formal models is beneficial in many ways, but expensive. The progress in medical knowledge requires clinical guidelines to be updated at relatively short intervals, leading to the term living guideline. This causes potentially expensive, frequent updates of the corresponding formal models. When performing these updates, there are two goals: The modelling effort must be minimised and the links between the original document and the formal model must be maintained. In this paper, we describe our solution, using tools and techniques developed during the Protocure II project.",
  author    = "Andreas Seyfang and Begoña Martínez-Salvador and Radu Serban and Jolanda Wittenberg and Silvia Miksch and Mar Marcos and {Ten Teije}, Annette and Rosenbrand, {Kitty C J G M}",
  year      = "2007",
  isbn      = "3540735984",
  volume    = "4594 LNAI",
  pages     = "441--445",
  booktitle = "Artificial Intelligence in Medicine - 11th Conference on Artificial Intelligence in Medicine, AIME 2007, Proceedings",
}


@inbook{a3658c341e5b4fb19372c0d8f012020e,
  title     = "Media, politics and the semantic web",
  abstract  = "The media play an important role in the functioning of our society. This role is extensively studied by Communication Scientists, requiring a systematic analysis of media content. The methods developed in this field utilize complex data models and background knowledge. This data is generally represented ad hoc, making it difficult to analyze, combine and share data sets. In this paper we present our work on formalizing this representation using RDF(S). We discuss the requirements for a good representation, highlighting a number of non-trivial modeling decisions. We conclude with a description of the resulting system and the benefits for a recent investigation of the 2006 Dutch parliamentary campaign. This case study shows concrete improvements for annotating, querying, and analyzing data, but also indicates a number of aspects that were more difficult to model in RDF(S), contributing to the discussion on modeling with and improving RDF(S) and associated tools.",
  author    = "{Van Atteveldt}, Wouter and Stefan Schlobach and {Van Harmelen}, Frank",
  year      = "2007",
  isbn      = "3540726667",
  volume    = "4519 LNCS",
  pages     = "205--219",
  booktitle = "The Semantic Web: Research and Applications - 4th European Semantic Web Conference, ESWC 2007, Proceedings",
}


@article{dc76cc7f74974587a75368900ab5ca98,
  title     = "Modelling web service composition for deductive web mining",
  abstract  = "Composition of simpler web services into custom applications is understood as promising technique for information requests in a heterogeneous and changing environment. This is also relevant for applications characterised as deductive web mining (DWM). We suggest to use problem-solving methods (PSMs) as templates for composed services, We developed a multi-dimensional, ontologybased framework, and a collection of PSMs, which enable to characterise DWM applications at an abstract level; we describe several existing applications in this framework. We show that the heterogeneity and unboundedness of the web demands for some modifications of the PSM paradigm used in the context of traditional artificial intelligence. Finally, as simple proof of concept, we simulate automated DWM service composition on a small collection of services, PSM-based templates, data objects and ontological knowledge, all implemented in Prolog.",
  keywords  = "Ontologies, Problem-solving methods, Web mining, Web services",
  author    = "Vojtěch Svátek and Miroslav Vacura and Martin Labský and {Ten Teije}, Annette",
  year      = "2007",
  volume    = "26",
  pages     = "255--279",
  journal   = "Computing and Informatics",
  issn      = "1335-9150",
  publisher = "Slovak Academy of Sciences",
  number    = "3",
}


@inbook{7db0bff634e34942bf7e4b3b037f4fac,
  title     = "Sample Evaluation of Ontology-Matching Systems",
  author    = "{van Hage}, W.R. and A.H.J.C.A. Isaac and Z. Aleksovski",
  note      = "hage2007sample",
  year      = "2007",
  booktitle = "Proceedings of the 5th International Evaluation of Ontologies and Ontology-based Tools Workshop (EON 2007)",
}


@article{f446dfde96584d05882e619ad6b21a41,
  title     = "The histamine H3 receptor antagonist clobenpropit enhances GABA release to protect against NMDA-induced excitotoxicity through the cAMP/protein kinase A pathway in cultured cortical neurons",
  author    = "H. Dai and Q. Fu and Y. Shen and W. Hu and Z. Zhang and H. Timmerman and R. Leurs and Z. Chen",
  year      = "2007",
  doi       = "10.1016/j.ejphar.2007.01.069",
  volume    = "563",
  pages     = "117--23",
  journal   = "European Journal of Pharmacology",
  issn      = "0014-2999",
  publisher = "Elsevier",
  number    = "1-3",
}


@inbook{c8660026c38c49ad891322ef6124401f,
  title     = "The OpenKnowledge system: An interaction-centered approach to knowledge sharing",
  abstract  = "The information that is made available through the semantic web will be accessed through complex programs (web-services, sensors, etc.) that may interact in sophisticated ways. Composition guided simply by the specifications of programs' inputs and outputs is insufficient to obtain reliable aggregate performance - hence the recognised need for process models to specify the interactions required between programs. These interaction models, however, are traditionally viewed as a consequence of service composition rather than as the focal point for facilitating composition. We describe an operational system that uses models of interaction as the focus for knowledge exchange. Our implementation adopts a peer to peer architecture, thus making minimal assumptions about centralisation of knowledge sources, discovery and interaction control.",
  author    = "Ronny Siebes and Dave Dupplaw and Spyros Kotoulas and {De Pinninck}, {Adrian Perreau} and {Van Harmelen}, Frank and David Robertson",
  year      = "2007",
  isbn      = "9783540768463",
  volume    = "4803 LNCS",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  number    = "PART 1",
  pages     = "381--390",
  booktitle = "On the Move to Meaningful Internet Systems 2007: CoopIS, DOA, ODBASE, GADA, and IS - OTM Confederated International Conferences CoopIS, DOA, ODBASE, GADA, and IS 2007, Proceedings",
  edition   = "PART 1",
}


@inbook{fe168c99784142baaf7ab0ded58564ca,
  title     = "The role of model checking in critiquing based on clinical guidelines",
  abstract  = "Medical critiquing systems criticise clinical actions performed by a physician. In order to provide useful feedback, an important task is to find differences between the actual actions and a set of 'ideal' actions as described by a clinical guideline. In case differences exist, insight to which extent they are compatible is provided by the critiquing system. We propose a methodology for such critiquing, where the ideal actions are given by a formal model of a clinical guideline, and where the actual actions are derived from real world patient data. We employ model checking to investigate whether a part of the actual treatment is consistent with the guideline. Furthermore, it is shown how critiquing can be cast in terms of temporal logic, and what can be achieved by using model checking. The methodology has been applied to a clinical guideline of breast cancer in conjunction with breast cancer patient data.",
  author    = "Perry Groot and Arjen Hommersom and Peter Lucas and Radu Serban and {Ten Teije}, Annette and {Van Harmelen}, Frank",
  year      = "2007",
  isbn      = "3540735984",
  volume    = "4594 LNAI",
  pages     = "411--420",
  booktitle = "Artificial Intelligence in Medicine - 11th Conference on Artificial Intelligence in Medicine, AIME 2007, Proceedings",
}


@inbook{5f5655376ba6488f9ac5430a4320ed17,
  title     = "Two obvious intuitions: Ontology-mapping needs background knowledge and approximation",
  abstract  = "Ontology mapping (or: ontology alignment, or integration) is one of the most active areas the Semantic Web area. An increasing amount of ontologies are becoming available in recent years, and if the Semantic Web is to be taken seriously, the problem of ontology mapping must be solved. Numerous approaches are being proposed, a yearly competition is being organized, and a number of survey papers have appeared. Nevertheless, with only a few exceptions, two obvious intuitions on ontology mapping have been overlooked: if humans perform {"}ontology mapping{"} in their daily life (a task we all solve every day), they do not do this in a vacuum. Instead, they exploit a rich body of background knowledge already shared by both agents involved in the mapping process. Similarly, humans do not expect that their daily-life ontology mapping is perfect. We can very well cope with approximate translations between concepts used by different agents (in fact, we are so good at it that we barely notice that we do this). In this talk I will discuss recent work where we have quantitatively shown that indeed, ontology mapping can benefit from background knowledge, and that, somewhat surprisingly, more background knowledge leads to continuously improving results. We also discuss how the use of such background knowledge can be exploited to find approximate mappings when perfect mappings cannot be found.",
  author    = "{Van Harmelen}, Frank",
  year      = "2007",
  doi       = "10.1109/IAT.2006.127",
  isbn      = "9780769527482",
  pages     = "11",
  booktitle = "Proceedings - 2006 IEEE/WIC/ACM International Conference on Intelligent Agent Technology (IAT 2006 Main Conference Proceedings), IAT'06",
}


@inbook{1b123029e3cf4002b156f95cf12584d8,
  title     = "Using Google distance to weight approximate ontology matches",
  abstract  = "Discovering mappings between concept hierarchies is widely regarded as one of the hardest and most urgent problems facing the Semantic Web. The problem is even harder in domains where concepts are inherently vague and ill-defined, and cannot be given a crisp definition. A notion of approximate concept mapping is required in such domains, but until now, no such notion is vailable. The first contribution of this paper is a definition for approximate mappings between concepts. Roughly, a mapping between two concepts is decomposed into a number of submappings, and a sloppiness value determines the fraction of these submappings that can be ignored when establishing the mapping. A potential problem of such a definition is that with an increasing sloppiness value, it will gradually allow mappings between any two arbitrary concepts. To improve on this trivial behaviour, we need to design a heuristic weighting which minimises the sloppiness required to conclude desirable matches, but at the same time maximises the sloppiness required to conclude undesirable matches. The second contribution of this paper is to show that a Google based similarity measure has exactly these desirable properties. We establish these results by experimental validation in the domain of musical genres. We show that this domain does suffer from ill-defined concepts. We take two real-life genre hierarchies from the Web, we compute approximate mappings between them at varying levels of sloppiness, and we validate our results against a handcrafted Gold Standard. Our method makes use of the huge amount of knowledge that is implicit in the current Web, and exploits this knowledge as a heuristic for establishing approximate mappings between ill-defined concepts.",
  keywords  = "Approximation, Google distance",
  author    = "Risto Gligorov and {Ten Kate}, Warner and Zharko Aleksovski and {Van Harmelen}, Frank",
  year      = "2007",
  doi       = "10.1145/1242572.1242676",
  isbn      = "1595936548",
  pages     = "767--776",
  booktitle = "16th International World Wide Web Conference, WWW2007",
}


@inbook{b324c489cc7c404a8db70f02dab314dd,
  title     = "Thesaurus-based Retrieval of Case Law",
  abstract  = "In the context of intelligent disclosure of case law, we report on our findings on methods for retrieving relevant case law within the domain of tort law from a repository of 68.000 court verdicts. We apply a thesaurus-based technique to find specific legal situations. It appears that statistical measures of term relevance are not sufficient, but that explicit knowledge about specific formulations used in law and case law are required to distinguish relevant case law from irrelevant. In addition, we found out that the retrieving legal concepts with an ``interpretive'' character requires a different method than concepts do not require additional interpretation.",
  author    = "Klein, {Michel C. A.} and {van Steenbergen}, Wouter and Uijttenbroek, {Elisabeth M.} and Lodder, {Arno R.} and Harmelen, {Frank van}",
  year      = "2006",
  month     = "12",
  isbn      = "978-1-58603-698-0",
  pages     = "61--70",
  editor    = "Engers, {Tom van}",
  booktitle = "Proceedings of the 19th International JURIX conference: JURIX 2006",
  publisher = "IOS Press",
}


@inbook{bb952e2524ac44c3a57a5fd2d13f4f8d,
  title     = "Reasoning With Inconsistent Ontologies: Framework, Prototype, and Experiment",
  keywords  = "Classical logical inference engines, DICE (Diagnoses for Intensive Care Evaluation), Inconsistency reasoner and selection function, Linear extension and linear reduction strategy, Logical entailment, Polysemy, Prototype of PION using SWI-Prolog, Reasoning with inconsistent ontologies, Syntactic relevance-based selection functions",
  author    = "Zhisheng Huang and Harmelen, {Frank Van} and {Ten Teije}, Annette",
  year      = "2006",
  month     = "7",
  doi       = "10.1002/047003033X.ch5",
  isbn      = "0470025964",
  pages     = "71--93",
  booktitle = "Semantic Web Technologies: Trends and Research in Ontology-based Systems",
  publisher = "John Wiley & Sons, Ltd",
}


@article{2e2cfe4d2d6840a9af6f68e04ad13615,
  title     = "Improving medical protocols by formal methods",
  abstract  = "OBJECTIVES: During the last decade, evidence-based medicine has given rise to an increasing number of medical practice guidelines and protocols. However, the work done on developing and distributing protocols outweighs the efforts on guaranteeing their quality. Indeed, anomalies like ambiguity and incompleteness are frequent in medical protocols. Recent efforts have tried to address the problem of protocol improvement, but they are not sufficient since they rely on informal processes and notations. Our objective is to improve the quality of medical protocols.APPROACH: The solution we suggest to the problem of quality improvement of protocols consists in the utilisation of formal methods. It requires the definition of an adequate protocol representation language, the development of techniques for the formal analysis of protocols described in that language and, more importantly, the evaluation of the feasibility of the approach based on the formalisation and verification of real-life medical protocols. For the first two aspects we rely on earlier work from the fields of knowledge representation and formal methods. The third aspect, i.e. the evaluation of the use of formal methods in the quality improvement of protocols, constitutes our main objective. The steps with which we have carried out this evaluation are the following: (1) take two real-life reference protocols which cover a wide variety of protocol characteristics; (2) formalise these reference protocols; (3) check the formalisation for the verification of interesting protocol properties; and (4) determine how many errors can be uncovered in this way.RESULTS: Our main results are: a consolidated formal language to model medical practice protocols; two protocols, each both modelled and formalised; a list of properties that medical protocols should satisfy; verification proofs for these protocols and properties; and perspectives of the potentials of this approach. Our results have been evaluated by a panel of medical experts, who judged that the problems we detected in the protocols with the help of formal methods were serious and should be avoided.CONCLUSIONS: We have succeeded in demonstrating the feasibility of formal methods for improving medical protocols.",
  keywords  = "Artificial Intelligence, Clinical Protocols, Feasibility Studies, Humans, Infant, Newborn, Jaundice, Neonatal, Practice Guidelines as Topic, Programming Languages, Quality Assurance, Health Care, Journal Article, Research Support, Non-U.S. Gov't",
  author    = "{ten Teije}, Annette and Mar Marcos and Michel Balser and {van Croonenborg}, Joyce and Christoph Duelli and {van Harmelen}, Frank and Peter Lucas and Silvia Miksch and Wolfgang Reif and Rosenbrand, {Kitty C J G M} and Andreas Seyfang",
  year      = "2006",
  month     = "3",
  doi       = "10.1016/j.artmed.2005.10.006",
  volume    = "36",
  pages     = "193--209",
  journal   = "Artificial Intelligence in Medicine",
  issn      = "0933-3657",
  publisher = "Elsevier",
  number    = "3",
}


@inbook{8c39755717ec492b8f29f86f653d2ade,
  title     = "Approximate semantic matching of music classes on the internet",
  abstract  = "We address the problem of semantic matching, which concerns the search for semantic agreement between heterogeneous concept hierarchies. We propose a new approximation method to discover and assess the {"}strength{"} (preciseness) of a semantic match between concepts from two such concept hierarchies. We apply the method in the music domain, and present the results of preliminary tests on concept hierarchies from actual sites on the Internet.",
  keywords  = "approximation, Music genre, music style, ontology, semantic matching, semantic web",
  author    = "Zharko Aleksovski and Kate, {Warner Ten} and {Van Harmelen}, Frank",
  year      = "2006",
  doi       = "10.1007/1-4020-4995-1_9",
  isbn      = "9781402049538",
  pages     = "133--147",
  booktitle = "Intelligent Algorithms in Ambient and Biomedical Computing",
  publisher = "Springer Netherlands",
}


@inbook{cf8614b39e9f4861839dd9ad3025e60f,
  title     = "Bibster-a semantics-based bibliographic peer-to-peer system",
  abstract  = "This chapter describes the design, implementation, and evaluation of Bibster, a Peer-to-Peer system for exchanging bibliographic data among researchers. Bibster exploits ontologies in data-storage, query formulation, query-routing and answer presentation: When bibliographic entries are made available for use in Bibster, they are structured and classified according to two different ontologies. This ontological structure is then exploited to help users formulate their queries. Subsequently, the ontologies are used to improve query routing across the Peer-to-Peer network. Finally, the ontologies are used to post-process the returned answers in order to do duplicate detection. The chapter describes each of these ontology-based aspects of Bibster.",
  author    = "Peter Haase and Björn Schnizler and Jeen Broekstra and Marc Ehrig and {Van Harmelen}, Frank and Maarten Menken and Peter Mika and Michal Plechawski and Pawel Pyszlak and Ronny Siebes and Steffen Staab and Christoph Tempich",
  year      = "2006",
  doi       = "10.1007/3-540-28347-1_19",
  isbn      = "3540283463",
  pages     = "349--363",
  booktitle = "Semantic Web and Peer-to-Peer: Decentralized Management and Exchange of Knowledge and Information",
  publisher = "Springer Berlin / Heidelberg",
}


@misc{006c7cb22b2049e69111270c9dc80eb0,
  title   = "Building web service ontologies",
  author  = "M. Sabou",
  note    = "Naam instelling promotie: VU Vrije Universiteit Naam instelling onderzoek: VU Vrije Universiteit",
  year    = "2006",
  school  = "Vrije Universiteit Amsterdam",
}


@inbook{f8169997e7ad44b59c25de04c6391b3a,
  title     = "Expertise-based peer selection",
  abstract  = "Peer-to-Peer systems have proven to be an effective way of sharing data. Finding the data in an efficient and robust manner still is a challenging problem. We propose a model in which peers advertise their expertise in the Peer-to-Peer network. The knowledge about the expertise of other peers forms a semantic overlay network (SON). Based on the semantic similarity between the subject of a query and the expertise of other peers, a peer can select appropriate peers to forward queries to, instead of broadcasting the query or sending it to a random set of peers. We evaluate the model in a bibliographic scenario, where peers share bibliographic descriptions of publications among each other. In simulation experiments complemented with a real-world field experiment we show how expertise based peer selection improves the performance of a Peer-to-Peer system with respect to precision, recall and the number of messages.",
  author    = "Ronny Siebes and Peter Haase and {Van Harmelen}, Frank",
  year      = "2006",
  doi       = "10.1007/3-540-28347-1_7",
  isbn      = "3540283463",
  pages     = "125--142",
  booktitle = "Semantic Web and Peer-to-Peer: Decentralized Management and Exchange of Knowledge and Information",
  publisher = "Springer Berlin / Heidelberg",
}


@article{29c60e65ac184520ae61b41b5302c78a,
  title     = "Exploiting the structure of background knowledge used in ontology matching",
  abstract  = "We investigate the use of a background knowledge ontology in ontology matching. We conducted experiments on matching two medical ontologies using a third extensive one as background knowledge, and compare the results with directly matching the two ontologies. Our results indicate that using background knowledge, in particular the exploitation of its structure, has enormous benefits on the matching. The structure of the background ontology needs closer examination to determine how to use it in order to obtain maximal benefit.",
  author    = "Zharko Aleksovski and {Ten Kate}, Warner and {Van Harmelen}, Frank",
  year      = "2006",
  volume    = "225",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@inbook{3f8e8b2550a542fe90820d10ce0ce4c2,
  title     = "From natural language to formal proof goal : Structured goal formalisation applied to medical guidelines",
  abstract  = "The main problem encountered when starting verification of goals for some formal system, is the ambiguity of those goals when they are specified in natural language. To verify goals given in natural language, a translation of those goals to the formalism of the verification tool is required. The main concern is to assure equivalence of the final translation and the original. A structured method is required to assure equivalence in every case. This article proposes a goal formalisation method in five steps, in which the domain expert is involved in such a way that the correctness of the result can be assured. The contribution of this article is a conceptual goal model, a formal expression language for this model, and a structured method which transforms any input goal to a fully formalised goal in the required target formalism. The proposed formalisation method guarantees essential properties like correctness, traceability, reduced variability and reusability.",
  author    = "S. Staab and V. Svatek and Ruud Stegers and {ten Teije}, Annette and {van Harmelen}, Frank",
  year      = "2006",
  doi       = "10.1007/11891451",
  isbn      = "3540463631",
  series    = "EKAW",
  publisher = "Springer",
  pages     = "51--58",
  editor    = "S Staab and V Svatek",
  booktitle = "EKAW",
}


@inbook{b9e4e042dd474445b581ce0311d52bd8,
  title     = "From natural language to formal proof goal* structured goal formalisation applied to medical guidelines (extended abstract)",
  abstract  = "The main problem encountered when starting verification of goals for some formal system, is the ambiguity of those goals when they are specified in natural language. To verify goals given in natural language, a translation of those goals to the formalism of the verification tool is required. The main concern is to assure equivalence of the final translation and the original. A structured method is required to assure equivalence in every case. This article proposes a goal formalisation method in five steps, in which the domain expert is involved in such a way that the correctness of the result can be assured. The contribution of this article is a conceptual goal model, a formal expression language for this model, and a structured method which transforms any input goal to a fully formalised goal in the required target formalism. The proposed formalisation method guarantees essential properties like correctness, traceability, reduced variability and reusability.",
  author    = "Ruud Stegers and Teije, {Annette Ten} and {Van Harmelen}, Frank",
  year      = "2006",
  isbn      = "3540463631",
  volume    = "4248 LNAI",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "51--58",
  booktitle = "Managing Knowledge in a World of Networks - 15th International Conference, EKAW 2006, Proceedings",
}


@book{349d90feb5a84c57874612916185947c,
  title     = "Fuzzy Logic and the Semantic Web",
  abstract  = "The publication Capturing Intelligence, Volume 1, focuses on research from all disciplines related to the issue of understanding and reproducing intelligent artificial systems. The volume presents a classical, rich and well-established area—namely the representation of fuzzy, noncrisp concepts, with a new and highly exciting challenge—namely the vision of the Semantic Web. The insight that any realistic approach to the Semantic Web will have to take into account the lessons from fuzzy logic approaches is gaining ground in a wide community. Higher precision of search engines, searching with semantic instead of syntactic queries, information integration among different web-resources, personalization, and semantic web services, are all part of the promises of this vision. The publication discusses a number of ways in which current Semantic Web technology can be “fuzzified,” and a number of ways in which such fuzzy representations can be used for tasks, such as search and information retrieval.",
  author    = "{van Harmelen}, Frank",
  year      = "2006",
  doi       = "10.1016/S1574-9576(06)80001-8",
  isbn      = "9780444519481",
  volume    = "1",
  publisher = "Elsevier",
}


@inbook{8e8a469a90534f2a99c45c6497232994,
  title     = "Incremental guideline formalization with tool support",
  abstract  = "Guideline formalization is recognized as an important component in improving computerized guidelines, which in turn leads to better informedness, lower inter-practician variability and, ultimately, to higher quality healthcare. By means of a modeling exercise, we investigate the role of guideline formalization tools which use two different knowledge transformation principles in producing re-usable knowledge objects useful for representing medical processes and performing updates of medical guidelines. We give a general evaluation of usefulness and state the main requirements for tools that reuse medical knowledge and support authoring of guidelines.",
  author    = "Radu Serban and Anna Puig-Centelles and {ten Teije}, Annette",
  year      = "2006",
  doi       = "10.1007/0-387-34224-9_13",
  isbn      = "0387342230",
  volume    = "204",
  series    = "IFIP International Federation for Information Processing",
  pages     = "106--118",
  booktitle = "Artificial Intelligence Applications and Innovations: 3rd IFIP Conference on Artificial Intelligence Applications and Innovations (AIAI) 2006",
}


@inbook{5c8046aa6f4542cfaf255e4fcc6c590c,
  title     = "Matching unstructured vocabularies using a background ontology",
  abstract  = "Existing ontology matching algorithms use a combination of lexical and structural correspondence between source and target ontologies. We present a realistic case-study where both types of overlap are low: matching two unstructured lists of vocabulary used to describe patients at Intensive Care Units in two different hospitals. We show that indeed existing matchers fail on our data. We then discuss the use of background knowledge in ontology matching problems. In particular, we discuss the case where the source and the target ontology are of poor semantics, such as flat lists, and where the background knowledge is of rich semantics, providing extensive descriptions of the properties of the concepts involved. We evaluate our results against a Gold Standard set of matches that we obtained from human experts.",
  author    = "Zharko Aleksovski and Michel Klein and {Ten Kate}, Warner and {Van Harmelen}, Frank",
  year      = "2006",
  doi       = "10.1007/11891451_18",
  isbn      = "3540463631",
  volume    = "4248 LNAI",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "182--197",
  booktitle = "Managing Knowledge in a World of Networks - 15th International Conference, EKAW 2006, Proceedings",
}


@inbook{ff0bc3281717488393369ce4d765b3e4,
  title     = "Meaning on the web: Evolution vs intelligent design?",
  abstract  = "It is a truism that as the Web grows in size and scope, it becomes harder to find what we want, to identify like-minded people and communities, to find the best ads to offer, and to have applications work together smoothly. Services don't interoperate; queries yield long lists of results, most of which seem to miss the point. If the Web were a person, we would expect richer and more successful interactions with it - interactions that were, quite literally, more meaningful. That's because in human discourse, it is shared meaning that gives us real communication. Yet with the current Web, meaning cannot be found.Much recent work has aspired to change this, both for human-machine interchange and machine-machine synchronization. Certainly the {"}semantic web{"} looks to add meaning to our current simplistic matching of mere strings of characters against mere {"}bags{"} of words. But can we legislate meaning from on high? Isn't meaning organic and determined by use, a moving and context-dependent target? But if meaning is an evolving organic soup, how are humans able to get anything done with one another? Don't we love to {"}define our terms{"}? But then again, is real definition even possible?These questions have daunted philosophers for years, and we probably won't solve them here. But we'll try to understand what's at the root of our own current religious debate: should meaning on the Web be evolutionary, driven organically through the bottom-up human assignment of tags? Or does it need to be carefully crafted and managed by a higher authority, using structured representations with defined semantics? Without picket signs or violence (we hope), our panelists will explore the two extreme ends of the spectrum - and several points in between.",
  author    = "Ron Brachman and Dan Connolly and Rohit Khare and Frank Smadja and {Van Harmelen}, Frank",
  year      = "2006",
  doi       = "10.1145/1135777.1135886",
  isbn      = "1595933239",
  pages     = "745",
  booktitle = "Proceedings of the 15th International Conference on World Wide Web",
}


@inbook{a5fc1e00cda54316b6db4dd2277e9f71,
  title     = "MultimediaN E-Culture Demonstrator",
  author    = "A.T. Schreiber and A. Amin and {van Assem}, M.F.J. and {de Boer}, V. and L. Hardman and M. Hildebrand and L. Hollink and Z. Huang and {van Kersen}, J. and {de Niet}, M. and B. Omelayenko and J.R. Ossenbruggen and R. Siebes and J. Taekema and J. Wielemaker and B.J. Wielinga",
  note      = "Schreiber06a Winner Semantic Web Challenge 2006",
  year      = "2006",
  series    = "LNCS",
  publisher = "Springer Verlag",
  pages     = "951--958",
  booktitle = "The Semnantic Web -- ISWC 2006, Athens, Georgia",
}


@inbook{6b853186f81545c598c481232c118f98,
  title     = "Ontology-based information visualization: Toward semantic web applications",
  abstract  = "This chapter has demonstrated an elegant way to visually represent ontological data. We have described how the Cluster Map visualization can use ontologies to create expressive information visualizations, with the attractive property that classes and objects that are semantically related are also spatially close in the visualization. Another key aspect of the visualization is that it focuses on visualizing instances rather than ontological models, thereby making it very useful for information retrieval purposes. A number of applications developed in the past few years have been described that prominently incorporate the Cluster Map visualization. Based on these descriptions, we could distinguish a number of generic information retrieval tasks that are well supported by the visualization. These applications prove the usability and usefulness of the Cluster Map in real-life scenarios. Furthermore, these applications show the applicability of the visualization in Semantic Web-based environments, where lightweight ontologies are playing a crucial role in organizing and accessing heterogeneous and decentralized information sources.",
  author    = "Christiaan Fluit and Marta Sabou and {Van Harmelen}, Frank",
  year      = "2006",
  doi       = "10.1007/1-84628-290-X_3",
  isbn      = "1852339764",
  pages     = "45--58",
  booktitle = "Visualizing the Semantic Web: XML-Based Internet and Information Visualization",
  publisher = "Springer London",
}


@inbook{82e49ce7bc05475ebaa9832d22a040e3,
  title     = "Ontology-Based Information Visualization: Toward Semantic Web Applications",
  abstract  = "The Semantic Web is an extension of the current World Wide Web, based on the idea of exchanging information with explicit, formal, and machine-accessible descriptions of meaning. Providing information with such semantics will enable the construction of applications that have an increased awareness of what is contained in the information they process and that can therefore operate more accurately. This has the potential of improving the way we deal with information in the broadest sense possible, for example, better search engines, mobile agents for various tasks, or even applications yet unheard of. Rather than being merely a vision, the Semantic Web has significant backing from various institutes such as DARPA, the European Union, and the W3C, all of which have performed a variety of Semantic Web activities. In order to be able to exchange the semantics of information, one first needs to agree on how to explicitly model it. Ontologies are a mechanism for representing such formal and shared domain descriptions. They can be used to annotate data with labels (metadata) indicating their meaning, thereby making their semantics explicit and machine-accessible. Many Semantic Web initiatives emphasize the capability of machines to exchange the meaning of information. Although their efforts will lead to an increased quality of the application's results, their user interfaces often take little or no advantage of the increased semantics. For example, an ontology-based search engine could use its ontology to enrich the presentation of the resulting list to the end user, for example, by replacing the endless list of hits with a navigation structure based on the semantics of the hits. Visualization is becoming increasingly important in Semantic Web tools. In par-ticular, visualization is used in tools that support the development of ontologies, such as ontology extraction tools (OntoLift, Text-to-Onto) or ontology editors (Protégé, OntoLift). The intended users of these tools are ontology engineers that need to gain an insight into the complexity of the ontology. Therefore, these tools employ schema visualization techniques that primarily focus on the structure of the ontology (i.e., its concepts and their relationships). We presented a detailed overview of these tools in Fluit et al. (2003). The Cluster Map visualization technique, developed by the Dutch software vendor Aduna (http://aduna.biz), bridges the gap between complex semantic structures and 45 46 Visualizing the Semantic Web their simple, intuitive user-oriented presentation. It presents semantic data to end users who want to leverage the benefits of Semantic Web technology without being burdened with the complexity of the underlying metadata. For end users, instance information is often more important than the structure of the ontology that is used to describe these instances. Accordingly, the Cluster Map technique focuses on visualizing instances and their classifications according to the concepts of the ontology. We have reported in previous work (Fluit et al., 2002; 2003) on case studies that exploit the expressive power of this technique. Since then, the growth of the Semantic Web has made it possible to take this technology a step further and integrate it in three different applications. Two of them are employed within Semantic Web research projects. The third is a commercial information retrieval application. These appli-cations exhibit the characteristics of a typical Semantic Web tool: they provide easy (visual) access to a set of heterogeneous, distributed data sources and rely on Semantic Web encoding languages and storage facilities for the manipulation of the visualized data. This chapter is centered on the description of these three applications. First, we will explain the contents of the Cluster Map visualization and the kind of ontologies it visualizes in Section 3.2. Section 3.3 presents the three real-life applications that incorporate the visualization. These two sections lead to a discussion in Section 3.4 on how the visualization can support several user tasks, such as analysis, search, and exploration. Some considerations for future work and a summary conclude this chapter. 3.2 Cluster Map Basics",
  author    = "Christiaan Fluit and Marta Sabou and Harmelen, {Frank van}",
  year      = "2006",
  doi       = "10.1109/IV.2001.942109",
  isbn      = "0-7695-1195-3",
  series    = "Visualizing the Semantic Web",
  pages     = "45--58",
  booktitle = "Visualizing the Semantic Web",
}


@inbook{7be36f1ce0cd45189c97468ea5ffb6ed,
  title     = "Peer-to -peer and semantic web",
  abstract  = "Just as the industrial society of the last century depended on natural resources, today's society depends on information. A lack of resources in the industrial society hindered development just as a lack of information hinders development in the information society. Consequently, the exchange of information becomes essential for more and more areas of society: Companies announce their products in online marketplaces and exchange electronic orders with their suppliers; in the medical area patient information is exchanged between general practitioners, hospitals and health insurances; public administration receive tax information from employers and offer online services to their citizens. As a reply to this increasing importance of information exchange, new technologies supporting a fast and accurate information exchange are being developed. Prominent examples of such new technologies are so-called Semantic Web and Peer-to-Peer technologies. These technologies address different aspects of the inherit complexity of information exchange. Semantic Web Technologies address the problem of information complexity by providing advanced support for representing and processing information. Peer-to-Peer technologies, on the other hand, address system complexity by allowing flexible and decentralized information storage and processing.",
  author    = "Heiner Stuckenschmidt and {Van Harmelen}, Frank and Wolf Siberski and Steffen Staab",
  year      = "2006",
  doi       = "10.1007/3-540-28347-1_1",
  isbn      = "3540283463",
  pages     = "1--17",
  booktitle = "Semantic Web and Peer-to-Peer: Decentralized Management and Exchange of Knowledge and Information",
  publisher = "Springer Berlin / Heidelberg",
}


@inbook{5e8f12c030ee4ef3b38f85a725924c7e,
  title     = "Semantic Web research anno 2006: Main streams, popular fallacies, current status and future challenges",
  abstract  = "In this topical paper we try to give an analysis and overview of the current state of Semantic Web research. We point to different interpretations of the Semantic Web as the reason underlying many controversies, we list (and debunk) four false objections which are often raised against the Semantic Web effort. We discuss the current status of the Semantic Web work by reviewing the current answers to four central research questions that need to be answered, and by surveying the uptake of Semantic Web technology in different application areas. Finally, we try to identify the main challenges facing the Semantic Web community.",
  author    = "{Van Harmelen}, Frank",
  year      = "2006",
  isbn      = "354038569X",
  volume    = "4149 LNAI",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "1--7",
  booktitle = "Cooperative Information Agents X - 10th International Workshop, CIA 2006. Proceedings",
}


@inbook{c98302a518384013910c9263c39e6d7a,
  title     = "Where does it break? or: Why the semantic web is not just {"}research as usual{"}",
  abstract  = "Work on the Semantic Web is all too often phrased as a technological challenge: how to improve the precision of search engines, how to personalise web-sites, how to integrate weakly-structured data-sources, etc. This suggests that we will be able to realise the Semantic Web by merely applying (and at most refining) the results that are already available from many branches of Computer Science. I will argue in this talk that instead of (just) a technological challenge, the Semantic Web forces us to rethink the foundations of many subfields of Computer Science. This is certainly true for my own field (Knowledge Representation), where the challenge of the Semantic Web continues to break many often silently held and shared assumptions underlying decades of research. With some caution, I claim that this is also true for other fields, such as Machine Learning, Natural Language Processing, Databases, and others. For each of these fields, I will try to identify silently held assumptions which are no longer true on the Semantic Web, prompting a radical rethink of many past results from these fields.",
  author    = "{Van Harmelen}, Frank",
  year      = "2006",
  isbn      = "3540345442",
  volume    = "4011 LNCS",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "1",
  booktitle = "The Semantic Web: Research and Applications: 3rd European Semantic Web Conference, ESWC 2006 Proceedings",
}


@article{4b5ef60013684a3e8d86c727ef4034d9,
  title     = "Semantic Web Challenge 2004",
  author    = "Michel Klein and Ubbo Visser",
  year      = "2005",
  month     = "10",
  doi       = "10.1016/j.websem.2005.05.007",
  volume    = "3",
  pages     = "209--210",
  journal   = "Journal of Web Semantics",
  issn      = "1570-8268",
  publisher = "Elsevier",
  number    = "2-3",
}


@article{48688278661c4d0db7292bfcf5fb7ce6,
  title     = "A quantitative analysis of the robustness of knowledge-based systems through degradation studies",
  abstract  = "The overall aim of this paper is to provide a general setting for quantitative quality measures of knowledge-based system behaviour that is widely applicable to many knowledge-based systems. We propose a general approach that we call degradation studies: an analysis of how system output changes as a function of degrading system input, such as incomplete or incorrect data or knowledge. To show the feasibility of our approach, we have applied it in a case study. We have taken a large and realistic vegetation-classification system, and have analysed its behaviour under various varieties of incomplete and incorrect input. This case study shows that degradation studies can reveal interesting and surprising properties of the system under study.",
  keywords  = "Knowledge-based systems, Quantitative analysis, Robust behaviour, Validation",
  author    = "Perry Groot and {ten Teije}, Annette and {van Harmelen}, Frank",
  year      = "2005",
  month     = "2",
  doi       = "10.1007/s10115-003-0140-7",
  volume    = "7",
  pages     = "224--245",
  journal   = "Knowledge and Information Systems",
  issn      = "0219-1377",
  publisher = "Springer London",
  number    = "2",
}


@inbook{c1cba8093b6c4b39a1e7594aacf502af,
  title     = "A framework for handling inconsistency in changing ontologies",
  abstract  = "One of the major problems of large scale, distributed and evolving ontologies is the potential introduction of inconsistencies. In this paper we survey four different approaches to handling inconsistency in DL-based ontologies: consistent ontology evolution, repairing inconsistencies, reasoning in the presence of inconsistencies and multi-version reasoning. We present a common formal basis for all of them, and use this common basis to compare these approaches. We discuss the different requirements for each of these methods, the conditions under which each of them is applicable, the knowledge requirements of the various methods, and the different usage scenarios to which they would apply.",
  author    = "Peter Haase and {Van Harmelen}, Frank and Zhisheng Huang and Heiner Stuckenschmidt and York Sure",
  year      = "2005",
  doi       = "10.1007/11574620_27",
  isbn      = "3540297545",
  volume    = "3729 LNCS",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  pages     = "353--367",
  booktitle = "The Semantic Web, ISWC 2005 - 4th International Semantic Web Conference, ISWC 2005, Proceedings",
}


@inbook{3eac575159114d188eaedeaa950d2788,
  title     = "Composing Web Services using an Agent Factory",
  author    = "D.J. Richards and {van Splunter}, S. and F.M. Brazier and M. Sabou",
  note      = "Richards.ea:05 Previously published in: Proceedings of AAMAS Workshop on Web Services and Agent-Based Engineering(WSABE), Melbourne, Australia, 2003, pp. 57-66",
  year      = "2005",
  series    = "Multiagent Systems, Artificial Societies, and Simulated Organiza",
  publisher = "Springer",
  number    = "13",
  pages     = "229--252",
  booktitle = "Unknown",
}


@inbook{204e70ca3e7e4cd0810a5437326e3cc3,
  title     = "Design patterns for modelling guidelines",
  abstract  = "It is by now widely accepted that medical guidelines can help to significantly improve the quality of medical care. Unfortunately, constructing the required medical guidelines is a very labour intensive and costly process. The cost of guideline construction would decrease if guidelines could be built, From a set of building blocks that can be reused across guidelines. Such reusable building blocks would also result in more standardised guidelines, facilitating their deployment. The goal of this paper is to identify a collection of patterns that can be used as guideline building blocks. We propose two different methods for finding such patterns We compare the collections of patterns obtained through these two methods, and experimentally validate some of the patterns by checking their usability in the actual modelling of a medical guideline for breastcancer treatment.",
  author    = "Radu Serban and {Ten Teije}, Annette and Mar Marcos and Cristina Polo-Conde and Rosenbrand, {Kitty C J G M} and Jolanda Wittenberg and {van Croonenborg}, Joyce",
  year      = "2005",
  isbn      = "3540278311",
  volume    = "3581 LNAI",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  pages     = "121--125",
  booktitle = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
}


@inbook{c0344399fef54f54882db3a71064640f,
  title     = "Formalising medical quality indicators to improve guidelines",
  abstract  = "Medical guidelines can significantly improve quality of medical care and reduce costs. But how do we get sound and well-structured guidelines? This paper investigates the use of quality indicators that are formulated by medical institutions to evaluate medical care. The main research questions are (i) whether it is possible to formalise those indicators in a specific knowledge representation language for medical guidelines, and (ii) whether it is possible to verify whether such guidelines do indeed satisfy these indicators. In a case study on two real-life guidelines (Diabetes and Jaundice) we have studied 35 indicators, that were developped independently from these guidelines. Of these 25 (71%!) suggested anomalies in one of the guidelines in our case study.",
  author    = "{Van Gendt}, Marjolein and {Ten Teije}, Annette and Radu Serban and {Van Harmelen}, Frank",
  year      = "2005",
  isbn      = "3540278311",
  volume    = "3581 LNAI",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  pages     = "201--210",
  booktitle = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
}


@inbook{bc56cc474e5740f99e47d0cd22d404be,
  title     = "Introduction to Semantic Web Ontology Languages",
  abstract  = "The aim of this chapter is to give a general introduction to some of the ontology languages that play a prominent role on the Semantic Web, and to discuss the formal foundations of these languages. Web ontology languages will be the main carriers of the information that we will want to share and integrate.",
  author    = "Grigoris Antoniou and Enrico Franconi and {Van Harmelen}, Frank",
  year      = "2005",
  doi       = "10.1007/11526988_1",
  isbn      = "3540278281",
  series    = "Reasoning Web",
  pages     = "1–21",
  booktitle = "Reasoning Web",
}


@article{2416a0dc6c8b4b078331ca2654b6716a,
  title   = "Ontology-driven extraction of linguistic patterns for modelling clinical guidelines",
  author  = "Radu Serban and {Ten Teije}, Annette and {Van Harmelen}, Frank and Mar Marcos and Cristina Polo-Conde",
  year    = "2005",
  pages   = "381--382",
  journal = "Belgian/Netherlands Artificial Intelligence Conference",
  issn    = "1568-7805",
}


@inbook{4c48cb1e8d0c41189180f415718623c3,
  title     = "Ontology-driven extraction of linguistic patterns for modelling clinical guidelines",
  abstract  = "Evidence-based clinical guidelines require frequent updates duo to research and technology advances. The quality of guideline updates can be improved if the knowledge underlying the guideline text is explicitly modelled using the so-called guideline patterns (GPs), mappings between a text fragment arid a formal representation of its corresponding medical knowledge. Ontology-driven extraction of linguistic patterns is a method to automatically reconstruct the control knowledge captured in guidelines, which facilitates a more effective modelling and authoring of clinical guidelines. We illustrate by examples the use of a method for generating and searching for linguistic guideline patterns in the text of a guideline for treatment of breast cancer, and provide a general evaluation of usefulness of these patterns in the modelling of the guideline analyzed.",
  author    = "Radu Serban and Teije, {Annette Ten} and {Van Harmelen}, Frank and Mar Marcos and Cristina Polo-Conde",
  year      = "2005",
  isbn      = "3540278311",
  volume    = "3581 LNAI",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  pages     = "191--200",
  booktitle = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
}


@inbook{a0c3c45a88394f33837eef6806985fda,
  title     = "Ontology mapping: A way out of the medical tower of babel?",
  author    = "{Van Harmelen}, Frank",
  year      = "2005",
  isbn      = "3540278311",
  volume    = "3581 LNAI",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  pages     = "3--6",
  booktitle = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
}


@inbook{830d342a93164dff8e224ed01f214351,
  title     = "Ontology mapping using background knowledge",
  abstract  = "In this paper, we report on a method for aligning two lists of terms using structure-rich ontologies as background knowledge. The results of the method can be seen as suggested mapping candidates to users that perform an ontology alignment task. We applied the method on lists of medical terms, and we discuss the outcome.",
  keywords  = "Ontology, Ontology alignment, Semantic web",
  author    = "Zharko Aleksovski and Michel Klein",
  year      = "2005",
  doi       = "10.1145/1088622.1088655",
  isbn      = "1595931635",
  pages     = "175--176",
  booktitle = "Proceedings of the 3rd International Conference on Knowledge Capture, K-CAP'05",
}


@inbook{d1b1ca8ecf1e4b0881d54557d518f794,
  title     = "Query Processing in Ontology-Based Peer-to-Peer Systems",
  abstract  = "The unstructured, heterogeneous and dynamic nature of the Web poses a new challenge to query-answering over multiple data sources. The so-called Semantic Web aims at providing more and semantically richer structures in terms of ontologies and meta-data. A problem that remains is the combined use of heterogeneous sources. In a dynamic environment, it is no longer realistic to assume that the involved data sources act as if they were a single (virtual) source, modelled as a global schema, as is done in classical data integration approaches. In this paper, we propose an alternative approach where we replace the role of a single virtual data source schema with a peer-to-peer approach relying on limited shared (or: overlapping) vocabularies between peers. Since overlaps between vocabularies of peers will be limited and the dynamic nature of the system prohibits the design of accurate mappings, query processing will have to be approximate. We provide a formal model for such approximate query processing based on limited shared vocabularies between peers, and we show how the quality of the approximation can be adjusted in a gradual manner. The result is a flexible architecture for query-processing in heterogenous and dynamic environments, based on a formal foundation. We present the approach and discuss it on the basis of a case study.",
  keywords  = "knowledge sharing, knowledge-based mediation architectures, methods and formalisms for, semantic web",
  author    = "Heiner Stuckenschmidt and Harmelen, {Frank Van} and Fausto Giunchiglia",
  year      = "2005",
  doi       = "10.1007/3-7643-7361-X_7",
  isbn      = "978-3-7643-7361-0",
  series    = "Ontologies for Agents Theory and Experiences",
  pages     = "145--167",
  booktitle = "Ontologies for Agents Theory and Experiences",
}


@article{691b809a87584cfb83812192f176a047,
  title    = "Reasoning with inconsistent ontologies",
  abstract = "In this paper we present a framework of reasoning with inconsistent ontologies, in which pre-defined selection functions are used to deal with concept relevance. We examine how the notion of {"}concept relevance{"} can be used for reasoning with inconsistent ontologies. We have implemented a prototype called PION (Processing Inconsistent ONtologies), which is based on a syntactic relevance-based selection function. In this paper, we also report the experiments with PION.",
  author   = "Zhisheng Huang and {Van Harmelen}, Frank and {Ten Teije}, Annette",
  year     = "2005",
  pages    = "454--459",
  journal  = "IJCAI International Joint Conference on Artificial Intelligence",
  issn     = "1045-0823",
}


@article{5fe3535446c2476fb38dfb7992e6a912,
  title     = "Semantics-based publication management using RSS and FOAF",
  abstract  = "Listing references to scientific publications on personal or group homepages is a common practice. Doing this in a consistent and structured manner either requires a lot of discipline or a centralized database. Scientific publication, however, is a distributed activity by nature. We present a completely distributed and RDF-based implementation for disseminating references to scientific publications. Our application only uses existing information sources and allows for different output formats, e.g. HTML, RSS and RDF.",
  author    = "Peter Mika and Michel Klein and Radu Serban",
  year      = "2005",
  volume    = "175",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@misc{2a85685b18cb4f0ab6abd1292648a8ae,
  title   = "Storage, Querying and Inferencing for Semantic Web Languages",
  author  = "J. Broekstra",
  note    = "Naam instelling promotie: VU Vrije Universiteit Naam instelling onderzoek: TU Eindhoven",
  year    = "2005",
  school  = "Vrije Universiteit Amsterdam",
}


@inbook{a06b18d7a38648cb868ea9081222dfef,
  title     = "Using lexical and logical methods for the alignment of medical terminologies",
  abstract  = "Standardized medical terminologies are often used for the registration of patient data. In several situations there is a need to align these terminologies to other terminologies. Even when the terminologies cover the same domain, this is often a non-trivial task. The task is even more complicated when the terminology docs not contain much structure. In this paper we describe the initial results of a procedure for mapping a terminology with little or no structure to a structure-rich terminology. This procedure uses the knowledge of the structure-rich terminology and a method for semantic explication of concept descriptions. The first results shows that, when compared to approaches based on syntactic analysis only, the recall can be greatly improved without sacrificing much of the precision.",
  author    = "Michel Klein and Zharko Aleksovski",
  year      = "2005",
  doi       = "10.1007/11527770_35",
  isbn      = "3540278311",
  volume    = "3581 LNAI",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  pages     = "241--245",
  booktitle = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
}


@article{bf6b51361ff04c95bfd87977043a9ef8,
  title     = "Bibster-a semantics-based bibliographic peer-to-peer system",
  abstract  = "This paper describes Bibster, a Peer-to-Peer system for exchanging bibliographic metadata among researchers. We show how Bibster exploits ontologies in data-representation, query formulation, query routing, and query result presentation. The Bibster system is freely available and is used by researchers across multiple organizations.",
  keywords  = "Knowledge management, Peer-to-peer, Semantic web ontologies",
  author    = "Peter Haase and Björn Schnizler and Jeen Broekstra and Marc Ehrig and {Van Harmelen}, Frank and Maarten Menken and Peter Mika and Michal Plechawski and Pawel Pyszlak and Ronny Siebes and Steffen Staab and Christoph Tempich",
  year      = "2004",
  month     = "12",
  doi       = "10.1016/j.websem.2004.09.006",
  volume    = "2",
  pages     = "99--103",
  journal   = "Journal of Web Semantics",
  issn      = "1570-8268",
  publisher = "Elsevier",
  number    = "1",
}


@article{ef5224f999734bafbf57098c8ef44c8e,
  title     = "Contextualizing ontologies",
  abstract  = "Ontologies are shared models of a domain that encode a view which is common to a set of different parties. Contexts are local models that encode a party's subjective view of a domain. In this paper, we show how ontologies can be contextualized, thus acquiring certain useful properties that a pure shared approach cannot provide. We say that an ontology is contextualized or, also, that it is a contextual ontology, when its contents are kept local, and therefore not shared with other ontologies, and mapped with the contents of other ontologies via explicit (context) mappings. The result is Context OWL (C-OWL), a language whose syntax and semantics have been obtained by extending the OWL syntax and semantics to allow for the representation of contextual ontologies.",
  keywords  = "Compatibilities, Context OWL, Contextual ontology",
  author    = "Paolo Bouquet and Fausto Giunchiglia and {Van Harmelen}, Frank and Luciano Serafini and Heiner Stuckenschmidt",
  year      = "2004",
  month     = "10",
  doi       = "10.1016/j.websem.2004.07.001",
  volume    = "1",
  pages     = "325--343",
  journal   = "Journal of Web Semantics",
  issn      = "1570-8268",
  publisher = "Elsevier",
  number    = "4",
}


@article{5c4909d3d0af4ce2b8e1e705ffc371b8,
  title     = "Generating and managing metadata for Web-based information systems",
  abstract  = "As metadata become of increasing importance to the Web, we will need to start managing such metadata. We argue that there is a strong need for metadata management. We introduce the Spectacle Workbench for verifying semi-structured information and show how it can be used to validate, aggregate and visualize the metadata of an existing information system. We conclude that the possibility to verify and aggregate metadata is an added value with respect to contents-based access to information and that it is possible to generate it on the basis of Web contents. ?? 2004 Elsevier B.V. All rights reserved.",
  keywords  = "Content-analysis, Knowledge-based methods, Metadata management",
  author    = "Heiner Stuckenschmidt and {Van Harmelen}, Frank",
  year      = "2004",
  month     = "8",
  doi       = "10.1016/j.knosys.2004.04.004",
  volume    = "17",
  pages     = "201--206",
  journal   = "Knowledge-Based Systems",
  issn      = "0950-7051",
  publisher = "Elsevier",
  number    = "5-6",
}


@inbook{2efa187b77bf4226bb5ffbc2ff805910,
  title     = "Towards Automatic Partitioning of Class Hierarchies",
  abstract  = "The increasing awareness of the benefits of ontologies for information processing has lead to the creation of a number of large ontologies about real world domains. The size of these ontologies and their monolithic character cause serious problems in handling them. In other areas, e.g. software engineering, these problems are tackled by partitioning monolithic entities into sets of meaningful and mostly self-contained modules. In this paper, we suggest a similar approach for ontologies. We propose propose an approach for automatically partitioning large ontologies into smaller modules based on the structure of the class hierarchy. The method is demonstrated on a part of the UMLS semantic network. Experiments with larger ontologies are available online at http://swserver.cs.vu.nl/partitioning/.",
  author    = "Heiner Stuckenschmidt and Michel Klein",
  note      = "See \http://swserver.cs.vu.nl/partitioning/",
  year      = "2004",
  month     = "7",
  booktitle = "Proceedings of the 1st International Conference on Knowledge Management and Decision Support (ICKMDS'04)",
}


@article{5b84ef08d70b4ef484d5260fb246b88a,
  title     = "The Semantic Web: What, why, how, and when",
  abstract  = "The Semantic Web aims to enrich the Web with a layer of machine-interpretable metadata so that computer programs can predictably derive new information. This goal will require the development of metadata syntax and vocabularies, and the creation of metadata for lots of Web pages.",
  keywords  = "Data integration, Intelligent support, Semantic Web",
  author    = "{Van Harmelen}, Frank",
  year      = "2004",
  month     = "3",
  volume    = "5",
  journal   = "Distributed Systems Online",
  issn      = "1541-4922",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  number    = "3",
}


@article{29b9ba977f0d4002be3128941590021e,
  title     = "A tool for gene expression based PubMed search through combining data sources.",
  abstract  = "We present a new tool for the semi-automated querying of PubMed using a batch of tens to thousands of GenBank accession numbers or UniGene cluster ids. By combining information from UniGene and SWISS-PROT, microGENIE obtains information on the biological relevance of expressed genes, as identified by micro-array experiments, with minimal user intervention and time investment. © Oxford University Press 2004; all rights reserved.",
  author    = "M. Korotkiy and R.A. Middelburg and H. Dekker and {van Harmelen}, F.A.H. and J. Lankelma",
  year      = "2004",
  doi       = "10.1093/bioinformatics/bth183",
  volume    = "20",
  pages     = "1980--1982",
  journal   = "Bioinformatics",
  issn      = "1367-4803",
  publisher = "Oxford University Press",
  number    = "12",
}


@article{d6c8740d8ca54972a7d6a18fbd06255e,
  title     = "Configuration of Web services as parametric design",
  abstract  = "The configuration of Web services is particularly hard given the heterogeneous, unreliable and open nature of the Web. Furthermore, such composite Web services are likely to be complex services, that will require adaptation for each specific use. Current approaches to Web service configuration are often based on pre/post-condition-style reasoning, resulting in a planning-style approach to service configuration, configuring a composite web service {"}from scratch{"} every time. In this paper, we propose instead a knowledge-intensive brokering approach to the creation of composite Web services. In our approach, we describe a complex Web service as a fixed template, which must be configured for each specific use. Web service configuration can then be regarded as parametric design, in which the parameters of the fixed template have to be instantiated with appropriate component services. During the configuration process, we exploit detailed knowledge about the template and the components, to obtain the required composite web service. We illustrate our proposal by applying it to a specific family of Web services, namely {"}heuristic classification services{"}. We have implemented a proto-type of our knowledge-intensive broker and describe its execution in a concrete scenario.",
  author    = "{Ten Teije}, Annette and {Van Harmelen}, Frank and Bob Wielinga",
  year      = "2004",
  volume    = "3257",
  pages     = "321--336",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@article{fb3d79a95a754d18bf359662e52be6c8,
  title     = "Exploring Large Document Repositories with RDF Technology: The DOPE Project",
  author    = "Heiner Stuckenschmidt and {Van Harmelen}, Frank and {De Waard}, Anita and Scerri Tony and Ravinder Bhogal and {Van Buel}, Jan and Ian Crowlesmith and Christiaan Fluit and Arjohn Kampman and Jeen Broekstra and {van Mulligen}, Erik",
  year      = "2004",
  doi       = "10.1109/MIS.2004.9",
  volume    = "19",
  pages     = "34--40",
  journal   = "IEEE Intelligent Systems",
  issn      = "1541-1672",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  number    = "3",
}


@inbook{93c27f7f2bc24f96a2ee217a3f5eca7e,
  title     = "OWL Web Ontology Language",
  abstract  = "The OWL Web Ontology Language is designed for use by applications that need to process the content of information instead of just presenting information to humans. OWL facilitates greater machine interpretability of Web content than that supported by XML, RDF, and RDF Schema (RDF-S) by providing additional vocabulary along with a formal semantics. OWL has three increasingly-expressive sublanguages: OWL Lite, OWL DL, and OWL Full. This document is written for readers who want a first impression of the capabilities of OWL. It provides an introduction to OWL by informally describing the features of each of the sublanguages of OWL. Some knowledge of RDF Schema is useful for understanding this document, but not essential. After this document, interested readers may turn to the OWL Guide for more detailed descriptions and extensive examples on the features of OWL. The normative formal definition of OWL can be found in the OWL Semantics and Abstract Syntax.",
  author    = "S. Staab and R. Studer and Grigoris Antoniou and {Van Harmelen}, Frank",
  year      = "2004",
  doi       = "10.1145/1295289.1295290",
  isbn      = "9781605660264",
  series    = "Ubiquity",
  publisher = "Springer",
  pages     = "1--1",
  editor    = "S Staab and R Studer",
  booktitle = "Ubiquity",
}


@book{1cbe43aedce045e8ab7d79f1a0b8bd41,
  title     = "OWL Web Ontology Language Reference",
  author    = "M. Dean and A.T. Schreiber and S. Bechofer and {van Harmelen}, F.A.H. and J. Hendler and I. Horrocks and D. MacGuinness and P. Patel-Schneider and Stein, {L. A.}",
  note      = "Dean04a",
  year      = "2004",
  publisher = "World Wide Web Consortium",
}


@article{3af4f0f0c17a4dd599da2009622732a4,
  title    = "OWL Web Ontology Language - Reference",
  abstract = "The {W}eb {O}ntology {L}anguage {OWL} is a semantic markup language for publishing and sharing ontologies on the {W}orld {W}ide {W}eb. {OWL} is developed as a vocabulary extension of {RDF} (the {R}esource {D}escription {F}ramework) and is derived from the {DAML}+{OIL} {W}eb {O}ntology {L}anguage. {T}his document contains a structured informal description of the full set of {OWL} language constructs and is meant to serve as a reference for {OWL} users who want to construct {OWL} ontologies.",
  keywords = "OWL Spec",
  author   = "McGuinness, {Deborah L} and Harmelen, {Frank van}",
  year     = "2004",
  pages    = "1--53, [Accessed: 02 February 2016]",
  journal  = "W3C Recommendation [Online], Available at: http://www.w3.org/TR/2004/REC-owl",
}


@article{cdb20c7e491b4996b8e1352c48fe9d9e,
  title     = "Peer selection in peer-to-peer networks with semantic topologies",
  abstract  = "Peer-to-Peer systems have proven to be an effective way of sharing data. Modern protocols are able to efficiently route a message to a given peer. However, determining the destination peer in the first place is not always trivial. We propose a model in which peers advertise their expertise in the Peer-to-Peer network. The knowledge about the expertise of other peers forms a semantic topology. Based on the semantic similarity between the subject of a query and the expertise of other peers, a peer can select appropriate peers to forward queries to, instead of broadcasting the query or sending it to a random set of peers. To calculate our semantic similarity measure we make the simplifying assumption that the peers share the same ontology. We evaluate the model in a bibliographic scenario, where peers share bibliographic descriptions of publications among each other. In simulation experiments we show how expertise based peer selection improves the performance of a Peer-to-Peer system with respect to precision, recall and the number of messages.",
  author    = "Peter Haase and Ronny Siebes and {Van Harmelen}, Frank",
  year      = "2004",
  volume    = "3226",
  pages     = "108--125",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@inbook{e22328f7b53b47eb93625bfeff139c61,
  title     = "Peer Selection in Peer-to-Peer Networks with Semantic Topologies",
  abstract  = "Peer-to-Peer systems have proven to be an effective way of sharing data. Modern protocols are able to efficiently route a message to a given peer. However, determining the destination peer in the first place is not always trivial.\nWe propose a model in which peers advertise their expertise in the Peer-to-Peer network. The knowledge about the expertise of other peers forms a semantic topology. Based on the semantic similarity between the subject of a query and the expertise of other peers, a peer can select appropriate peers to forward queries to, instead of broadcasting the query or sending it to a random set of peers. To calculate our semantic similarity measure we make the simplifying assumption that the peers share the same ontology. We evaluate the model in a bibliographic scenario, where peers share bibliographic descriptions of publications among each other. In simulation experiments we show how expertise based peer selection improves the performance of a Peer-to-Peer system with respect to precision, recall and the number of messages.",
  author    = "Peter Haase and Ronny Siebes and {van Harmelen}, Frank",
  year      = "2004",
  doi       = "10.1007-978-3-540-30145-5_7",
  isbn      = "978-3-540-23609-2",
  series    = "Proceedings of the International Conference on Semantics in a Networked World ({ICNSW'04})",
  pages     = "108--125",
  booktitle = "Proceedings of the International Conference on Semantics in a Networked World ({ICNSW'04})",
}


@article{bd30f9749bc0415aa960a33637b93d1a,
  title     = "Protocure: supporting the development of medical protocols through formal methods",
  abstract  = "Medical guidelines and protocols describe the optimal care for a specific group of patients and therefore, when properly applied, improve the quality of patient care. During the last decade, a large number of medical guidelines and protocols have been published. However, the work done on developing and disseminating them far outweighs the efforts on guaranteeing their quality. Indeed, anomalies like ambiguity and incompleteness are frequent in medical guidelines and protocols. An approach grounded on a formal representation, can answer these needs, as we have demonstrated in the Protocure project'. The Protocure II project will aim at integrating formal methods in the life cycle of guidelines.",
  keywords  = "Clinical Protocols, Decision Support Techniques, Evidence-Based Medicine, Humans, Planning Techniques, Practice Guidelines as Topic, Programming Languages, Software, Journal Article, Research Support, Non-U.S. Gov't",
  author    = "Michael Balser and Oscar Coltell and {van Croonenborg}, Joyce and Christoph Duelli and {van Harmelen}, Frank and Albert Jovell and Peter Lucas and Mar Marcos and Silvia Miksch and Wolfgang Reif and Rosenbrand, {Kitty C J G M} and Andreas Seyfang and {ten Teije}, Annette",
  year      = "2004",
  volume    = "101",
  pages     = "103--107",
  journal   = "Studies in Health Technology and Informatics",
  issn      = "0926-9630",
  publisher = "IOS Press",
}


@inbook{32287d7d1b274cfebe15445a6dfebf85,
  title     = "Reaching diagnostic agreement in multi-agent diagnosis",
  abstract  = "We consider the problem of finding a commonly agreed upon diagnosis for errors observed in a system monitored by a number of different agents. Each agent is assumed to have its own specialized (expert) view on the system. Collectively, the agents have to agree on one or more diagnoses based on their views. Reaching an agreement is complicated by the fact that the knowledge of different specialists need not always be correct.",
  author    = "Nico Roos and {Ten Teije}, Annette and Cees Witteveen",
  year      = "2004",
  isbn      = "1581138644",
  volume    = "3",
  pages     = "1256--1257",
  editor    = "N.R. Jennings and C. Sierra and L. Sonenberg and M. Tambe",
  booktitle = "Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS 2004",
}


@book{98baea1ecbb34018b717b1de16de5a5b,
  title     = "Semantic Web Primer",
  abstract  = "The development of the Semantic Web, with machine-readable content, has the potential to revolutionize the World Wide Web and its use. A Semantic Web Primer provides an introduction and guide to this still emerging field, describing its key ideas, languages, and technologies. Suitable for use as a textbook or for self-study by professionals, it concentrates on undergraduate-level fundamental concepts and techniques that will enable readers to proceed with building applications on their own and includes exercises, project descriptions, and annotated references to relevant online materials. A Semantic Web Primer provides a systematic treatment of the different languages (XML, RDF, OWL, and rules) and technologies (explicit metadata, ontologies, and logic and inference) that are central to Semantic Web development as well as such crucial related topics as ontology engineering and application scenarios. This substantially revised and updated second edition reflects recent developments in the field, covering new application areas and tools. The new material includes a discussion of such topics as SPARQL as the RDF query language; OWL DLP and its interesting practical and theoretical properties; the SWRL language (in the chapter on rules); OWL-S (on which the discussion of Web services is now based). The new final chapter considers the state of the art of the field today, captures ongoing discussions, and outlines the most challenging issues facing the Semantic Web in the future. Supplementary materials, including slides, online versions of many of the code fragments in the book, and links to further reading, can be found at http://www.semanticwebprimer.org.Grigoris Antoniou is Professor at the Institute for Computer Science, FORTH (Foundation for Research and Technology-Hellas), Heraklion, Greece. Frank van Harmelen is Professor in the Department of Artificial Intelligence at the Vrije Universiteit, Amsterdam, the Netherlands.",
  author    = "Grigoris Antoniou and Harmelen, {Frank van}",
  year      = "2004",
  isbn      = "0262012103",
  publisher = "MIT Press",
}


@article{642c6cb423f44d04a5308009b87e04e0,
  title     = "Structure-Based Partitioning of Large Concept Hierarchies",
  abstract  = "The increasing awareness of the benefits of ontologies for information processing has lead to the creation of a number of large ontologies about real-world domains. The size of these ontologies and their monolithic character cause serious problems in handling them. In other areas, e.g. software engineering, these problems are tackled by partitioning monolithic entities into sets of meaningful and mostly self-contained modules. In this paper, we suggest a similar approach for ontologies. We propose a method for automatically partitioning large ontologies into smaller modules based on the structure of the class hierarchy. We show that the structure-based method performs surprisingly well on real-world ontologies. We support this claim by experiments carried out on real-world ontologies including SUMO and the NCI cancer ontology. The results of these experiments are available online at http: //swserver. cs .vu.nl/partitioning/.",
  author    = "Heiner Stuckenschmidt and Michel Klein",
  year      = "2004",
  doi       = "10.1007/978-3-540-30475-3_21",
  volume    = "3298",
  pages     = "289--303",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@inbook{7b70f7f48ea9415c8ee7ff1a75259f6c,
  title     = "The SemanticWeb – ISWC 2004: Third International SemanticWeb Conference Hiroshima, Japan, November 7-11, 2004 Proceedings",
  author    = "Sheila McIlraith and Dimitris Plexousakis and {van Harmelen}, Frank",
  year      = "2004",
  isbn      = "3540237984",
  volume    = "3298",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  booktitle = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
}


@misc{e8adce6202874f8283a147f388e811fc,
  title    = "Using C-OWL for the Alignment and Merging of Medical Ontologies",
  abstract = "A number of sophisticated medical ontologies have been created over the past years. With their de-velopment the need for supporting the alignment of different ontologies is gaining importance. We proposed C-OWL, an extension of the Web Ontology Language OWL that supports alignment mappings between different, possibly incompatible ontologies on a semantic level. In this paper we report experiences from using C-OWL for the alignment of medical ontologies. We briefly review key concepts of the C-OWL semantics, explain the setting of the case study including some examples from the alignment and discuss the possibility of reasoning about the mapping based on the C-OWL semantics We conclude by arguing that C-OWL provides an adequate frame-work for aligning complex ontologies in the medical domain.",
  keywords = "Biomedical Knowledge representation, Knowledge Representa-tion Languages, Terminology Integration, validation and maintenance",
  author   = "Heiner Stuckenschmidt and {van Harmelen}, Frank and Luciano Serafini and Paolo Bouquet and Fausto Giunchiglia",
  year     = "2004",
}


@inbook{35cebc494cf0476b8cccc7a872efef48,
  title     = "Web Ontology Language",
  abstract  = "McGuinness, D., van Harmelen, F. (2004). OWL Web Ontology Language - W3C Recommendation. Retrieved June 14, 2009, http://www.w3.org/TR/owl-features/",
  author    = "DL McGuinness and {van Harmelen}, Frank",
  year      = "2004",
  doi       = "10.1007/978-1-4614-6170-8_113",
  isbn      = "978-1-4614-6170-8",
  series    = "W3C Recommendation.",
  booktitle = "W3C Recommendation.",
}


@inbook{606c3c9c70224bcb9a8f00b4fc935f63,
  title     = "Web ontology language: Owl",
  abstract  = "Summary. The expressivity of RDF and RDF Schema that was described in 12 is deliberately very limited: RDF is (roughly) limited to binary ground predicates, and RDF Schema is (again roughly) limited to a subclass hierarchy and a property hierarchy, with domain and range",
  author    = "G. Antoniou and {Van Harmelen}, F.",
  year      = "2004",
  doi       = "10.1.1.6.9313",
  isbn      = "9783540709992",
  series    = "Handbook on ontologies",
  publisher = "Springer_Verlag",
  pages     = "45–60",
  booktitle = "Handbook on ontologies",
}


@inbook{fdbc8e232a364a2b92162a707fa6a743,
  title     = "Evolution Management for Interconnected Ontologies",
  abstract  = "Mappings between ontologies are easily harmed by changes in the ontologies. In this paper we explain a mechanism to define modular ontologies and mappings in a way that allows for local containment of terminological reasoning. We have also developed a change detection and analysis method that predicts the effect of changes on the concept hierarchy. This method determines whether the changes in one ontology affect the reasoning inside other ontologies or not. Together, these mechanisms allow ontologies to evolve without unpredictable effects on other ontologies. In this paper, we also apply these methods in a case study that is undertaken in a EU IST project.",
  author    = "Michel Klein and Heiner Stuckenschmidt",
  year      = "2003",
  month     = "10",
  editor    = "AnHai Doan and Alon Halevy and Natasha Noy",
  booktitle = "Workshop on Semantic Integration at ISWC 2003",
}


@article{a8f1c38ebbb94257ba54da65d5a7b1eb,
  title    = "A Metadata Model for Semantics-Based Peer-to-Peer Systems",
  abstract = "Peer-to-Peer systems are a new paradigm for information sharing and some systems have successfully been deployed. It has been argued that current Peer-to-Peer systems suffer from the lack of semantics. The SWAP project (Semantic Web and Peer-to-Peer) aims at overcoming this problem by combining the Peer-to-Peer paradigm with Semantic Web technologies. In the course of our investigations it turned out that the nature of Peer-to-Peer systems requires some compromises with respect to the use of semantic knowledge models. In particular, the notion of ontology does not really apply as we often do not find a shared understanding of the domain. In this paper, we propose a data model for encoding semantic information that combines features of ontology (concept hierarchies, relational structures) with a flexible description and rating model that allows us to handle heterogeneous and even contradictory views on the domain of interest. We discuss the role of this model in the SWAP environment and describe the model as well as its creation and access.",
  author   = "Marc Ehrig and Ch. Tempich and Jeen Broekstra and {van Harmelen}, Frank and Marta Sabou and Ronny Siebes and Steffen Staab and Heiner Stuckenschmidt",
  year     = "2003",
  pages    = "1--4",
  journal  = "Proceedings of the second Konferenz Professionelles Wissensmanagement",
}


@inbook{ad4d0835f8a04412930c72d1a2f8fadb,
  title     = "A Protocol for Multi-Agent Diagnosis with Spatially Distributed Knowledge",
  abstract  = "In a large distributed system it is often infeasible or even impossible to perform diagnosis using a single model of the whole system. Instead, several spatially distributed local models of the system have to be used to detect possible faults. Traditional diagnostic tools, however, are not suitable to deal with such a set of spatially distributed local models. A Multi-Agent System of diagnostic agents, where each agent has a model of a subsystem, may be proposed as a solution for establishing global diagnoses of large distributed systems. Unfortunately, establishing a global minimal diagnosis is NP-Hard, even if every agent is able to determine local minimal diagnoses in polynomial time. Moreover, communication overhead in establishing a global diagnosis will be high: unless P = NP a super-polynomial number of messages between the agents will be required for establishing a global diagnosis. In this paper we present a protocol that overcomes this complexity issue by exchanging diagnostic precision for enables agents to determine local minimal diagnoses that are consistent with global diagnoses. Moreover, the protocol ensures that no agent acquires knowledge of global diagnoses. The protocol does not guarantee that a combination of the agents' local minimal diagnoses is also a global minimal diagnosis. However, for every global minimal diagnosis, there is a combination of local minimal diagnoses.",
  keywords  = "Model-Based Diagnosis",
  author    = "Nico Roos and {Ten Teije}, Annette and Cees Witteveen",
  year      = "2003",
  volume    = "2",
  pages     = "655--661",
  editor    = "J.S. Rosenschein and T. Sandholm and M. Wooldridge and M. Yakoo",
  booktitle = "Proceedings of the Second International Joint Conference on Autonomous Agents and multiagent Systems, AAMAS 03",
}


@article{39a06dba90094459ad5e1f75241fa348,
  title     = "C-OWL: Contextualizing ontologies",
  abstract  = "Ontologies are shared models of a domain that encode a view which is common to a set of different parties. Contexts are local models that encode a party's subjective view of a domain. In this paper we show how ontologies can be contextualized, thus acquiring certain useful properties that a pure shared approach cannot provide. We say that an ontology is contextualized or, also, that it is a contextual ontology, when its contents are kept local, and therefore not shared with other ontologies, and mapped with the contents of other ontologies via explicit (context) mappings. The result is Context OWL (C-OWL), a language whose syntax and semantics have been obtained by extending the OWL syntax and semantics to allow for the representation of contextual ontologies.",
  author    = "Paolo Bouquet and Fausto Giunchiglia and {Van Harmelen}, Frank and Luciano Serafini and Heiner Stuckenschmidt",
  year      = "2003",
  volume    = "2870",
  pages     = "164--179",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@article{43af14a00e83403a80a48fc3aa316052,
  title     = "Experiences in the formalisation and verification of medical protocols",
  abstract  = "Medical practice protocols or guidelines are statements to assist practitioners and patient decisions about appropriate health care for specific circumstances. In order to reach their potential benefits, protocols must fulfill strong quality requirements. Medical bodies worldwide have made efforts in this direction, mostly using informal methods such as peer review of protocols. We are concerned with, a different approach, namely the quality improvement of medical protocols by formal methods. In this paper we report on our experiences in the formalisation and verification of a real-world medical protocol. We have fully formalised a medical protocol in a two-stage formalisation process. Then, we have used a theorem prover to confirm whether the protocol formalisation complies with certain protocol properties. As a result, we have shown that formal verification can be used to analyse, and eventually improve, medical protocols.",
  author    = "Mar Marcos and Michael Balser and {Ten Teije}, Annette and {Van Harmelen}, Frank and Christoph Duelli",
  year      = "2003",
  volume    = "2780",
  pages     = "132--141",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@article{fce48238792241078b29a4c4e4ba5aa7,
  title     = "From SHIQ and RDF to OWL:The Making of a Web Ontology Language",
  abstract  = "The OWL Web Ontology Language is a new formal language\nfor representing ontologies in the Semantic Web. OWL has features from\nseveral families of representation languages, including primarily Description\nLogics and frames. OWL also shares many characteristics with RDF,\nthe W3C base of the Semantic Web. In this paper we discuss how the\nphilosophy and features of OWL can be traced back to these older formalisms,\nwith modifications driven by several other constraints on OWL.\nSeveral interesting problems have arisen where these influences on OWL\nhave clashed.",
  keywords  = "description logics, frames, ontologies, semantic web",
  author    = "Ian Horrocks and Peter Patel-Schneider and {van Harmelen}, Frank",
  year      = "2003",
  doi       = "10.1016/j.websem.2003.07.001",
  volume    = "1",
  pages     = "7--26",
  journal   = "Journal of Web Semantics",
  issn      = "1570-8268",
  publisher = "Elsevier",
  number    = "1",
}


@article{82d380a0329944ba839d10f7d3e0d1f1,
  title     = "Informal and formal medical guidelines: Bridging the gap",
  abstract  = "The role of medical guidelines is becoming more and more important in the medical field. Within the Protocure project it has been shown that the quality of medical guidelines can be improved by formalisation. However formalisation turns out to be a very time-consuming task, resulting in a formal guideline that is much more complex than the original version and the relation with this original guideline is often unclear. This paper presents a case study where the relation between the informal medical guideline and its formal counterpart is investigated. This has been used to determine the gaps between the formal and informal guidelines and the cause of the size explosion of the formal guidelines.",
  author    = "Marije Geldof and {Ten Teije}, Annette and {Van Harmelen}, Frank and Mar Marcos and Peter Votruba",
  year      = "2003",
  volume    = "2780",
  pages     = "173--178",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@book{904bcc8992c849a0b31cccc6ca65d5f6,
  title     = "Information Sharing on the Semantic Web",
  abstract  = "The large-scale and almost ubiquitous availability of information has become as much of a curse as it is a blessing. The more information is available, the harder it is to locate any particular piece of it. And even when it has been successfully found, it is even harder still to usefully combine it with other information we may already possess. This problem occurs at many different levels, ranging from the overcrowded disks of our own PCs to the mass of unstructured information on the World Wide Web. It is commonly understood that this problem of information sharing can only be solved by giving computers better access to the semantics of the information. While it has been recognized that ontologies play a crucial role in solving the open problems, most approaches rely on the existence of well-established data structures. To overcome these shortcomings, Stuckenschmidt and van Harmelen describe ontology-based approaches for resolving semantic heterogeneity in weakly structured environments, in particular the World Wide Web. Addressing problems like missing conceptual models, unclear system boundaries, and heterogeneous representations, they design a framework for ontology-based information sharing in weakly structured environments like the Semantic Web. For researchers and students in areas related to the Semantic Web, the authors provide not only a comprehensive overview of the State of the art, but also present in detail recent research in areas like ontology design for information integration, metadata generation and management, and representation and management of distributed ontologies. For professionals in areas such as e-commerce (e.g., the exchange of product knowledge) and knowledge management (e.g., in large and distributed organizations), the book provides decision support on the use of novel technologies, information about potential problems, and guidelines for the successful application of existing technologies.",
  author    = "Heiner Stuckenschmidt and Harmelen, {Frank Van}",
  year      = "2003",
  isbn      = "3540205942",
  publisher = "Springer/Verlag",
}


@article{4459737dc40e4174aecd22ca21645d8d,
  title    = "Integrity and change in modular ontologies",
  abstract = "The benefits of modular representations arc well known from many areas of computer science. In this paper, we concentrate on the benefits of modular ontologies with respect to local containment of terminological reasoning. We define an architecture for modular ontologies that supports local reasoning by compiling implied subsumption relations. We further address the problem of guaranteeing the integrity of a modular ontology in the presence of local changes. We propose a strategy for analyzing changes and guiding the process of updating compiled information.",
  author   = "Heiner Stuckenschmidt and Michel Klein",
  year     = "2003",
  pages    = "900--905",
  journal  = "IJCAI International Joint Conference on Artificial Intelligence",
  issn     = "1045-0823",
}


@inbook{8e6f8c918eb94fbcb1d304e80ea5c2e1,
  title     = "Supporting User Tasks through Visualisation of Light-weight Ontologies",
  abstract  = "Introduction\n\nAs is abundantly clear from the other chapters in this volume, ontologies will\nplay a central role in the development and deployment of the Semantic Web.\nThey will be used for many di#erent purposes, ranging across information\nlocalisation, integration, querying, presentation and navigation.\n\nExperiences in other fields (Data Mining, Scientific Computing) have\nshown that visualisation techniques can be successfully employed to support\nmany of these tasks in those areas. The...",
  author    = "Christiaan Fluit and Marta Sabou and {van Harmelen}, Frank",
  year      = "2003",
  doi       = "10.1.1.15.5304",
  isbn      = "3540408347",
  series    = "Handbook on Ontologies in Information Systems",
  pages     = "1--20",
  booktitle = "Handbook on Ontologies in Information Systems",
}


@book{1be539fb07104c5eb9904af692ac7392,
  title     = "Towards the Semantic Web",
  abstract  = "With the current changes driven by the expansion of the World Wide Web, this book uses a different approach from other books on the market: it applies ontologies to electronically available information to improve the quality of knowledge management in large and distributed organizations. Ontologies are formal theories supporting knowledge sharing and reuse. They can be used to explicitly represent semantics of semi-structured information. These enable sophisticated automatic support for acquiring, maintaining and accessing information. Methodology and tools are developed for intelligent access to large volumes of semi-structured and textual information sources in intra- and extra-, and internet-based environments to employ the full power of ontologies in supporting knowledge management from the information client perspective and the information provider. The aim of the book is to support efficient and effective knowledge management and focuses on weakly-structured online information sources. It is aimed primarily at researchers in the area of knowledge management and information retrieval and will also be a useful reference for students in computer science at the postgraduate level and for business managers who are aiming to increase the corporations information infrastructure. The Semantic Web is a very important initiative affecting the future of the WWW that is currently generating huge interest. The book covers several highly significant contributions to the semantic web research effort, including a new language for defining ontologies, several novel software tools and a coherent methodology for the application of the tools for business advantage. It also provides 3 case studies which give examples of the real benefits to be derived from the adoption of semantic-web based ontologies in {"}real world{"} situations. As such, the book is an excellent mixture of theory, tools and applications in an important area of WWW research. Provides guidelines for introducing knowledge management concepts and tools into enterprises, to help knowledge providers present their knowledge efficiently and effectively. Introduces an intelligent search tool that supports users in accessing information and a tool environment for maintenance, conversion and acquisition of information sources. Discusses three large case studies which will help to develop the technology according to the actual needs of large and or virtual organisations and will provide a testbed for evaluating tools and methods. The book is aimed at people with at least a good understanding of existing WWW technology and some level of technical understanding of the underpinning technologies (XML/RDF). It will be of interest to graduate students, academic and industrial researchers in the field, and the many industrial personnel who are tracking WWW technology developments in order to understand the business implications. It could also be used to support undergraduate courses in the area but is not itself an introductory text. {"}Generating huge interest and backed by the global WorldWideWeb consortium the semantic web is the key initiative driving the future of the World Wide Web. Towards the Semantic Web focuses on the application of Semantic Web technology and ontologies in particular to electronically available information to improve the quality of knowledge management in large and distributed organizations. Ontologies are formal structures supporting knowledge sharing and reuse. They can be used to represent explicitly the semantics of structured and semi-structured information which enable sophisticated automatic support for acquiring, maintaining and accessing information. Covering the key technologies for the next generation of the WWW, this book is an excellent mixture of theory, tools and applications in an important area of WWW research. Aims to support more efficient and effective knowledge management and focuses on weakly-structured online information sources. Covers highly significant contributions to the semantic web research effort, including a new language for defining ontologies, several novel software tools and a coherent methodology for the application of the tools for business advantage. Provides guidelines for introducing knowledge management concepts and tools into businesses. Introduces an intelligent search tool that supports users in accessing information and a tool environment for maintenance, conversion and acquisition of information sources. Also describes information visualisation and knowledge sharing tools. Describes a state-of-the-art system for storage and retrieval of metadata expressed in RDF and RDF Schema. Also discusses a system for automatic metadata extraction. Examines three significant case studies providing examples of the real benefits to be derived from the adoption of semantic-web based ontologies in {"}{"}real world{"}{"} situations.",
  author    = "John Davies and Dieter Fensel and Harmelen, {Frank Van}",
  year      = "2003",
  isbn      = "0470848677",
  publisher = "John Wiley",
}


@article{b5075dbf07fa45fa8173d5898b9a8972,
  title     = "Enabling knowledge representation on the Web by extending RDF Schema",
  abstract  = "Recently, a widespread interest has emerged in using ontologies on the Web. Resource Description Framework Schema (RDFS) is a basic tool that enables users to define vocabulary, structure and constraints for expressing meta data about Web resources. However, it includes no provisions for formal semantics, and its expressivity is not sufficient for full-fledged ontological modeling and reasoning. In this paper, we will show how RDFS can be extended to include a more expressive knowledge representation language. That, in turn, would enrich it with the required additional expressivity and the semantics of that language. We do this by describing the ontology language Ontology Inference Layer (OIL) as an extension of RDFS. An important advantage to our approach is that it ensures maximal sharing of meta data on the Web: even partial interpretation of an OIL ontology by less semantically aware processors will yield a correct partial interpretation of the meta data.",
  keywords  = "DAML, Knowledge representation, OIL, Ontologies, RDF, Semantic Web",
  author    = "Jeen Broekstra and Michel Klein and Stefan Decker and Dieter Fensel and {Van Harmelen}, Frank and Ian Horrocks",
  year      = "2002",
  month     = "8",
  doi       = "10.1016/S1389-1286(02)00217-7",
  volume    = "39",
  pages     = "609--634",
  journal   = "Computer Networks (1999)",
  issn      = "1389-1286",
  publisher = "Elsevier",
  number    = "5",
}


@article{34b40f2bd03442f08fd4699c2281d9eb,
  title     = "How the semantic web will change KR: Challenges and opportunities for a new research agenda",
  abstract  = "Issues related to the application of semantic web to knowledge representation (KR) are discussed. Some of the assumptions underlying current KR technology that need to be revised when applied to semantic web are described. Areas which require future research include variable quality of knowledge, diversity of content, distributed authority, and multiple knowledge sources. Techniques for the local containment of inconsistencies will become much more important than they have been in current KR research.",
  author    = "{Van Harmelen}, Frank",
  year      = "2002",
  month     = "3",
  doi       = "10.1017/S0269888902000383",
  volume    = "17",
  pages     = "93--96",
  journal   = "Knowledge Engineering Review",
  issn      = "0269-8889",
  publisher = "Cambridge University Press",
  number    = "1",
}


@inbook{68bc50094491420db20af0fc165e9222,
  title     = "An analysis of multi-agent diagnosis",
  abstract  = "This paper analyzes the use of a Multi-Agent System for Model-Based Diagnosis. In a large dynamical system, it is often infeasible or even impossible to maintain a model of the whole system. Instead, several incomplete models of the system have to be used to establish a diagnosis and to detect possible faults. These models may also be physically distributed. A Multi-Agent System of diagnostic agents may offer solutions for establishing a global diagnosis. If we use a separate agent for each incomplete model of the system, establishing a global diagnosis becomes a problem of cooperation and negotiation between the diagnostic agents. This raises the question whether 'a set of diagnostic agents, each having an incomplete model of the system, can (efficiently) determine the same global diagnosis as an ideal single diagnostic agent having the combined knowledge of these agents?'.",
  author    = "Nico Roos and {Ten Teije}, Annette and André Bos and Cees Witteveen",
  year      = "2002",
  pages     = "986--987",
  editor    = "C. Castelfranchi and W.L. Johnson",
  booktitle = "Proceedings of the International Conference on Autonomous Agents",
  edition   = "1",
}


@inbook{3041593d264c478884e38f11e6e6e8fa,
  title     = "Approximating terminological queries",
  abstract  = "Current proposals for languages to encode terminological knowledge in intelligent systems support logical reasoning for answering user queries about objects and classes. An application of these languages on the World Wide Web, however, is hampered by the limitations of logical reasoning in terms ofefficiency and flexibility. In this paper we describe, how techniques from approximate reasoning can be used to overcome these problems. We discuss terminological knowledge and approximate reasoning in general and show the benefits ofapproximate reasoning using the example ofbuilding and maintaining semantic catalogues that can be used to query resource locations based on object classes.",
  author    = "Heiner Stuckenschmidt and {Van Harmelen}, Frank",
  year      = "2002",
  isbn      = "3540000747",
  volume    = "2522 LNAI",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  pages     = "329--343",
  booktitle = "Flexible Query Answering Systems - 5th International Conference, FQAS 2002, Proceedings",
}


@inbook{67261de6afa54370801ff6d951a47102,
  title     = "From informal knowledge to formal logic: A realistic case study in medical protocols",
  abstract  = "We report our experience in a case study with constructing fully formalised knowledge models of realistic, specialised medical knowledge. We have taken a medical protocol in daily use by medical specialists, modelled this knowledge in a specific-purpose knowledge representation language, and finally formalised this knowledge representation in terms of temporal logic and parallel programs. The value of this formalisation process is that each successive formalisation step has contributed to improving the quality of the original medical protocol, and that the final formalisation allows us to provide machine-assisted proofs of properties that are satisfied by the original medical protocol (or, alternatively, precise arguments why the original protocol fails to satisfy certain desirable properties). We believe that this the first time that a significant body of medical knowledge (in our case: a protocol for the management of jaundice in newborns) has been formalised to the extent that it becomes amenable to automated theorem proving, and that this has actually lead to improvement of the original body of medical knowledge.",
  author    = "Mar Marcos and Michael Balser and {Ten Teije}, Annette and {van Harmelen}, Frank",
  year      = "2002",
  isbn      = "3540442685",
  volume    = "2473",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "49--64",
  booktitle = "Knowledge Engineering and Knowledge Management: Ontologies and the Semantic Web - 13th International Conference, EKAW 2002, Proceedings",
}


@misc{1ed1d738b5ab4c4b9192563f10370569,
  title    = "Learning Structural Classification Rules for Web-page Categorization",
  abstract = "Content-related metadata plays an important role in the effort of developing intelligent web applications. One of the most established form of providing content-related metadata is the assignment of web-pages to content categories. We describe the Spectacle system for classifying individual web pages on the basis of their syntactic structure. This classification requires the spe-cification of classification rules associating common pa-ge structures with predefined classes. In this paper, we propose an approach for the automatic acquisition of these classification rules using techniques from inducti-ve logic programming and describe experiments in ap-plying the approach to an existing web-based informa-tion system.",
  author   = "Heiner Stuckenschmidt and Jens Hartmann and {Van Harmelen}, Frank",
  year     = "2002",
}


@inbook{0977e50ff58f4f31b7120f5b4c72e182,
  title     = "Ontology-based Information Visualisation",
  abstract  = "The main contribution of this paper is to show how visual representations of information can be based on ontological classifications of that information. We first discuss the central r ole of ontologies on the Semantic Web. We subsequently outline our general approach to the construction of ontology-based visualisations of data. This is followed by a set of examples of ontology-based visualisations which all differ in interesting respects. The paper concludes with a brief discussion of related work. 1",
  keywords  = "Semantic Web",
  author    = "Vladimir Geroimenko and Christiaan Fluit and Marta Sabou and {van Harmelen}, Frank",
  year      = "2002",
  isbn      = "0-7695-1195-3",
  series    = "Visualising the Semantic Web",
  publisher = "Springer/Verlag",
  editor    = "Vladimir Geroimenko",
  booktitle = "Visualising the Semantic Web",
}


@article{af387e71d0e5414b93b4ab7b7c66a69e,
  title    = "Ranking Agent Statements for Building Evolving Ontologies",
  abstract = "In this paper a methodology is described for ranking information received by different agents, based on previous experience with them. These rankings again are used for asking the right questions to the right agents. In this way agents can build up a reputation. The methods in this paper are strongly influenced on human heuristics regarding the assignment of confidence ratings on humans. The methods provide a solution to current problems with ontologies: (1) handling contradicting and sloppy information, (2) efficient network use instead of broadcasting information and (3) dealing with ontological drift.",
  author   = "Ronny Siebes and {van Harmelen}, Frank",
  year     = "2002",
  doi      = "10.1.1.6.8294",
  pages    = "2--4",
  journal  = "Workshop on Meaning Negotation, in conjunction with the Eighteenth National Conference on Artificial Intelligence",
}


@inbook{c20818146d15404d84428ec75b4f1497,
  title     = "Reviewing the design of DAML+OIL: An ontology language for the Semantic Web",
  abstract  = "In the current {"}Syntactic Web{"}, uninterpreted syntactic constructs are given meaning only by private off-line agreements that are inaccessible to computers. In the Semantic Web vision, this is replaced by a web where both data and its semantic definition are accessible and manipulable by computer software. DAML+OIL is an ontology language specifically designed for this use in the Web; it exploits existing Web standards (XML and RDF), adding the familiar ontological primitives of object oriented and frame based systems, and the formal rigor of a very expressive description logic. The definition of DAML+OIL is now over a year old, and the language has been in fairly widespread use. In this paper, we review DAML+OIL's relation with its key ingredients (XML, RDF, OIL, DAML-ONT, Description Logics), we discuss the design decisions and trade-offs that were the basis for the language definition, and identify a number of implementation challenges posed by the current language. These issues are important for designers of other representation languages for the Semantic Web, be they competitors or successors of DAML+OIL, such as the language currently under definition by W3C.",
  author    = "Ian Horrocks and Patel-Schneider, {Peter F.} and {Van Harmelen}, Frank",
  year      = "2002",
  pages     = "792--797",
  booktitle = "Proceedings of the National Conference on Artificial Intelligence",
}


@inbook{49ce1d5b12a640598767d522cd113302,
  title     = "Sesame: A generic architecture for storing and querying RDF and RDF Schema",
  abstract  = "RDF and RDF Schema are two W3C standards aimed at enriching the Web with machine-processable semantic data. We have developed Sesame, an architecture for efficient storage and expressive querying of large quantities of metadata in RDF and RDF Schema. Sesame's design and implementation are independent from any specific storage device. Thus, Sesame can be deployed on top of a variety of storage devices, such as relational databases, triple stores, or object-oriented databases, without having to change the query engine or other functional modules. Sesame offers support for concurrency control, independent export of RDF and RDFS information and a query engine for RQL, a query language for RDF that offers native support for RDF Schema semantics. We present an overview of Sesame as a generic architecture, as well as its implementation and our first experiences with this implementation.",
  author    = "Jeen Broekstra and Arjohn Kampman and {Van Harmelen}, Frank",
  year      = "2002",
  isbn      = "3540437606",
  volume    = "2342 LNCS",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  pages     = "54--68",
  booktitle = "The Semantic Web, ISWC 2002 - First International Semantic Web Conference, Proceedings",
}


@article{98c93a250fa641278ebdde3e4dbcd1fe,
  title     = "A survey of languages for specifying dynamics: A knowledge engineering perspective",
  abstract  = "During the last years, a number of formal specification languages for knowledge-based systems has been developed. Characteristics for knowledge-based systems are a complex knowledge base and an inference engine which uses this knowledge to solve a given problem. Specification languages for knowledge-based systems have to cover both aspects. They have to provide the means to specify a complex and large amount of knowledge and they have to provide the means to specify the dynamic reasoning behavior of a knowledge-based system. This paper focuses on the second aspect. For this purpose, we survey existing approaches for specifying dynamic behavior in related areas of research. In fact, we have taken approaches for the specification of information systems (Language for Conceptual Modeling and TROLL), approaches for the specification of database updates and logic programming (Transaction Logic and Dynamic Database Logic) and the generic specification framework of Abstract State Machines.",
  keywords  = "Dynamics, Inference control, Knowledge-based systems, Specification languages, Update logics",
  author    = "{Van Eck}, Pascal and Joeri Engelfriet and Dieter Fensel and {Van Harmelen}, Frank and Yde Venema and Mark Willems",
  year      = "2001",
  month     = "5",
  doi       = "10.1109/69.929903",
  volume    = "13",
  pages     = "462--496",
  journal   = "IEEE Transactions on Knowledge and Data Engineering",
  issn      = "1041-4347",
  publisher = "IEEE Computer Society",
  number    = "3",
}


@inbook{695bd5862fba4dd3b33c964dcab0c8b3,
  title     = "Knowledge-based validation, aggregation, and visualization of meta-data: Analyzing a web-based information system",
  abstract  = "As meta-data become of increasing importance to the Web, we will need to start managing such meta-data. We argue that there is a strong need for meta-data validation and aggregation. We introduce the Spectacle Workbench for verifying semi-structured information and show how it can be used to validate, aggregate and visualize the metadata of an existing Information System. We conclude that the possibility to verify and aggregate meta-data is an added value with respect to contents-based access to information.",
  author    = "Heiner Stuckenschmidt and {Van Harmelen}, Frank",
  year      = "2001",
  isbn      = "3540427309",
  volume    = "2198",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "217--226",
  booktitle = "Web Intelligence: Research and Development - 1st Asia-Pacific Conference, WI 2001, Proceedings",
}


@article{6c2d3aaf30574c7783a1d47dfe23d493,
  title    = "OIL Ontology Infrastructure to Enable the Semantic Web",
  abstract = "Currently computers are changing from single isolated devices to entry points into a worldwide network of information exchange and business transactions. Therefore, support in the exchange of data, information, and knowledge is becoming the key issue in computer technology today. Ontologies provide a shared and common understanding of a domain that can be communicated between people and across application systems. Ontologies will play a major role in supporting information exchange processes in various areas. A prerequisite for such a role is the development of a joint standard for specifying and exchanging Ontologies well-integrated with existing web standards. This paper deals with precisely this necessity. We will present OIL which is a proposal for such a standard enabling the semantic web, i.e. information with machine processable semantics. It is based on existing proposals such as OKBC, XOL and RDFS, and enriches them with necessary features for expressing rich ontologies. The paper presents the motivation, underlying rationale, modeling primitives, syntax, semantics, tool environment, and applications of OIL",
  author   = "Dieter Fensel and Ian Horrocks and Harmelen, {Frank Van} and McGuinness, {Deborah L}",
  year     = "2001",
  doi      = "10.1109/5254.920598",
  volume   = "16",
  pages    = "38--45",
  journal  = "Intelligent Systems, IEEE",
  number   = "2",
}


@misc{218fe7547df84745a4ebdb5b317e312f,
  title    = "Ontology-based Information Visualisation",
  abstract = "The main contribution of this paper is to show how vi- sual representations of information can be based on onto- logical classifications of that information. We first discuss the central role of ontologies on the SemanticWeb. We sub- sequently outline our general approach to the construction of ontology-based visualisations of data. This is followed by a set of examples of ontology-based visualisations which all differ in interesting respects. The paper concludes with a brief discussion of related work.",
  author   = "Harmelen, {Frank Van} and Jeen Broekstra and Christiaan Fluit and {ter Horst}, Herko and Arjohn Kampman and {Van Der Meer}, Jos and Marta Sabou",
  year     = "2001",
}


@inbook{296aea5f4f60437bad004b44f8994791,
  title     = "Ontology-based metadata generation from semi-structured information",
  abstract  = "Content-related metadata plays an important role in intelligent information systems. Especially on the world-wide web meaningful metadata describing the contents of a web-site is the key to intelligent retrieval and access of information. Metadata description standards like RDF and RDF schema have been developed and work in progress addresses the use of ontologies to provide a logical foundation for metadata. However, the acquisition of appropriate metadata is still a problem. The main part of the paper is concerned with the specification of ontologies and metadata models. We describe the Spectacle approach, a knowledge-based approach for metadata validation and generation as well as tools related to the ontology language OIL. We conclude that the specification of ontologies and the generation of metadata models are processes that supplement each other and propose a method for semi-automatic generation of metadata models on the basis of ontologies.",
  author    = "Heiner Stuckenschmidt and {Van Harmelen}, Frank",
  year      = "2001",
  isbn      = "1581133804",
  pages     = "163--170",
  booktitle = "Proceedings of the First International Conference on Knowledge Capture",
}


@inbook{3050714490394955af00ed895a24a2b4,
  title     = "Using critiquing for improving medical protocols: Harder than it seems",
  abstract  = "Medical protocols are widely recognised to provide clinicians with high-quality and up-to-date recommendations. A critical condition for this is of course that the protocols themselves are of high quality. In this paper we investigate the use of critiquing for improving the quality of medical protocols. We constructed a detailed formal model of the jaundice protocol of the American Association of Pediatrics in the Asbru representation language. We recorded the actions performed by a pediatrician while solving a set of test cases. We then compared these expert actions with the steps recommended by the formalised protocol, and analysed the differences that we observed. Even our relatively small test set of 7 cases revealed many mismatches between the actions performed by the expert and the protocol recommendations, which suggest improvements of the protocol. A major problem in our case study was to establish a mapping between the actions performed by the expert and the steps suggested by the protocol. We discuss the reasons for this difficulty, and assess its consequences for the automation of the critiquing process.",
  author    = "Mar Marcos and Geert Berger and {van Harmelen}, Frank and {ten Teije}, Annette and Hugo Roomans and Silvia Miksch",
  year      = "2001",
  isbn      = "3540422943",
  volume    = "2101",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "431--441",
  booktitle = "Artificial Intelligence in Medicine - 8th Conference on Artificial Intelligence in Medicine in Europe, AIME 2001, Proceedings",
}


@inbook{316fd65572bf405190f0e1ba8e4dfd35,
  title     = "Maintenance of KBS’s by domain experts the holy grail in practice",
  abstract  = "Enabling a domain expert to maintain his own knowledge in a Knowledge Based System has long been an ideal for the Knowledge Engineering community. In this paper we report on our experience with trying to achieve this ideal in a practical setting, by building a maintenance tool for an existing KBS. After a brief survey of various approaches to this problem described in literature, we select a domain-and task-specific modelling approach as the most promising and appropriate. First, we construct a domain ontology and a task model for the KBS system to be maintained, as well as a task analysis of the maintenance tool itself. The maintenance tool is subsequently implemented using a two layer architecture which seperates domain and system concepts. Although no full-scale evaluation has been undertaken, we report on our initial experience with this approach and present our conclusions.",
  author    = "Arne Bultman and Joris Kuipers and {Van Harmelen}, Frank",
  year      = "2000",
  doi       = "10.1007/3-540-45049-1_17",
  isbn      = "3540676899",
  volume    = "1821",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "139--149",
  booktitle = "Intelligent Problem Solving: Methodologies and Approaches - 13th International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems, IEA/AIE 2000, Proceedings",
}


@article{24590824e19943c092430458283ff48b,
  title     = "The DARPA agent markup language",
  abstract  = "The DARPA Agent Markup Language (DAML) program is a United States government sponsored endeavor aimed at providing the foundation for the next web evolution – the semantic web. The program is funding critical research to develop languages, tools and techniques for making considerably more of the content on the web machine-understandable. We believe that this will lead to the next major generation of web technology, and will enable considerably more “machine to machine” (agent-based) communication. The program includes participation from academic researchers, government agencies, software development companies, and industrial organizations such as the World Wide Web consortium (W3C). The DAML project is also working closely with other efforts, including European Union funded Semantic Web projects (e.g. On-to-Knowledge[3] and Ibrow[4]), and the ongoing W3C RDF recommendation effort[5]. In the remainder of this report, we provide a short motivation, description, and status report about the program.",
  author    = "O Lassila and {van Harmelen}, F and I. Horrocks and J. Hendler and DL McGuinness",
  year      = "2000",
  volume    = "15",
  pages     = "67--73",
  journal   = "IEEE Intelligent Systems & their Applications",
  issn      = "1094-7167",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  number    = "6",
}


@article{d297fe9213f24ac0a3b435eef68dfe7b,
  title     = "The semantic Web and its languages",
  abstract  = "The Web has drastically changed the availability of electronic information, but its success and exponential growth have made it increasingly difficult to find, access, present and maintain such information for a wide variety of users. In reaction to this bottleneck many new research initiatives and commercial enterprises have been set up to enrich available information with machine-processable semantics. The paper considers how the semantic Web will provide intelligent access to heterogeneous and distributed information, enabling software products (agents) to mediate between user needs and available information sources. The paper discusses the Resource Description Framework, XML and other languages",
  author    = "Ora Lassila and {van Harmelen}, F. and Ian Horrocks and Hendler, {James A.} and McGuinness, {Deborah L}",
  year      = "2000",
  doi       = "10.1109/5254.895864",
  volume    = "15",
  pages     = "67--73",
  journal   = "IEEE Intelligent Systems & their Applications",
  issn      = "1094-7167",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  number    = "6",
}


@inbook{48232687b6044cb498dc7e46b4054a96,
  title     = "Torture tests: A quantitative analysis for the robustness of knowledge-based systems",
  author    = "P Groot and {Van Harmelen}, F and {ten Teije}, Annette",
  year      = "2000",
  isbn      = "3-540-41119-4",
  volume    = "1937",
  series    = "LECTURE NOTES IN ARTIFICIAL INTELLIGENCE",
  pages     = "403--418",
  booktitle = "KNOWLEDGE ENGINEERING AND KNOWLEDGE MANAGEMENT, PROCEEDINGS",
}


@article{b4cbf2d229bc4a48b95f11cb0062e82c,
  title     = "Construction of problem-solving methods as parametric design",
  abstract  = "The knowledge-engineering literature contains a number of approaches for constructing or selecting problem solvers. Some of these approaches are based on indexing and selecting a problem solver from a library, others are based on a knowledge acquisition process, or are based on search-strategies. None of these approaches sees constructing a problem solver as a configuration task that could be solved with an appropriate configuration method. We introduce a representation of the functionality of problem-solving methods that allows us to view the construction of problem solvers as a configuration problem, and specifically as a parametric design problem. From the available methods for parametric design, we use propose-critique-modify for the automated configuration of problem-solving methods. We illustrate this approach by a scenario in a small car domain example.",
  author    = "{Ten Teije}, A. and {Van Harmelen}, F. and Schreiber, {A. Th} and Wielinga, {B. J.}",
  year      = "1998",
  month     = "10",
  volume    = "49",
  pages     = "363--389",
  journal   = "International Journal of Human-computer Studies",
  issn      = "1071-5819",
  publisher = "Academic Press Inc.",
  number    = "4",
}


@article{d896c05d87e04e30adbac060c7b9303b,
  title     = "Formal support for development of knowledge-based systems",
  abstract  = "This article provides an approach for developing reliable knowledge-based systems. Its main contributions are: Specification is done at an architectural level that abstracts from a specific implementation formalism. The model of expertise of CommonKADS distinguishs different types of knowledge and describes their interaction. Our architecture refines this model and adds an additional level of formalization. The formal specification and verification system KIV is used for specifying and verifying such architectures. We have chosen KIV for four reasons: (1) it provides the formal means required for specifying the dynamics of knowledge-based systems (i.e., dynamic logic), (2) it provides compositional specifications, (3) it provides an interactive theorem prover, and (4) last but not least it comes with a sophisticated tool environment developed in several realistic application projects.",
  author    = "Dieter Fensel and {van Harmelen}, Frank and Wolfgang Reif and {ten Teije}, Annette",
  year      = "1998",
  pages     = "1--18",
  journal   = "Failure and Lessons …",
  issn      = "1088-128X",
  publisher = "Cognizant Communication Corporation",
}


@article{45834b0c245a47b1b63bd545cc2054ed,
  title     = "Formal Support for Development of Knowledge-Based Systems",
  abstract  = "This article provides an approach for developing reliable knowledge-based systems. Its main contributions are: Specification is done at an architectural level that abstracts from a specific implementation formalism. The model of expertise of CommonKADS distinguishs different types of knowledge and describes their interaction. Our architecture refines this model and adds an additional level of formalization. The formal specification and verification system KIV is used for specifying and verifying such architectures. We have chosen KIV for four reasons: (1) it provides the formal means required for specifying the dynamics of knowledge-based systems (i.e., dynamic logic), (2) it provides compositional specifications, (3) it provides an interactive theorem prover, and (4) last but not least it comes with a sophisticated tool environment developed in several realistic application projects.",
  keywords  = "Formal methods, Knowledge-based systems, Validation, Verification",
  author    = "Dieter Fensel and {Van Harmelen}, Frank and Wolfgang Reif and {Ten Teije}, Annette",
  year      = "1998",
  volume    = "2",
  pages     = "173--182",
  journal   = "Failure and Lessons …",
  issn      = "1088-128X",
  publisher = "Cognizant Communication Corporation",
  number    = "4",
}


@inbook{6ab2f7260ef1493f81a644c5346238e4,
  title     = "Specification of dynamics for knowledge-based systems",
  abstract  = "During the last years, a number of formal specification languages for knowledge-based systems have been developed. Characteristic for knowledge-based systems are a complex knowledge base and an inference engine which uses this knowledge to solve a given problem. Specification languages for knowledge-based systems have to cover both aspects: they have to provide means to specify a complex and large amount of knowledge and they have to provide means to specify the dynamic reasoning behaviour of a knowledge-based system. This paper will focus on the second aspect, which is an issue considered to be unsolved. For this purpose, we have surveyed existing approaches in related areas of research. We have taken approaches for the specification of information systems (i.e., Language for Conceptual Modelling and Troll), approaches for the specification of database updates and the dynamics of logic programs (Transaction Logic and Dynamic Database Logic), and the approach of Abstract State Machines.",
  author    = "{Van Eck}, Pascal and Joeri Engelfriet and Dieter Fensel and {Van Harmelen}, Frank and Yde Venema and Mark Willems",
  year      = "1998",
  isbn      = "3540653058",
  volume    = "1472",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "37--68",
  booktitle = "Transactions and Change in Logic Databases - International Seminar on Logic Databases and the Meaning of Change and ILPS 1997 Post-Conference Workshop on (Trans)Actions and Change in Logic Programming and Deductive Databases, DYNAMICS 1997, Invited Surveys and Selected Papers",
}


@article{6dd3f5c9499c462fab1d070bc644f5e2,
  title     = "Applying rule-base anomalies to KADS inference structures",
  abstract  = "The literature on validation and verification of knowledge-based systems contains a catalogue of anomalies for knowledge-based systems, such as redundant, contradictory or deficient knowledge. Detecting such anomalies is a method for verifying knowledge-based systems. Unfortunately, the traditional formulation of the anomalies in the literature is very specific to a rule-based knowledge representation, which greatly restricts their applicability. In this paper, we show how the traditional anomalies can be reinterpreted in terms of conceptual models (in particular KADS inference structures). For this purpose, we present a formalisation of KADS inference structures which enables us to apply the traditional rule-base anomalies to these inference structures. This greatly improves the usefulness of the anomalies, since they can now be applied to a much wider class of knowledge-based systems. Besides this reformulation and wider applicability of the traditional anomalies, further contributions of this paper are a novel formalisation of KADS inference structures and a number of improvements to the existing formalisation of the traditional anomalies.",
  keywords  = "Anomalies, Inference structures, Knowledge-based systems, Validation, Verification",
  author    = "{Van Harmelen}, Frank",
  year      = "1997",
  month     = "12",
  volume    = "21",
  pages     = "271--280",
  journal   = "Decision Support Systems",
  issn      = "0167-9236",
  publisher = "North Holland",
  number    = "4",
}


@article{b24000c856414830948c326f9708ff73,
  title     = "Formalisation for decision support in anaesthesiology",
  abstract  = "This paper reports on research for decision support for anaesthesiologists at the University Hospital in Groningen, the Netherlands. Based on CAROLA, an existing automated operation documentation system, we designed a support environment that will assist in real-time diagnosis. The core of the work presented here consists of a knowledge base (containing anaesthesiological knowledge) and a diagnosis system. The knowledge base is specified in the logic-based formal specification language AFSL. This leads to a powerful and precise treatment of knowledge structuring and data abstraction.",
  keywords  = "Anesthesiology, Artificial Intelligence, Body Surface Area, Body Temperature, Decision Support Techniques, Hematocrit, Humans, Models, Theoretical, Journal Article",
  author    = "{Renardel de Lavalette}, {G R} and R. Groenboom and E Rotterdam and {van Harmelen}, F and {ten Teije}, A and {de Geus}, F.",
  year      = "1997",
  month     = "11",
  volume    = "11",
  pages     = "189--214",
  journal   = "Artificial Intelligence in Medicine",
  issn      = "0933-3657",
  publisher = "Elsevier",
  number    = "3",
}


@article{7482b42880d043558aa294350e9d0f7a,
  title     = "Applying rule-base anomalies to KADS inference structures",
  abstract  = "The literature on validation and verification of knowledge-based systems contains a catalouge of anomalies for knowledge-based systems, such as redundant, contradictory or deficient knowledge. Detecting such anomalies is a method for verifying knowledge-based systems. (continued)",
  keywords  = "validation, verfication, knowledge-based systems,",
  author    = "{van Harmelen}, F",
  year      = "1997",
  volume    = "21",
  pages     = "271--280",
  journal   = "Decision Support Systems",
  issn      = "0167-9236",
  publisher = "North Holland",
}


@article{702aad7d1b6b4abdabb3e49ebf7bd553,
  title     = "Using reflection techniques for flexible problem solving (with examples from diagnosis)",
  abstract  = "Flexible problem solving consists of the dynamic selection and configuration of problem solving methods for a particular problem type, depending on the particular problem and the goal of problem solving. In this paper, we propose an architecture that supports such flexible problem solving automatically. For this purpose, problem solving methods are described in a uniform way, by an abstract model of components, which together define the functionality of the methods. Such an abstract model is used for dynamic selection and configuration of the problem solving methods. The proposed architecture for flexible problem solving consists of well-known reflection techniques: two object-meta relations, a definable naming mechanism and the axiomhood and theoremhood reflection rules. We have succeeded in using standard meta-architecture techniques to enable flexible problem solving.",
  keywords  = "Applications of meta-reasoning, Architectures for meta-reasoning, Configuration of problem solving methods, Diagnosis",
  author    = "{Ten Teije}, Annette and {Van Harmelen}, Frank",
  year      = "1996",
  month     = "9",
  doi       = "10.1016/0167-739X(96)88794-2",
  volume    = "12",
  pages     = "217--234",
  journal   = "Future Generation Computer Systems",
  issn      = "0167-739X",
  publisher = "Elsevier",
  number    = "2-3 SPEC. ISS.",
}


@article{76a11dc0f09f4423bb76bab0709a691d,
  title     = "Evaluating a formal KBS specification language",
  abstract  = "Formal specification languages can improve the development of knowledge-based systems (KBS), but several problems limit their usefulness. (ML)2, a formal language based on the knowledge models used in the CommonKADS KBS development method, avoids many of these problems. (ML)2 specifically aims at formalizing the CommonKADS expertise model. To analyze (ML)2 usability, a set of evaluation criteria was designed. A small case study was then performed, constructing and expertise model in (ML)2, to test and refine this criteria.",
  author    = "{van Harmelen}, Frank and Manfred Aben and Fidel Ruiz and {van de Plassche}, Joke",
  year      = "1996",
  month     = "2",
  doi       = "10.1109/64.482959",
  volume    = "11",
  pages     = "56--62",
  journal   = "IEEE expert",
  issn      = "0885-9000",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  number    = "1",
}


@article{daf4c16d543c4ce0aec82cc5f52ece68,
  title     = "Structure-preserving specification languages for knowledge-based systems",
  abstract  = "Much of the work on validation and verification of knowledge based systems (KBSs) has been done in terms of implementation languages (mostly rule-based languages). Recent papers have argued that it is advantageous to do validation and verification in terms of a more abstract and formal specification of the system. However, constructing such formal specifications is a difficult task. This paper proposes the use of formal specification languages for KBS-development that are closely based on the structure of informal knowledge-models. The use of such formal languages has as advantages that (i) we can give strong support for the construction of a formal specification, namely on the basis of the informal description of the system; and (ii) we can use the structural correspondence to verify that the formal specification does indeed capture the informally stated requirements.",
  author    = "{Van Harmelen}, Frank and Manfred Aben",
  year      = "1996",
  month     = "2",
  doi       = "10.1006/ijhc.1996.0010",
  volume    = "44",
  pages     = "187--212",
  journal   = "International Journal of Human-computer Studies",
  issn      = "1071-5819",
  publisher = "Academic Press Inc.",
  number    = "2",
}


@inbook{068f3f9559274570aa6d39bc11083297,
  title     = "Comparing Formal Specification Languages for Complex Reasoning Systems",
  author    = "{van Harmelen}, Frank and {Lopez de Mantaras}, Ramon and Jacek Malec and Jan Treur",
  year      = "1993",
  pages     = "257--282",
  editor    = "J. Treur and T. Wetter",
  booktitle = "Formal Specification of Complex Reasoning Systems",
  publisher = "Ellis Horwood",
}

%% Added 13-08-2019 by Erman %%

@article{AIMed2018,
author = "Kerstin Denecke and Frank van Harmelen",
title = "Recent advances in extracting and processing rich semantics from medical texts",
journal = "Artificial Intelligence in Medicine",
year = "2018",
issn = "0933-3657",
doi = "https://doi.org/10.1016/j.artmed.2018.07.004",
urlPaper = "http://www.cs.vu.nl/~frankh/postscript/AIMed2018.pdf",
keywords = {Medical Knowledge Representation}
}


@InProceedings{ISWC2018,
  author    = {Joe Raad and Wouter Beek and Frank van Harmelen and
               Nathalie Pernelle and Fatiha Sa{\"{\i}}s},
  title     = {Detecting Erroneous Identity Links on the Web Using Network Metrics},
  booktitle = "Proceedings of the 17th International Semantic Web Conference",
  year =      "2018",
  pages =     "391--407--68",
  editor    = {Denny Vrandecic and
               Kalina Bontcheva and
               Mari Carmen Su{\'{a}}rez{-}Figueroa and
               Valentina Presutti and
               Irene Celino and
               Marta Sabou and
               Lucie{-}Aim{\'{e}}e Kaffee and
               Elena Simperl},
  publisher = "Springer Verlag",
  series =    "Lecture Notes in Computer Science",
  number =    "11136",
  keywords = {Semantic Web},
  urlPaper = "http://www.cs.vu.nl/~frankh/postscript/ISWC2018.pdf"
}

@InProceedings{EKAW2018,
  author={Al Koudous Idrissou and Frank van Harmelen and
         Peter Van den Besselaar},
  title = {Network Metrics for Assessing the Quality of Entity Resolution
           Between Multiple Datasets},
  booktitle={21st International Conference on Knowledge Engineering and
             Knowledge Management, {EKAW} 2018},
  editor={Catherine Faron{-}Zucker and Chiara Ghidini and
               Amedeo Napoli and Yannick Toussaint},
  year = 2018,
  volume = 11313,
  isbn={978-3-030-03666-9},
  pages={147--162},
  doi       = {10.1007/978-3-030-03667-6\_10},
  series = {LNCS},
  publisher={Springer International Publishing},
  keywords = {Medical Knowledge Representation},
  urlPaper= "http://www.cs.vu.nl/~frankh/postscript/EKAW2018.pdf"
}


@inproceedings{ESWC2018,
  title={sameAs.cc: The Closure of 500M owl:sameAs Statements},
  author={Wouter Beek and Joe Raad and Jan Wielemaker and Frank van Harmelen},
  booktitle={European Semantic Web Conference, ESWC 2018},
  pages={},
  year={2018},
  publisher={Springer Berlin Heidelberg},
  keywords = {Semantic Web},
  urlPaper = "http://www.cs.vu.nl/~frankh/postscript/ESWC2018.pdf"
}


@article{DataIntelligence2019,
title = "Constructing and Cleaning Identity Graphs in the LOD Cloud",
author = "Joe Raad and Wouter Beek and Frank van Harmelen and Jan Wielemaker And Nathalie Pernelle and Fatiha Sais",
journal = "Data Intelligence",
year = "2019",
volume = "xxx",
number = "xxx",
pages = "xxx",
urlPaper = "http://www.cs.vu.nl/~frankh/postscript/DataIntelligence2019.pdf",
keywords = {Semantic Web}
}


@InProceedings{WI2019,
  author =       "Floris den Hengst and Mark Hoogendoorn and Frank van Harmelen and Joost Bosman",
  title =        "Reinforcement Learning for Personalized Dialogue Management",
  booktitle =    "IEEE/WIC/ACM International Conference on Web Intelligence
                  ({WI '19})",
  year =         2019,
  editor =       "xxx",
  number =       "xxx",
  pages =        "xxx",
  publisher =    "IEEE",
  isbn =         "978-1-4503-6934-3/19/10",
  doi =          "http://dx.doi.org/10.1145/3350546.3352501",
  keywords = {Machine Learning},
  urlPaper = "http://www.cs.vu.nl/~frankh/postscript/WI2019.pdf"
}

@InProceedings{ISWC2019-RelationLinker,
  author    = {Jeff Z. Pan and Mei Zhang and Kuldeep Singh and
               Frank van Harmelen  and Jinguang Gu and Zhi Zhang},
  title     = {Entity Enabled Relation Linking},
  booktitle = "Proceedings of the 18th Internation Semantic Web Conference",
  year =      "2019",
  pages =     "xxx-xxx",
  editor    = {xxx},
  publisher = "Springer Verlag",
  series =    "Lecture Notes in Computer Science",
  number =    "xxx",
  keywords = {Semantic Web},
  urlPaper = "http://www.cs.vu.nl/~frankh/postscript/ISWC2019-RelationLinker.pdf"
}


@InProceedings{ISWC2019-LODanalytics,
  author    = {Luigi Asprino and Wouter Beek and Paolo Ciancarini and Frank van Harmelen and Valentina Presutti},
  title     = {Observing the LOD Cloud using Equivalent Set Graphs:
               the LOD Cloud is mostly flat and sparsely linked},
  booktitle = "Proceedings of the 18th Internation Semantic Web Conference",
  year =      "2019",
  pages =     "xxx-xxx",
  editor    = {xxx},
  publisher = "Springer Verlag",
  series =    "Lecture Notes in Computer Science",
  number =    "xxx",
  keywords = {Semantic Web},
  urlPaper = "http://www.cs.vu.nl/~frankh/postscript/ISWC2019-LODanalytics.pdf"
}


@article{JWE2019,
title = "A Boxology of Design Patterns for Hybrid Learning and Reasoning Systems",
author = "Frank van Harmelen and Annette ten Teije",
journal = "Journal of Web Engineering",
year = "2019",
volume = "18",
number = "1-3",
pages = "97-124",
doi = " 10.13052/jwe1540-9589.18133",
urlPaper = "http://www.cs.vu.nl/~frankh/postscript/JWE2019.pdf",
keywords = {Knowledge Representation},
keywords = {Machine Learning},
abstract = "We propose a set of compositional design patterns to describe a large variety of systems that combine statistical techniques from machine learning with symbolic techniques from knowledge representation. As in other areas of computer science (knowledge engineering, software engineering, ontology engineering, process mining and others), such design patterns help to systematize the literature, clarify which combinations of techniques serve which purposes, and encourage re-use of software components. We have validated our set of compositional design patterns against a large body of recent literature."
}

@article{JWS2019,
title = "User-centric pattern mining on knowledge graphs: An archaeological case study",
author = "W.X. Wilcke and V. de Boer and M.T.M. de Kleijn and F.A.H. van Harmelen and H.J. Scholten",
journal = "Journal of Web Semantics",
year = "2019",
volume = {in print},
issn = "1570-8268",
doi = "10.1016/j.websem.2018.12.004",
url = "http://www.sciencedirect.com/science/article/pii/S1570826818300660",
urlPaper = "http://www.cs.vu.nl/~frankh/postscript/JWS2019.pdf",
keywords = {Semantic Web},
abstract = "In recent years, there has been a growing interest from the digital humanities in knowledge graphs as data modelling paradigm. Already, this has led to the creation of many such knowledge graphs, many of which are now available as part of the Linked Open Data cloud. This presents new opportunities for data mining. In this work, we develop, implement, and evaluate (both data-driven and user-driven) an end-to-end pipeline for user-centric pattern mining on knowledge graphs in the humanities. This pipeline combines constrained generalized association rule mining with natural language output and facet rule browsing to allow for transparency and interpretability?two key domain requirements. Experiments in the archaeological domain show that domain experts were positively surprised by the range of patterns that were discovered and were overall optimistic about the future potential of this approach."
}

@Article{JAL2019,
  author = 	 {Emile van Krieken and Erman Acar and Frank van Harmelen},
  title = 	 {Semi-Supervised Learning using Differentiable Reasoning},
  journal = 	 {Journal of Applied Logics},
  year = 	 2019,
  volume = 	 {in print},
  keywords =     {Knowledge Representation},
  urlPaper = "http://www.cs.vu.nl/~frankh/postscript/JAL2019.pdf"
}


@article{raad2019sameas,
  title={The sameAs Problem: A Survey on Identity Management in the Web of Data},
  author={Raad, Joe and Pernelle, Nathalie and Sa{\"\i}s, Fatiha and Beek, Wouter and van Harmelen, Frank},
  journal={arXiv preprint arXiv:1907.10528},
  year={2019}
}

@article{asprino2019linked,
  title={The Linked Open Data cloud is more abstract, flatter and less linked than you may think!},
  author={Asprino, Luigi and Beek, Wouter and Ciancarini, Paolo and van Harmelen, Frank and Presutti, Valentina},
  journal={arXiv preprint arXiv:1906.08097},
  year={2019}
}

@inproceedings{folmer2019enhancing,
  title={Enhancing the Usefulness of Open Governmental Data with Linked Data Viewing Techniques},
  author={Folmer, Erwin and Beek, Wouter and Rietveld, Laurens and Ronzhin, Stanislav and Geerling, Rutger and den Haan, Davey},
  booktitle={Proceedings of the 52nd Hawaii International Conference on System Sciences},
  year={2019}
}

@article{avila2018classifying,
  title={CLASSIFYING THE LOD CLOUD},
  author={{\'A}vila, Daniel Mart{\'\i}nez and Smiraglia, Richard P and Szostak, Rick and Scharnhorst, Andrea and Beek, Wouter and Siebes, Ronald and Ridenour, Laura and Schlais, Vanessa},
  journal={Brazilian Journal of Information Science: research trends},
  volume={12},
  number={4},
  pages={06--10},
  year={2018}
}




@article{folmer2018linked,
  title={Linked Data Viewing as part of the Spatial Data Platform of the Future},
  author={Folmer, E and Beek, W and Rietveld, Luuk},
  journal={International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  volume={42},
  number={4},
  pages={49--52},
  year={2018},
  publisher={Copernicus Publications}
}


@article{beek2018adding,
  title={Adding 3d GIS visualization and navigation to the sparqlquery loop},
  author={Beek, W and Folmer, E and Rietveld, L and Baving, T and Van Altena, V},
  journal={International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences-ISPRS Archives},
  volume={42},
  number={4/W8},
  pages={11--14},
  year={2018}
}


@article{beek2018k,
  title={The'K'in'Semantic Web'Stands for'Knowledge'},
  author={Beek, WGJ},
  year={2018},
  publisher={Amsterdam: Vrije Universiteit}
}


@inproceedings{beek2018nlgis,
  title={nlGis: A use case in linked historic geodata},
  author={Beek, Wouter and Zijdeman, Richard},
  booktitle={European Semantic Web Conference},
  pages={437--447},
  year={2018},
  organization={Springer}
}





@article{szostak2018connecting,
  title={Connecting KOSs and the LOD Cloud},
  author={Szostak, Rick and Scharnhorst, Andrea and Beek, Wouter and Smiraglia, Richard P},
  journal={arXiv preprint arXiv:1802.08141},
  year={2018}
}


@article{raad2018detection,
  title={D{\'e}tection de liens d?identit{\'e} erron{\'e}s en utilisant la d{\'e}tection de communaut{\'e}s dans les graphes d?identit{\'e}},
  author={Raad, Joe and Beek, Wouter and Pernelle, Nathalie and Sa{\"\i}s, Fatiha and Van Harmelen, Frank},
  journal={ISI},
  volume={23},
  number={3-4},
  year={2018}
}


@inproceedings{daga2019modelling,
  title={Modelling and Querying Lists in RDF. A Pragmatic Study},
  author={Daga, Enrico and Mero{\~n}o-Pe{\~n}uela, Albert and Motta, Enrico},
  booktitle={ISWC Workshops: QuWeDa},
  pages={In--Press},
  year={2019}
}

@article{merono2019list,
  title={List. MID: A MIDI-Based Benchmark for Evaluating RDF Lists},
  author={Mero{\~n}o-Pe{\~n}uela, Albert and Daga, Enrico},
  journal={Lecture Notes in Computer Science},
  pages={In--Press},
  year={2019},
  publisher={Springer}
}


@inproceedings{merono2018semantic,
  title={The semantic web MIDI tape: An interface for interlinking MIDI and context metadata},
  author={Mero{\~n}o-Pe{\~n}uela, Albert and De Valk, Reinier and Daga, Enrico and Daquino, Marilena and Kent-Muller, Anna},
  booktitle={Proceedings of the 1st International Workshop on Semantic Applications for Audio and Music},
  pages={24--32},
  year={2018},
  organization={ACM}
}


@inproceedings{kuhn2018nanopublications,
  title={Nanopublications: A growing resource of provenance-centric scientific linked data},
  author={Kuhn, Tobias and Merono-Penuela, Albert and Malic, Alexander and Poelen, Jorrit H and Hurlbert, Allen H and Ortiz, Emilio Centeno and Furlong, Laura I and Queralt-Rosinach, N{\'u}ria and Chichester, Christine and Banda, Juan M and others},
  booktitle={2018 IEEE 14th International Conference on e-Science (e-Science)},
  pages={83--92},
  organization={IEEE}
}




@article{hoekstra2018datalegend,
  title={The dataLegend ecosystem for historical statistics},
  author={Hoekstra, Rinke and Mero{\~n}o-Pe{\~n}uela, Albert and Rijpma, Auke and Zijdeman, Richard and Ashkpour, Ashkan and Dentler, Kathrin and Zandhuis, Ivo and Rietveld, Laurens},
  journal={Journal of Web Semantics},
  volume={50},
  pages={49--61},
  year={2018},
  publisher={Elsevier}
}


@article{merono2018improving,
  title={Improving Access to the Dutch Historical Censuses with Linked Open Data: Social and Economic History},
  author={Mero{\~n}o-Pe{\~n}uela, Albert and Ashkpour, Ashkan and Gilissen, Valentijn and Jonker, Jan and Vreugdenhil, Tom and Doorn, Peter},
  journal={Research Data Journal for the Humanities and Social Sciences},
  volume={3},
  number={1},
  pages={13--26},
  year={2018},
  publisher={Brill}
}


@article{fensel2018joint,
  title={Joint Proceedings of SEMANTiCS 2017 Workshops co-located with the 13th International Conference on Semantic Systems (SEMANTiCS 2017), Amsterdam, Netherlands, September 11 and 14, 2017},
  author={Fensel, Anna and Daniele, Laura and Aroyo, Lora and de Boer, Victor and Dar{\'a}nyi, S{\'a}ndor and Elloumi, Omar and Garc{\'\i}a-Castro, Ra{\'u}l and Hollink, Laura and Inel, Oana and Kuys, Gerard and others},
  year={2018},
  publisher={CEUR-WS. org}
}


@inproceedings{zhao2018text,
  title={Text Classification of Micro-blog's Tree Hole Based on Convolutional Neural Network},
  author={Zhao, Xiaoli and Lin, Shaofu and Huang, Zhisheng},
  booktitle={Proceedings of the 2018 International Conference on Algorithms, Computing and Artificial Intelligence},
  pages={61},
  year={2018},
  organization={ACM}
}

@inproceedings{zhao2018extraction,
  title={Extraction of semantic relations from medical literature based on semantic predicates and SVM},
  author={Zhao, Xiaoli and Lin, Shaofu and Huang, Zhisheng},
  booktitle={International Conference on Health Information Science},
  pages={17--24},
  year={2018},
  organization={Springer}
}


@inproceedings{du2018making,
  title={Making semantic annotation on patient data of depression},
  author={Du, Yanan and Lin, Shaofu and Huang, Zhisheng},
  booktitle={2nd International Conference on Medical and Health Informatics, ICMHI 2018},
  pages={134--137},
  year={2018},
  organization={Association for Computing Machinery}
}

@article{li2018wcp,
  title={WCP-RNN: a novel RNN-based approach for Bio-NER in Chinese EMRs},
  author={Li, Jianqiang and Zhao, Shenhe and Yang, Jijiang and Huang, Zhisheng and Liu, Bo and Chen, Shi and Pan, Hui and Wang, Qing},
  journal={The Journal of Supercomputing},
  pages={1--18},
  publisher={Springer}
}

@misc{teije2018preface,
  title={Preface: AIME 2017.},
  author={Teije, AT and Popow, Christian and Holmes, John H and Sacchi, Lucia},
  year={2018}
}

@conference{b56ce393e04c4f3d8a1c0c70927c9e5c,
title = "Filtering clinical guideline interactions with pre-conditions: A case study on diabetes guideline",
author = "Veruska Zamborlini and {van der Heijden}, Roelof and {ten Teije}, Annette",
year = "2018",
month = "1",
day = "1",
language = "English",
note = "2018 Joint Reasoning with Ambiguous and Conflicting Evidence and Recommendations in Medicine and the 3rd International Workshop on Ontology Modularity, Contextuality, and Evolution, MedRACER + WOMoCoE 2018 ; Conference date: 29-10-2018",
}


@inproceedings{ilievski2018systematic,
  title={Systematic Study of Long Tail Phenomena in Entity Linking},
  author={Ilievski, Filip and Vossen, Piek and Schlobach, Stefan},
  booktitle={Proceedings of the 27th International Conference on Computational Linguistics},
  pages={664--674},
  year={2018}
}


@inproceedings{kiesel2018preferential,
  title={Preferential default reasoning on the semantic web},
  author={Kiesel, Rafael and Acar, Erman and Schlobach, Stefan},
  booktitle={31st International Workshop on Description Logics, DL 2018},
  pages={1--12},
  year={2018}
}




@article{van2018three,
  title={Three Tools for Practical Differential Privacy},
  author={van der Veen, Koen Lennart and Seggers, Ruben and Bloem, Peter and Patrini, Giorgio},
  journal={arXiv preprint arXiv:1812.02890},
  year={2018}
}


@article{bloem2018tutorial,
  title={A tutorial on MDL hypothesis testing for graph analysis},
  author={Bloem, Peter and de Rooij, Steven},
  journal={arXiv preprint arXiv:1810.13163},
  year={2018}
}


@article{bloem2018learning,
  title={Learning sparse transformations through backpropagation},
  author={Bloem, Peter},
  journal={arXiv preprint arXiv:1810.09184},
  year={2018}
}

@article{veer2018deep,
  title={Deep Learning for Classification Tasks on Geospatial Vector Polygons},
  author={Veer, Rein van't and Bloem, Peter and Folmer, Erwin},
  journal={arXiv preprint arXiv:1806.03857},
  year={2018}
}


@inproceedings{schlichtkrull2018modeling,
  title={Modeling relational data with graph convolutional networks},
  author={Schlichtkrull, Michael and Kipf, Thomas N and Bloem, Peter and Van Den Berg, Rianne and Titov, Ivan and Welling, Max},
  booktitle={European Semantic Web Conference},
  pages={593--607},
  year={2018},
  organization={Springer}
}


@inproceedings{acar2019making,
  title={Making Decisions over Contextual Ontologies},
  author={Acar, E and Penaloza, Rafael and Prediou, Livia},
  booktitle={Description Logics 2019 Workshop Proceedings},
  year={2019}
}

@inproceedings{acar2019satisfiability,
  title={Satisfiability in Strategy Logic can be Easier than Model Checking},
  author={Acar, E and Benerecetti, Massimo and Mogavero, Fabio},
  booktitle={Proceedings of AAAI Conference on Artificial Intelligence 2019},
  year={2019}
}


@article{acar2018knowledge,
  title={Knowledge Representation for Automated Decision Making},
  author={Acar, E},
  year={2018},
  publisher={Mannheim University Library}
}


@article{van2019differentiable,
  title={Differentiable Fuzzy Logics},
  author={van Krieken, Emile},
  year={2019}
}

@article{tiddi2019robot,
  title={Robot--City Interaction: Mapping the Research Landscape?A Survey of the Interactions Between Robots and Modern Cities},
  author={Tiddi, Ilaria and Bastianelli, Emanuele and Daga, Enrico and d?Aquin, Mathieu and Motta, Enrico},
  journal={International Journal of Social Robotics},
  pages={1--26},
  publisher={Springer}
}


@inproceedings{chiatti2019exploring,
  title={Exploring task-agnostic, ShapeNet-based object recognition for mobile robots},
  author={Chiatti, Agnese and Bardaro, Gianluca and Bastianelli, Emanuele and Tiddi, Ilaria and Mitra, Prasenjit and Motta, Enrico},
  booktitle={2019 Workshops of the EDBT/ICDT Joint Conference, EDBT/ICDT-WS 2019},
  pages={1--8},
  year={2019},
  organization={CEUR Workshop Proceedings}
}


@article{mensio2018multi,
  title={A Multi-layer LSTM-based Approach for Robot Command Interaction Modeling},
  author={Mensio, Martino and Bastianelli, Emanuele and Tiddi, Ilaria and Rizzo, Giuseppe},
  journal={arXiv preprint arXiv:1811.05242},
  year={2018}
}

@inproceedings{troullinou2018re,
  title={Re-coding Black Mirror Chairs' Welcome \& Organization},
  author={Troullinou, Pinelopi and d'Aquin, Mathieu and Tiddi, Ilaria},
  booktitle={Companion Proceedings of the The Web Conference 2018},
  pages={1527--1528},
  year={2018},
  organization={International World Wide Web Conferences Steering Committee}
}

@inproceedings{bastianelli2018meet,
  title={Meet HanS, the Health\&Safety autonomous inspector},
  author={Bastianelli, Emanuele and Bardaro, Gianluca and Tiddi, Ilaria and Motta, Enrico},
  booktitle={2018 ISWC Posters and Demonstrations, Industry and Blue Sky Ideas Tracks, ISWC-P and D-Industry-BlueSky 2018},
  year={2018},
  organization={CEUR Workshop Proceedings}
}


@inproceedings{tiddi2018user,
  title={A user-friendly interface to control ROS robotic platforms},
  author={Tiddi, Ilaria and Bastianelli, Emanuele and Bardaro, Gianluca and Motta, Enrico},
  booktitle={2018 ISWC Posters and Demonstrations, Industry and Blue Sky Ideas Tracks, ISWC-P and D-Industry-BlueSky 2018},
  year={2018},
  organization={CEUR}
}

@inproceedings{daquino2018creating,
  title={Creating open citation data with BCITE},
  author={Daquino, Marilena and Tiddi, Ilaria and Peroni, Silvio and Shotton, David},
  booktitle={2nd Workshop on Enabling Open Semantic Science, SemSci 2018},
  pages={1--11},
  year={2018},
  organization={CEUR-WS. org}
}

@book{48dc6c68586e4478989885f8a40a478a,
title = "Creating an organization-dataset through entity linking: RISIS Deliverable 8.3",
author = "O.A.K. Idrissou and A. Khalili and {van den Besselaar}, P.A.A. and {van Harmelen}, Frank",
year = "2018",
month = "6",
language = "English",
publisher = "Vrije Universtiteit",
}

@book{3e007352e5b4412dbfc46d255ba899cb,
title = "Interfacing with external heterogeneous data: RISIS Deliverable 7.4",
author = "A. Khalili and {van den Besselaar}, P.A.A. and {de Graaf}, K.A. and O.A.K. Idrissou and {van Harmelen}, Frank",
year = "2018",
language = "English",
publisher = "Vrije Universtiteit",
}

@inbook{ae824c9530eb43e2934e6c5ddcc57acf,
title = "Network metrics for assessing the quality of entity resolution between multiple datasets",
keywords = "Network metrics, Data Integration, Entity resolution",
author = "O.A.K. Idrissou and {van Harmelen}, Frank and {van den Besselaar}, P.A.A.",
year = "2018",
doi = "10.1007/978-3-030-03667-6_10",
language = "English",
isbn = "978-3-030-03666-9",
volume = "11313",
series = "Lecture Notes in Arti?cial Intelligence",
publisher = "Springer Nature Switzerland AG",
pages = "147--162",
editor = "{Faron Zucker}, Catherine and Chiara Ghidini and Amedeo Napoli and Yannick Toussaint",
booktitle = "Knowledge Engineering and Knowledge Management",
}

@book{3543c742576444209797c642013fae03,
title = "The Lenticular Lens: Customized linking and link modification (the SMS disambiguation service): RISIS Deliverable 25-4: Disambiguation methods",
author = "O.A.K. Idrissou and A. Khalili and R.J. Hoekstra and {Carretta Zamborlini}, V. and {van Harmelen}, Frank and {van den Besselaar}, P.A.A.",
year = "2018",
month = "6",
language = "English",
publisher = "Vrije Universtiteit",
}


@conference{590c1a2aa47946c7af309e2ef6917d7d,
title = "ReferenceNet: A semantic-pragmatic network for capturing reference relations",
author = "Piek Vossen and Marten Postma and Filip Ilievski",
year = "2018",
month = "1",
language = "English",
note = "9th Global WordNet Conference, GWC 2018 ; Conference date: 08-01-2018 Through 12-01-2018",
}

@inproceedings{4ad527e286804cd1aeef15921662a614,
title = "Don't Annotate, but Validate: a Data-to-Text Method for Capturing Event Data",
keywords = "Event coreference, Structured data, Text corpora",
author = "Piek Vossen and Filip Ilievski and Marten Postma and R.H. Segers",
year = "2018",
language = "English",
pages = "3034--3042",
editor = "Hitoshi Isahara and Bente Maegaard and Stelios Piperidis and Christopher Cieri and Thierry Declerck and Koiti Hasida and Helene Mazo and Khalid Choukri and Sara Goggi and Joseph Mariani and Asuncion Moreno and Nicoletta Calzolari and Jan Odijk and Takenobu Tokunaga",
booktitle = "[Proceedings of the] Eleventh International Conference on Language Resources and Evaluation (LREC 2018)",
publisher = "LREC",
}

@inproceedings{DBLP:conf/ectel/EliasJRSGKWLA18,
  author    = {Mirette Elias and
               Abi James and
               Edna Ruckhaus and
               Mari Carmen Su{\'{a}}rez{-}Figueroa and
               Klaas Andries de Graaf and
               Ali Khalili and
               Benjamin Wulff and
               Steffen Lohmann and
               S{\"{o}}ren Auer},
  title     = {SlideWiki - Towards a Collaborative and Accessible Platform for Slide
               Presentations},
  booktitle = {{EC-TEL} Practitioner Proceedings 2018: 13th European Conference On
               Technology Enhanced Learning, Leeds, UK, September 3-6, 2018.},
  year      = {2018},
  crossref  = {DBLP:conf/ectel/2018p},
  url       = {http://ceur-ws.org/Vol-2193/paper6.pdf},
  timestamp = {Tue, 28 May 2019 16:23:29 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/ectel/EliasJRSGKWLA18},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/esws/KhaliliBG18a,
  author    = {Ali Khalili and
               Peter van den Besselaar and
               Klaas Andries de Graaf},
  title     = {Using Linked Open Geo Boundaries for Adaptive Delineation of Functional
               Urban Areas},
  booktitle = {Proceedings of the 3rd International Workshop on Geospatial Linked
               Data and the 2nd Workshop on Querying the Web of Data co-located with
               15th Extended Semantic Web Conference {(ESWC} 2018), Heraklion, Greece,
               June 3, 2018.},
  pages     = {9--21},
  year      = {2018},
  crossref  = {DBLP:conf/esws/2018geold},
  url       = {http://ceur-ws.org/Vol-2110/paper1.pdf},
  timestamp = {Mon, 17 Jun 2019 17:36:46 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/esws/KhaliliBG18a},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/esws/KhaliliBG18b,
  author    = {Ali Khalili and
               Peter van den Besselaar and
               Klaas Andries de Graaf},
  title     = {Using Linked Open Geo Boundaries for Adaptive Delineation of Functional
               Urban Areas},
  booktitle = {The Semantic Web: {ESWC} 2018 Satellite Events - {ESWC} 2018 Satellite
               Events, Heraklion, Crete, Greece, June 3-7, 2018, Revised Selected
               Papers},
  pages     = {327--341},
  year      = {2018},
  crossref  = {DBLP:conf/esws/2018s},
  url       = {https://doi.org/10.1007/978-3-319-98192-5\_51},
  doi       = {10.1007/978-3-319-98192-5\_51},
  timestamp = {Mon, 17 Jun 2019 17:36:46 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/esws/KhaliliBG18b},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/esws/KhaliliBG18,
  author    = {Ali Khalili and
               Peter van den Besselaar and
               Klaas Andries de Graaf},
  title     = {{FERASAT:} {A} Serendipity-Fostering Faceted Browser for Linked Data},
  booktitle = {The Semantic Web - 15th International Conference, {ESWC} 2018, Heraklion,
               Crete, Greece, June 3-7, 2018, Proceedings},
  pages     = {351--366},
  year      = {2018},
  crossref  = {DBLP:conf/esws/2018},
  url       = {https://doi.org/10.1007/978-3-319-93417-4\_23},
  doi       = {10.1007/978-3-319-93417-4\_23},
  timestamp = {Mon, 17 Jun 2019 17:36:46 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/esws/KhaliliBG18},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/i-semantics/KhaliliG18,
  author    = {Ali Khalili and
               Klaas Andries de Graaf},
  title     = {SlideWiki - {A} Platform for Authoring {FAIR} Educational Content},
  booktitle = {Proceedings of the Posters and Demos Track of the 14th International
               Conference on Semantic Systems co-located with the 14th International
               Conference on Semantic Systems (SEMANTiCS 2018), Vienna, Austria,
               September 10-13, 2018.},
  year      = {2018},
  crossref  = {DBLP:conf/i-semantics/2018pd},
  url       = {http://ceur-ws.org/Vol-2198/paper\_108.pdf},
  timestamp = {Tue, 28 May 2019 16:23:30 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/i-semantics/KhaliliG18},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/icsa/SangwanG18,
  author    = {Raghvinder Sangwan and
               Klaas Andries de Graaf},
  title     = {{ICSA} 2018: Engineering Track: Message from the Chairs},
  booktitle = {2018 {IEEE} International Conference on Software Architecture Companion,
               {ICSA} Companion 2018, Seattle, WA, USA, April 30 - May 4, 2018},
  pages     = {76},
  year      = {2018},
  crossref  = {DBLP:conf/icsa/2018c},
  url       = {https://doi.org/10.1109/ICSA-C.2018.00028},
  doi       = {10.1109/ICSA-C.2018.00028},
  timestamp = {Mon, 05 Nov 2018 11:12:21 +0100},
  biburl    = {https://dblp.org/rec/bib/conf/icsa/SangwanG18},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1907-10528,
  author    = {Joe Raad and
               Nathalie Pernelle and
               Fatiha Sa{\"{\i}}s and
               Wouter Beek and
               Frank van Harmelen},
  title     = {The sameAs Problem: {A} Survey on Identity Management in the Web of
               Data},
  journal   = {CoRR},
  volume    = {abs/1907.10528},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.10528},
  archivePrefix = {arXiv},
  eprint    = {1907.10528},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1907-10528},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@phdthesis{DBLP:phd/hal/Raad18,
  author    = {Joe Raad},
  title     = {Identity Management in Knowledge Graphs. (Gestion d'identit{\'{e}}
               dans des graphes de connaissances)},
  school    = {University of Paris-Saclay, France},
  year      = {2018},
  url       = {https://tel.archives-ouvertes.fr/tel-02073961},
  timestamp = {Thu, 25 Apr 2019 11:13:30 +0200},
  biburl    = {https://dblp.org/rec/bib/phd/hal/Raad18},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/ekaw/IbanescuADDGPR18,
  author    = {Liliana Ibanescu and
               Thomas Allard and
               St{\'{e}}phane Dervaux and
               Juliette Dibie and
               Elisabeth Guichard and
               Caroline P{\'{e}}nicaud and
               Joe Raad},
  title     = {A Use Case of Data Integration in Food Production},
  booktitle = {Proceedings of the {EKAW} 2018 Posters and Demonstrations Session
               co-located with 21st International Conference on Knowledge Engineering
               and Knowledge Management {(EKAW} 2018), Nancy, France, November 12-16,
               2018.},
  pages     = {17--20},
  year      = {2018},
  crossref  = {DBLP:conf/ekaw/2018pd},
  url       = {http://ceur-ws.org/Vol-2262/ekaw-poster-11.pdf},
  timestamp = {Tue, 28 May 2019 16:23:29 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/ekaw/IbanescuADDGPR18},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
